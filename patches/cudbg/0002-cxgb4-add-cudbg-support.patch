From 3914286e724e6c1a1300e22d75aaf0a935d49100 Mon Sep 17 00:00:00 2001
Message-Id: <3914286e724e6c1a1300e22d75aaf0a935d49100.1505903155.git.rahul.lakkireddy@chelsio.com>
In-Reply-To: <6ef9c2bbd38d89a8e0c607bf990ffefc8ed351a2.1505903155.git.rahul.lakkireddy@chelsio.com>
References: <6ef9c2bbd38d89a8e0c607bf990ffefc8ed351a2.1505903155.git.rahul.lakkireddy@chelsio.com>
From: Rahul Lakkireddy <rahul.lakkireddy@chelsio.com>
Date: Wed, 20 Sep 2017 09:08:13 +0530
Subject: [PATCH 2/3] cxgb4: add cudbg support

Add support to collect cudbg debug logs from cxgb4.

Signed-off-by: Rahul Lakkireddy <rahul.lakkireddy@chelsio.com>
---
 drivers/net/ethernet/chelsio/cxgb4/Makefile        |    2 +-
 drivers/net/ethernet/chelsio/cxgb4/cudbg_common.c  |  100 +
 drivers/net/ethernet/chelsio/cxgb4/cudbg_entity.h  |  550 +++
 .../net/ethernet/chelsio/cxgb4/cudbg_flash_utils.c |  302 ++
 drivers/net/ethernet/chelsio/cxgb4/cudbg_if.h      |  452 +++
 drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.c     | 3603 ++++++++++++++++++++
 drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.h     |  307 ++
 .../net/ethernet/chelsio/cxgb4/cudbg_lib_common.h  |  133 +
 drivers/net/ethernet/chelsio/cxgb4/cudbg_utls.c    |  226 ++
 drivers/net/ethernet/chelsio/cxgb4/cudbg_wtp.c     |   12 +
 drivers/net/ethernet/chelsio/cxgb4/cxgb4.h         |    1 +
 drivers/net/ethernet/chelsio/cxgb4/cxgb4_cudbg.c   |   82 +
 drivers/net/ethernet/chelsio/cxgb4/fastlz.c        |  327 ++
 drivers/net/ethernet/chelsio/cxgb4/fastlz.h        |   78 +
 drivers/net/ethernet/chelsio/cxgb4/fastlz_api.c    |  204 ++
 drivers/net/ethernet/chelsio/cxgb4/fastlz_common.h |  107 +
 16 files changed, 6485 insertions(+), 1 deletion(-)
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_common.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_entity.h
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_flash_utils.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_if.h
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.h
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_lib_common.h
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_utls.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cudbg_wtp.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/cxgb4_cudbg.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/fastlz.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/fastlz.h
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/fastlz_api.c
 create mode 100644 drivers/net/ethernet/chelsio/cxgb4/fastlz_common.h

diff --git a/drivers/net/ethernet/chelsio/cxgb4/Makefile b/drivers/net/ethernet/chelsio/cxgb4/Makefile
index 742fd720..b993eddb 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/Makefile
+++ b/drivers/net/ethernet/chelsio/cxgb4/Makefile
@@ -4,7 +4,7 @@
 
 obj-$(CONFIG_CHELSIO_T4) += cxgb4.o
 
-cxgb4-objs := cxgb4_main.o l2t.o t4_hw.o sge.o clip_tbl.o cxgb4_ethtool.o
+cxgb4-objs := cxgb4_main.o l2t.o t4_hw.o sge.o clip_tbl.o cxgb4_ethtool.o cxgb4_cudbg.o fastlz_api.o fastlz.o cudbg_lib.o cudbg_wtp.o cudbg_flash_utils.o cudbg_common.o
 cxgb4-$(CONFIG_CHELSIO_T4_DCB) +=  cxgb4_dcb.o
 cxgb4-$(CONFIG_CHELSIO_T4_UWIRE) +=  cxgb4_ppm.o
 cxgb4-$(CONFIG_DEBUG_FS) += cxgb4_debugfs.o
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_common.c b/drivers/net/ethernet/chelsio/cxgb4/cudbg_common.c
new file mode 100644
index 00000000..da1226dd
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_common.c
@@ -0,0 +1,100 @@
+#include <linux/types.h>
+#include "cudbg_if.h"
+#include "cxgb4.h"
+#include "cudbg_lib_common.h"
+#include "fastlz_common.h"
+
+struct cudbg_flash_sec_info sec_info;
+
+int get_scratch_buff(struct cudbg_buffer *pdbg_buff, u32 size,
+		     struct cudbg_buffer *pscratch_buff)
+{
+	u32 scratch_offset;
+	int rc = 0;
+
+	scratch_offset = pdbg_buff->size - size;
+	if (pdbg_buff->offset > (int)scratch_offset || pdbg_buff->size < size) {
+		rc = CUDBG_STATUS_NO_SCRATCH_MEM;
+		goto err;
+	} else {
+		pscratch_buff->data = (char *)pdbg_buff->data + scratch_offset;
+		pscratch_buff->offset = 0;
+		pscratch_buff->size = size;
+		pdbg_buff->size -= size;
+	}
+err:
+	return rc;
+}
+
+void release_scratch_buff(struct cudbg_buffer *pscratch_buff,
+			  struct cudbg_buffer *pdbg_buff)
+{
+	pdbg_buff->size += pscratch_buff->size;
+	/* Reset the used buffer to zero.
+	 * If we dont do this, then it will effect the ext entity logic.
+	 */
+	memset(pscratch_buff->data, 0, pscratch_buff->size);
+	pscratch_buff->data = NULL;
+	pscratch_buff->offset = 0;
+	pscratch_buff->size = 0;
+}
+
+struct cudbg_private g_context;
+unsigned char *hash_table[FASTLZ_HASH_SIZE];
+int cudbg_hello(struct cudbg_init *dbg_init, void **handle)
+{
+	int rc = 0;
+
+	dbg_init->hash_table = (unsigned char *)hash_table;
+	memset(&g_context, 0, sizeof(struct cudbg_private));
+	memcpy(&(g_context.dbg_init), dbg_init, sizeof(struct cudbg_init));
+	g_context.psec_info = (struct cudbg_flash_sec_info *)&sec_info;
+	*handle = (void *) &g_context;
+	return rc;
+}
+
+/* cudbg_hello2 : extended version of cudbg_hello
+ * calling method:
+ * 1. first call to cudbg_hello2 with buf_size == 0 will fill buf_size with
+ *    required size
+ * 2. second call will be actual cudbg_hello2 with previous call buf_size*/
+int cudbg_hello2(struct cudbg_init *dbg_init, void **handle, u8 *buf,
+				 u32 *buf_size)
+{
+	struct cudbg_private *context;
+	u32 total_size = sizeof(struct cudbg_private) +
+			 sizeof(struct cudbg_flash_sec_info) +
+			 sizeof(char *) * FASTLZ_HASH_SIZE;
+
+	if (*buf_size < total_size) {
+			*buf_size = total_size;
+			return CUDBG_STATUS_SMALL_BUFF;
+	}
+
+	if (buf == NULL)
+		return CUDBG_STATUS_INVALID_BUFF;
+
+	context = (struct cudbg_private *)buf;
+	memset(context, 0, sizeof(struct cudbg_private));
+	context->psec_info = (struct cudbg_flash_sec_info *)(buf +
+				sizeof(struct cudbg_private));
+	dbg_init->hash_table = (unsigned char *)(context->psec_info) +
+				sizeof(struct cudbg_flash_sec_info);
+	memcpy(&(context->dbg_init), dbg_init, sizeof(struct cudbg_init));
+	*handle = (void *)context;
+
+	return 0;
+}
+
+void reset_sec_info(struct cudbg_flash_sec_info *psec_info)
+{
+	memset(psec_info, 0, sizeof(struct cudbg_flash_sec_info));
+}
+
+int cudbg_bye(void *handle)
+{
+	struct cudbg_private *context = (struct cudbg_private *)handle;
+
+	reset_sec_info(context->psec_info);
+	return 0;
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_entity.h b/drivers/net/ethernet/chelsio/cxgb4/cudbg_entity.h
new file mode 100644
index 00000000..1bc45499
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_entity.h
@@ -0,0 +1,550 @@
+#ifndef __CUDBG_ENTITY_H__
+#define __CUDBG_ENTITY_H__
+
+#define ATTRIBUTE_UNUSED __attribute__ ((unused))
+
+#define MC0_FLAG    1
+#define MC1_FLAG    2
+#define EDC0_FLAG   3
+#define EDC1_FLAG   4
+#define HMA_FLAG    5
+
+#define NUM_PCIE_CONFIG_REGS 0x61
+#define CUDBG_CTXT_SIZE_BYTES 24
+#define CUDBG_MAX_INGRESS_QIDS 65536
+#define CUDBG_MAX_FL_QIDS 2048
+#define CUDBG_MAX_CNM_QIDS 1024
+#define CUDBG_LOWMEM_MAX_CTXT_QIDS 256
+#define ETH_ALEN 6
+#define CUDBG_MAX_RPLC_SIZE 128
+#define CUDBG_NUM_REQ_REGS 17
+#define CUDBG_MAX_TCAM_TID 0x800
+#define CUDBG_NUM_ULPTX 11
+#define CUDBG_NUM_ULPTX_READ 512
+
+#define SN_REG_ADDR 0x183f
+#define BN_REG_ADDR 0x1819
+#define NA_REG_ADDR 0x185a
+#define MN_REG_ADDR 0x1803
+
+#define PORT_TYPE_ADDR 0x1869
+#define PORT_TYPE_LEN 8
+
+/* For T6 */
+#define SN_T6_ADDR 0x83f
+#define BN_T6_ADDR 0x819
+#define NA_T6_ADDR 0x85a
+#define MN_T6_ADDR 0x803
+
+#define SN_MAX_LEN	 24
+#define BN_MAX_LEN	 16
+#define NA_MAX_LEN	 12
+#define MN_MAX_LEN	 16
+#define MAX_VPD_DATA_LEN 32
+
+#define VPD_VER_ADDR     0x18c7
+#define VPD_VER_LEN      2
+#define SCFG_VER_ADDR    0x06
+#define SCFG_VER_LEN     4
+
+#define CUDBG_CIM_BUSY_BIT (1 << 17)
+
+#define CUDBG_CHAC_PBT_ADDR 0x2800
+#define CUDBG_CHAC_PBT_LRF  0x3000
+#define CUDBG_CHAC_PBT_DATA 0x3800
+#define CUDBG_PBT_DYNAMIC_ENTRIES 8
+#define CUDBG_PBT_STATIC_ENTRIES 16
+#define CUDBG_LRF_ENTRIES 8
+#define CUDBG_PBT_DATA_ENTRIES 512
+
+#define CUDBG_ENTITY_SIGNATURE 0xCCEDB001
+#define CUDBG_TID_INFO_REV 1
+#define CUDBG_MAC_STATS_REV 1
+
+#ifndef ARRAY_SIZE
+#define ARRAY_SIZE(_a)  (sizeof((_a)) / sizeof((_a)[0]))
+#endif
+
+struct cudbg_ver_hdr {
+	u32 signature;
+	u16 revision;
+	u16 size;
+};
+
+struct cudbg_pbt_tables {
+	u32 pbt_dynamic[CUDBG_PBT_DYNAMIC_ENTRIES];
+	u32 pbt_static[CUDBG_PBT_STATIC_ENTRIES];
+	u32 lrf_table[CUDBG_LRF_ENTRIES];
+	u32 pbt_data[CUDBG_PBT_DATA_ENTRIES];
+};
+
+struct card_mem {
+	u16 size_mc0;
+	u16 size_mc1;
+	u16 size_edc0;
+	u16 size_edc1;
+	u16 size_hma;
+	u16 mem_flag;
+	u16 res;
+};
+
+struct rss_pf_conf {
+	u32 rss_pf_map;
+	u32 rss_pf_mask;
+	u32 rss_pf_config;
+};
+
+struct cudbg_ch_cntxt {
+	uint32_t cntxt_type;
+	uint32_t cntxt_id;
+	uint32_t data[SGE_CTXT_SIZE / 4];
+};
+
+struct cudbg_tcam {
+	u32 filter_start;
+	u32 server_start;
+	u32 clip_start;
+	u32 routing_start;
+	u32 tid_hash_base;
+	u32 max_tid;
+};
+
+struct cudbg_mbox_log {
+	struct mbox_cmd entry;
+	u32 hi[MBOX_LEN / 8];
+	u32 lo[MBOX_LEN / 8];
+};
+
+struct cudbg_tid_data {
+	u32 tid;
+	u32 dbig_cmd;
+	u32 dbig_conf;
+	u32 dbig_rsp_stat;
+	u32 data[CUDBG_NUM_REQ_REGS];
+};
+
+struct cudbg_cntxt_field {
+	char *name;
+	u32 start_bit;
+	u32 end_bit;
+	u32 shift;
+	u32 islog2;
+};
+
+struct cudbg_mps_tcam {
+	u64 mask;
+	u32 rplc[8];
+	u32 idx;
+	u32 cls_lo;
+	u32 cls_hi;
+	u32 rplc_size;
+	u32 vniy;
+	u32 vnix;
+	u32 dip_hit;
+	u32 vlan_vld;
+	u32 repli;
+	u16 ivlan;
+	u8 addr[ETH_ALEN];
+	u8 lookup_type;
+	u8 port_num;
+	u8 reserved[2];
+};
+
+struct rss_vf_conf {
+	u32 rss_vf_vfl;
+	u32 rss_vf_vfh;
+};
+
+struct rss_config {
+	u32 tp_rssconf;		/* A_TP_RSS_CONFIG	*/
+	u32 tp_rssconf_tnl;	/* A_TP_RSS_CONFIG_TNL	*/
+	u32 tp_rssconf_ofd;	/* A_TP_RSS_CONFIG_OFD	*/
+	u32 tp_rssconf_syn;	/* A_TP_RSS_CONFIG_SYN	*/
+	u32 tp_rssconf_vrt;	/* A_TP_RSS_CONFIG_VRT	*/
+	u32 tp_rssconf_cng;	/* A_TP_RSS_CONFIG_CNG	*/
+	u32 chip;
+};
+
+struct struct_pm_stats {
+	u32 tx_cnt[T6_PM_NSTATS];
+	u32 rx_cnt[T6_PM_NSTATS];
+	u64 tx_cyc[T6_PM_NSTATS];
+	u64 rx_cyc[T6_PM_NSTATS];
+};
+
+struct struct_hw_sched {
+	u32 kbps[NTX_SCHED];
+	u32 ipg[NTX_SCHED];
+	u32 pace_tab[NTX_SCHED];
+	u32 mode;
+	u32 map;
+};
+
+struct struct_tcp_stats {
+	struct tp_tcp_stats v4, v6;
+};
+
+struct struct_tp_err_stats {
+	struct tp_err_stats stats;
+	u32 nchan;
+};
+
+struct struct_tp_fcoe_stats {
+	struct tp_fcoe_stats stats[4];
+	u32 nchan;
+};
+
+struct struct_mac_stats {
+	u32 port_count;
+	struct port_stats stats[4];
+};
+
+struct struct_mac_stats_rev1 {
+	struct cudbg_ver_hdr ver_hdr;
+	u32 port_count;
+	u32 reserved;
+	struct port_stats stats[4];
+};
+
+struct struct_tp_cpl_stats {
+	struct tp_cpl_stats stats;
+	u32 nchan;
+};
+
+struct struct_wc_stats {
+	u32 wr_cl_success;
+	u32 wr_cl_fail;
+};
+
+struct struct_ulptx_la {
+	u32 rdptr[CUDBG_NUM_ULPTX];
+	u32 wrptr[CUDBG_NUM_ULPTX];
+	u32 rddata[CUDBG_NUM_ULPTX];
+	u32 rd_data[CUDBG_NUM_ULPTX][CUDBG_NUM_ULPTX_READ];
+};
+
+struct struct_ulprx_la {
+	u32 data[ULPRX_LA_SIZE * 8];
+	u32 size;
+};
+
+struct struct_cim_qcfg {
+	u8 chip;
+	u16 base[CIM_NUM_IBQ + CIM_NUM_OBQ_T5];
+	u16 size[CIM_NUM_IBQ + CIM_NUM_OBQ_T5];
+	u16 thres[CIM_NUM_IBQ];
+	u32 obq_wr[2 * CIM_NUM_OBQ_T5];
+	u32 stat[4 * (CIM_NUM_IBQ + CIM_NUM_OBQ_T5)];
+};
+
+static const char * const region[] = {
+	"DBQ contexts:", "IMSG contexts:", "FLM cache:", "TCBs:",
+	"Pstructs:", "Timers:", "Rx FL:", "Tx FL:", "Pstruct FL:",
+	"Tx payload:", "Rx payload:", "LE hash:", "iSCSI region:",
+	"TDDP region:", "TPT region:", "STAG region:", "RQ region:",
+	"RQUDP region:", "PBL region:", "TXPBL region:",
+	"DBVFIFO region:", "ULPRX state:", "ULPTX state:",
+#ifndef __NO_DRIVER_OCQ_SUPPORT__
+	"On-chip queues:"
+#endif
+};
+
+/* Info relative to memory region (i.e. wrt 0). */
+struct struct_region_info {
+	bool exist; /* Does region exists in current memory region? */
+	u32 start;  /* Start wrt 0 */
+	u32 end;    /* End wrt 0 */
+};
+
+struct struct_port_usage {
+	u32 id;
+	u32 used;
+	u32 alloc;
+};
+
+struct struct_mem_desc {
+	u32 base;
+	u32 limit;
+	u32 idx;
+};
+
+struct struct_meminfo {
+	struct struct_mem_desc avail[4];
+	struct struct_mem_desc mem[ARRAY_SIZE(region) + 3];
+	u32 avail_c;
+	u32 mem_c;
+	u32 up_ram_lo;
+	u32 up_ram_hi;
+	u32 up_extmem2_lo;
+	u32 up_extmem2_hi;
+	u32 rx_pages_data[3];
+	u32 tx_pages_data[4];
+	u32 p_structs;
+	struct struct_port_usage port_data[4];
+	u32 port_used[4];
+	u32 port_alloc[4];
+	u32 loopback_used[NCHAN];
+	u32 loopback_alloc[NCHAN];
+};
+
+struct struct_lb_stats {
+	int nchan;
+	struct lb_port_stats s[0];
+};
+
+struct struct_clk_info {
+	u64 retransmit_min;
+	u64 retransmit_max;
+	u64 persist_timer_min;
+	u64 persist_timer_max;
+	u64 keepalive_idle_timer;
+	u64 keepalive_interval;
+	u64 initial_srtt;
+	u64 finwait2_timer;
+	u32 dack_timer;
+	u32 res;
+	u32 cclk_ps;
+	u32 tre;
+	u32 dack_re;
+	char core_clk_period[32];
+	char tp_timer_tick[32];
+	char tcp_tstamp_tick[32];
+	char dack_tick[32];
+};
+
+struct cim_pif_la {
+	int size;
+	u8 data[0];
+};
+
+struct struct_tp_la {
+	u32 size;
+	u32 mode;
+	u8 data[0];
+};
+
+struct ireg_field {
+	u32 ireg_addr;
+	u32 ireg_data;
+	u32 ireg_local_offset;
+	u32 ireg_offset_range;
+};
+
+struct ireg_buf {
+	struct ireg_field tp_pio;
+	u32 outbuf[32];
+};
+
+struct tx_rate {
+	u64 nrate[NCHAN];
+	u64 orate[NCHAN];
+	u32 nchan;
+};
+
+struct tid_info_region {
+	u32 ntids;
+	u32 nstids;
+	u32 stid_base;
+	u32 hash_base;
+
+	u32 natids;
+	u32 nftids;
+	u32 ftid_base;
+	u32 aftid_base;
+	u32 aftid_end;
+
+	/* Server filter region */
+	u32 sftid_base;
+	u32 nsftids;
+
+	/* UO context range */
+	u32 uotid_base;
+	u32 nuotids;
+
+	u32 sb;
+	u32 flags;
+	u32 le_db_conf;
+	u32 IP_users;
+	u32 IPv6_users;
+
+	u32 hpftid_base;
+	u32 nhpftids;
+};
+
+struct tid_info_region_rev1 {
+	struct cudbg_ver_hdr ver_hdr;
+	struct tid_info_region tid;
+	u32 tid_start;
+	u32 reserved[16];
+};
+
+struct struct_vpd_data {
+	u8 sn[SN_MAX_LEN + 1];
+	u8 bn[BN_MAX_LEN + 1];
+	u8 na[NA_MAX_LEN + 1];
+	u8 mn[MN_MAX_LEN + 1];
+	u16 fw_major;
+	u16 fw_minor;
+	u16 fw_micro;
+	u16 fw_build;
+	u32 scfg_vers;
+	u32 vpd_vers;
+};
+
+struct sw_state {
+	u32 fw_state;
+	u8 caller_string[100];
+	u8 os_type;
+	u8 reserved[3];
+	u32 reserved1[16];
+};
+
+static u32 ATTRIBUTE_UNUSED t6_tp_pio_array[][4] = {
+	{0x7e40, 0x7e44, 0x020, 28}, /* t6_tp_pio_regs_20_to_3b */
+	{0x7e40, 0x7e44, 0x040, 10}, /* t6_tp_pio_regs_40_to_49 */
+	{0x7e40, 0x7e44, 0x050, 10}, /* t6_tp_pio_regs_50_to_59 */
+	{0x7e40, 0x7e44, 0x060, 14}, /* t6_tp_pio_regs_60_to_6d */
+	{0x7e40, 0x7e44, 0x06F, 1}, /* t6_tp_pio_regs_6f */
+	{0x7e40, 0x7e44, 0x070, 6}, /* t6_tp_pio_regs_70_to_75 */
+	{0x7e40, 0x7e44, 0x130, 18},  /* t6_tp_pio_regs_130_to_141 */
+	{0x7e40, 0x7e44, 0x145, 19}, /* t6_tp_pio_regs_145_to_157 */
+	{0x7e40, 0x7e44, 0x160, 1}, /* t6_tp_pio_regs_160 */
+	{0x7e40, 0x7e44, 0x230, 25}, /* t6_tp_pio_regs_230_to_248 */
+	{0x7e40, 0x7e44, 0x24a, 3}, /* t6_tp_pio_regs_24c */
+	{0x7e40, 0x7e44, 0x8C0, 1} /* t6_tp_pio_regs_8c0 */
+};
+
+static u32 ATTRIBUTE_UNUSED t5_tp_pio_array[][4] = {
+	{0x7e40, 0x7e44, 0x020, 28}, /* t5_tp_pio_regs_20_to_3b */
+	{0x7e40, 0x7e44, 0x040, 19}, /* t5_tp_pio_regs_40_to_52 */
+	{0x7e40, 0x7e44, 0x054, 2}, /* t5_tp_pio_regs_54_to_55 */
+	{0x7e40, 0x7e44, 0x060, 13}, /* t5_tp_pio_regs_60_to_6c */
+	{0x7e40, 0x7e44, 0x06F, 1}, /* t5_tp_pio_regs_6f */
+	{0x7e40, 0x7e44, 0x120, 4}, /* t5_tp_pio_regs_120_to_123 */
+	{0x7e40, 0x7e44, 0x12b, 2},  /* t5_tp_pio_regs_12b_to_12c */
+	{0x7e40, 0x7e44, 0x12f, 21}, /* t5_tp_pio_regs_12f_to_143 */
+	{0x7e40, 0x7e44, 0x145, 19}, /* t5_tp_pio_regs_145_to_157 */
+	{0x7e40, 0x7e44, 0x230, 25}, /* t5_tp_pio_regs_230_to_248 */
+	{0x7e40, 0x7e44, 0x8C0, 1} /* t5_tp_pio_regs_8c0 */
+};
+
+static u32 ATTRIBUTE_UNUSED t6_ma_ireg_array[][4] = {
+	{0x78f8, 0x78fc, 0xa000, 23}, /* t6_ma_regs_a000_to_a016 */
+	{0x78f8, 0x78fc, 0xa400, 30}, /* t6_ma_regs_a400_to_a41e */
+	{0x78f8, 0x78fc, 0xa800, 20}  /* t6_ma_regs_a800_to_a813 */
+};
+
+static u32 ATTRIBUTE_UNUSED t6_ma_ireg_array2[][4] = {
+	{0x78f8, 0x78fc, 0xe400, 17}, /* t6_ma_regs_e400_to_e600 */
+	{0x78f8, 0x78fc, 0xe640, 13} /* t6_ma_regs_e640_to_e7c0 */
+};
+
+static u32 ATTRIBUTE_UNUSED t6_hma_ireg_array[][4] = {
+	{0x51320, 0x51324, 0xa000, 32} /* t6_hma_regs_a000_to_a01f */
+};
+static u32 ATTRIBUTE_UNUSED t5_pcie_pdbg_array[][4] = {
+	{0x5a04, 0x5a0c, 0x00, 0x20}, /* t5_pcie_pdbg_regs_00_to_20 */
+	{0x5a04, 0x5a0c, 0x21, 0x20}, /* t5_pcie_pdbg_regs_21_to_40 */
+	{0x5a04, 0x5a0c, 0x41, 0x10}, /* t5_pcie_pdbg_regs_41_to_50 */
+};
+
+static u32 ATTRIBUTE_UNUSED t5_pcie_config_array[][2] = {
+	{0x0, 0x34},
+	{0x3c, 0x40},
+	{0x50, 0x64},
+	{0x70, 0x80},
+	{0x94, 0xa0},
+	{0xb0, 0xb8},
+	{0xd0, 0xd4},
+	{0x100, 0x128},
+	{0x140, 0x148},
+	{0x150, 0x164},
+	{0x170, 0x178},
+	{0x180, 0x194},
+	{0x1a0, 0x1b8},
+	{0x1c0, 0x208},
+};
+
+static u32 ATTRIBUTE_UNUSED t5_pcie_cdbg_array[][4] = {
+	{0x5a10, 0x5a18, 0x00, 0x20}, /* t5_pcie_cdbg_regs_00_to_20 */
+	{0x5a10, 0x5a18, 0x21, 0x18}, /* t5_pcie_cdbg_regs_21_to_37 */
+};
+
+static u32 ATTRIBUTE_UNUSED t6_tp_tm_pio_array[1][4] = {
+	{0x7e18, 0x7e1c, 0x0, 12}
+};
+
+static u32 ATTRIBUTE_UNUSED t5_tp_tm_pio_array[1][4] = {
+	{0x7e18, 0x7e1c, 0x0, 12}
+};
+
+static u32 ATTRIBUTE_UNUSED t5_pm_rx_array[][4] = {
+	{0x8FD0, 0x8FD4, 0x10000, 0x20}, /* t5_pm_rx_regs_10000_to_10020 */
+	{0x8FD0, 0x8FD4, 0x10021, 0x0D}, /* t5_pm_rx_regs_10021_to_1002c */
+};
+
+static u32 ATTRIBUTE_UNUSED t5_pm_tx_array[][4] = {
+	{0x8FF0, 0x8FF4, 0x10000, 0x20}, /* t5_pm_tx_regs_10000_to_10020 */
+	{0x8FF0, 0x8FF4, 0x10021, 0x1D}, /* t5_pm_tx_regs_10021_to_1003c */
+};
+
+static u32 ATTRIBUTE_UNUSED t6_tp_mib_index_array[6][4] = {
+	{0x7e50, 0x7e54, 0x0, 13},
+	{0x7e50, 0x7e54, 0x10, 6},
+	{0x7e50, 0x7e54, 0x18, 21},
+	{0x7e50, 0x7e54, 0x30, 32},
+	{0x7e50, 0x7e54, 0x50, 22},
+	{0x7e50, 0x7e54, 0x68, 12}
+};
+
+static u32 ATTRIBUTE_UNUSED t5_tp_mib_index_array[9][4] = {
+	{0x7e50, 0x7e54, 0x0, 13},
+	{0x7e50, 0x7e54, 0x10, 6},
+	{0x7e50, 0x7e54, 0x18, 8},
+	{0x7e50, 0x7e54, 0x20, 13},
+	{0x7e50, 0x7e54, 0x30, 16},
+	{0x7e50, 0x7e54, 0x40, 16},
+	{0x7e50, 0x7e54, 0x50, 16},
+	{0x7e50, 0x7e54, 0x60, 6},
+	{0x7e50, 0x7e54, 0x68, 4}
+};
+
+static u32 ATTRIBUTE_UNUSED t5_sge_dbg_index_array[9][4] = {
+	{0x10cc, 0x10d0, 0x0, 16},
+	{0x10cc, 0x10d4, 0x0, 16},
+};
+
+static u32 ATTRIBUTE_UNUSED t6_up_cim_reg_array[][4] = {
+	{0x7b50, 0x7b54, 0x2000, 0x20},   /* up_cim_2000_to_207c */
+	{0x7b50, 0x7b54, 0x2080, 0x1d},   /* up_cim_2080_to_20fc */
+	{0x7b50, 0x7b54, 0x00, 0x20},     /* up_cim_00_to_7c */
+	{0x7b50, 0x7b54, 0x80, 0x20},     /* up_cim_80_to_fc */
+	{0x7b50, 0x7b54, 0x100, 0x11},    /* up_cim_100_to_14c */
+	{0x7b50, 0x7b54, 0x200, 0x10},    /* up_cim_200_to_23c */
+	{0x7b50, 0x7b54, 0x240, 0x2},     /* up_cim_240_to_244 */
+	{0x7b50, 0x7b54, 0x250, 0x2},     /* up_cim_250_to_254 */
+	{0x7b50, 0x7b54, 0x260, 0x2},     /* up_cim_260_to_264 */
+	{0x7b50, 0x7b54, 0x270, 0x2},     /* up_cim_270_to_274 */
+	{0x7b50, 0x7b54, 0x280, 0x20},    /* up_cim_280_to_2fc */
+	{0x7b50, 0x7b54, 0x300, 0x20},    /* up_cim_300_to_37c */
+	{0x7b50, 0x7b54, 0x380, 0x14},    /* up_cim_380_to_3cc */
+
+};
+
+static u32 ATTRIBUTE_UNUSED t5_up_cim_reg_array[][4] = {
+	{0x7b50, 0x7b54, 0x2000, 0x20},   /* up_cim_2000_to_207c */
+	{0x7b50, 0x7b54, 0x2080, 0x19},   /* up_cim_2080_to_20ec */
+	{0x7b50, 0x7b54, 0x00, 0x20},     /* up_cim_00_to_7c */
+	{0x7b50, 0x7b54, 0x80, 0x20},     /* up_cim_80_to_fc */
+	{0x7b50, 0x7b54, 0x100, 0x11},    /* up_cim_100_to_14c */
+	{0x7b50, 0x7b54, 0x200, 0x10},    /* up_cim_200_to_23c */
+	{0x7b50, 0x7b54, 0x240, 0x2},     /* up_cim_240_to_244 */
+	{0x7b50, 0x7b54, 0x250, 0x2},     /* up_cim_250_to_254 */
+	{0x7b50, 0x7b54, 0x260, 0x2},     /* up_cim_260_to_264 */
+	{0x7b50, 0x7b54, 0x270, 0x2},     /* up_cim_270_to_274 */
+	{0x7b50, 0x7b54, 0x280, 0x20},    /* up_cim_280_to_2fc */
+	{0x7b50, 0x7b54, 0x300, 0x20},    /* up_cim_300_to_37c */
+	{0x7b50, 0x7b54, 0x380, 0x14},    /* up_cim_380_to_3cc */
+};
+
+#endif
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_flash_utils.c b/drivers/net/ethernet/chelsio/cxgb4/cudbg_flash_utils.c
new file mode 100644
index 00000000..90313c58
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_flash_utils.c
@@ -0,0 +1,302 @@
+#include "t4_regs.h"
+#include "cxgb4.h"
+#include "t4_hw.h"
+#include "t4_chip_type.h"
+#include "cudbg_if.h"
+#include "cudbg_lib_common.h"
+
+enum {
+	SF_ATTEMPTS = 10,		/* max retries for SF operations */
+
+	/* flash command opcodes */
+	SF_PROG_PAGE	= 2,	/* program page */
+	SF_WR_DISABLE	= 4,	/* disable writes */
+	SF_RD_STATUS	= 5,	/* read status register */
+	SF_WR_ENABLE	= 6,	/* enable writes */
+	SF_RD_DATA_FAST = 0xb,	/* read flash */
+	SF_RD_ID	= 0x9f, /* read ID */
+	SF_ERASE_SECTOR = 0xd8, /* erase sector */
+};
+
+#if 0
+/*** Flash Layout	***/
+
+	OPROM_START_SEC			0  /* First flash sector for
+					      Option-ROM */
+	NUM_OPROM_SEC			7  /* Number of flash sectors
+					      for Option-ROM */
+	FW_START_SEC			8  /* First flash sector for Firmware */
+
+	NUM_FW_SEC			16 /* Number of flash sectors
+					      for Firmware */
+	BOOT_CONFIG_START_SEC		7  /* First flash sector for config
+					      utility i params */
+	NUM_BOOT_CONFIG_SEC		1  /* Number of flash sectors for config
+					      utility params */
+	HW_CONFIG_START_SEC		31 /* First flash sector for hardware
+					      config file */
+	NUM_HW_CONFIG_SEC		1  /* Number of flash sectors for
+					      hardware config file */
+	UTIL_START_SEC			7  /* First flash sector for Config
+					      Utility */
+	FCOE_BOOT_INFO_SEC		30 /* First flash sector for FCOE BOOT
+					      INFO to OS */
+	NUM_FCOE_BOOT_INFO_SEC		1  /* Number of flash sectors for FCOE
+					      BOOT INFO to OS */
+	ISCSI_BOOT_INFO_SEC		29 /* First flash sector for ISCSI BOOT
+					      INFO to OS */
+	NUM_ISCSI_BOOT_INFO_SEC		1  /* Number of flash sectors for ISCSI
+					      BOOT INFO to OS */
+	VPDINIT_BOOT_INFO_SEC		26 /* First flash sector for VPD INIT */
+
+	NUM_VPDINIT_BOOT_SEC		1  /* Number of flash sectors for VPD
+					      INIT */
+#endif
+
+/* cudbg is writing to second half of the flash 2 MB to 4 MB */
+
+int write_flash(struct adapter *adap, u32 start_sec, void *data, u32 size);
+
+void update_skip_size(struct cudbg_flash_sec_info *psec_info, u32 size)
+{
+	psec_info->skip_size += size;
+}
+
+u32 get_skip_size(struct cudbg_flash_sec_info *psec_info)
+{
+	return psec_info->skip_size;
+}
+
+void set_sector_availability(struct cudbg_flash_sec_info *psec_info,
+							 int sector_nu, int avail)
+{
+	sector_nu -= CUDBG_START_SEC;
+	if (avail)
+		set_dbg_bitmap(psec_info->sec_bitmap, sector_nu);
+	else
+		reset_dbg_bitmap(psec_info->sec_bitmap, sector_nu);
+}
+
+/* This function will return empty sector available for filling */
+int find_empty_sec(struct cudbg_flash_sec_info *psec_info)
+{
+	int i, index, bit;
+
+	for (i = CUDBG_START_SEC; i < CUDBG_SF_MAX_SECTOR; i++) {
+		index = (i - CUDBG_START_SEC) / 8;
+		bit = (i - CUDBG_START_SEC) % 8;
+		if (!(psec_info->sec_bitmap[index] & (1 << bit)))
+			return i;
+	}
+	return CUDBG_STATUS_FLASH_FULL;
+}
+
+/* This function will get header initially. If header is already there
+ * then it will update that header */
+void update_headers(void *handle, struct cudbg_buffer *dbg_buff,
+		    u64 timestamp, u32 cur_entity_hdr_offset,
+		    u32 start_offset, u32 ext_size)
+{
+	void *sec_hdr;
+	struct cudbg_hdr *cudbg_hdr;
+	struct cudbg_flash_hdr *flash_hdr;
+	struct cudbg_entity_hdr *entity_hdr;
+	struct cudbg_flash_sec_info *psec_info;
+	u32 hdr_offset;
+	u32 data_hdr_size;
+	u32 total_hdr_size;
+	u32 sec_hdr_start_addr;
+
+	psec_info = ((struct cudbg_private *)handle)->psec_info;
+	data_hdr_size = CUDBG_MAX_ENTITY * sizeof(struct cudbg_entity_hdr) +
+				sizeof(struct cudbg_hdr);
+	total_hdr_size = data_hdr_size + sizeof(struct cudbg_flash_hdr);
+	sec_hdr_start_addr = CUDBG_SF_SECTOR_SIZE - total_hdr_size;
+	sec_hdr  = psec_info->sec_data + sec_hdr_start_addr;
+
+	flash_hdr = (struct cudbg_flash_hdr *)(sec_hdr);
+	cudbg_hdr = (struct cudbg_hdr *)dbg_buff->data;
+
+	/* initially initialize flash hdr and copy all data headers and
+	 * in next calling (else part) copy only current entity header
+	 */
+	if ((start_offset - psec_info->skip_size) == data_hdr_size) {
+		flash_hdr->signature = CUDBG_FL_SIGNATURE;
+		flash_hdr->major_ver = CUDBG_FL_MAJOR_VERSION;
+		flash_hdr->minor_ver = CUDBG_FL_MINOR_VERSION;
+		flash_hdr->build_ver = CUDBG_FL_BUILD_VERSION;
+		flash_hdr->hdr_len = sizeof(struct cudbg_flash_hdr);
+		hdr_offset =  sizeof(struct cudbg_flash_hdr);
+
+		memcpy((void *)((char *)sec_hdr + hdr_offset),
+		       (void *)((char *)dbg_buff->data), data_hdr_size);
+	} else
+		memcpy((void *)((char *)sec_hdr +
+			sizeof(struct cudbg_flash_hdr) +
+			cur_entity_hdr_offset),
+			(void *)((char *)dbg_buff->data +
+			cur_entity_hdr_offset),
+			sizeof(struct cudbg_entity_hdr));
+
+	hdr_offset = data_hdr_size + sizeof(struct cudbg_flash_hdr);
+	flash_hdr->data_len = cudbg_hdr->data_len - psec_info->skip_size;
+	flash_hdr->timestamp = timestamp;
+
+	entity_hdr = (struct cudbg_entity_hdr *)((char *)sec_hdr +
+		      sizeof(struct cudbg_flash_hdr) +
+		      cur_entity_hdr_offset);
+	/* big entity like mc need to be skipped */
+	entity_hdr->start_offset -= psec_info->skip_size;
+
+	cudbg_hdr = (struct cudbg_hdr *)((char *)sec_hdr +
+			sizeof(struct cudbg_flash_hdr));
+	cudbg_hdr->data_len = flash_hdr->data_len;
+	flash_hdr->data_len += ext_size;
+}
+
+/* Write CUDBG data into serial flash */
+int cudbg_write_flash(void *handle, u64 timestamp, void *data,
+		      u32 start_offset, u32 cur_entity_hdr_offset,
+		      u32 cur_entity_size,
+		      u32 ext_size)
+{
+	struct cudbg_init *cudbg_init = NULL;
+	struct adapter *adap = NULL;
+	struct cudbg_flash_hdr *flash_hdr = NULL;
+	struct cudbg_buffer *dbg_buff = (struct cudbg_buffer *)data;
+	struct cudbg_flash_sec_info *psec_info;
+	struct cudbg_private *context;
+	u32 data_hdr_size;
+	u32 total_hdr_size;
+	u32 tmp_size;
+	u32 sec_data_offset;
+	u32 sec_hdr_start_addr;
+	u32 sec_data_size;
+	u32 space_left;
+	int rc = 0;
+	int sec;
+
+	context = (struct cudbg_private *)handle;
+	cudbg_init = &(context->dbg_init);
+	psec_info = context->psec_info;
+	adap = cudbg_init->adap;
+
+	data_hdr_size = CUDBG_MAX_ENTITY * sizeof(struct cudbg_entity_hdr) +
+			sizeof(struct cudbg_hdr);
+	total_hdr_size = data_hdr_size + sizeof(struct cudbg_flash_hdr);
+	sec_hdr_start_addr = CUDBG_SF_SECTOR_SIZE - total_hdr_size;
+	sec_data_size = sec_hdr_start_addr;
+
+	cudbg_init->print("\tWriting %u bytes to flash\n",
+			  cur_entity_size);
+
+	/* this function will get header if psec_info->sec_data does not
+	 * have any header and
+	 * will update the header if it has header
+	 */
+
+	update_headers(handle, dbg_buff, timestamp,
+		       cur_entity_hdr_offset,
+		       start_offset, ext_size);
+
+	if (ext_size) {
+		cur_entity_size += sizeof(struct cudbg_entity_hdr);
+		start_offset = dbg_buff->offset - cur_entity_size;
+	}
+
+	flash_hdr = (struct cudbg_flash_hdr *)(psec_info->sec_data +
+			sec_hdr_start_addr);
+
+	if (flash_hdr->data_len > CUDBG_FLASH_SIZE) {
+		rc = CUDBG_STATUS_FLASH_FULL;
+		goto out;
+	}
+
+	space_left = CUDBG_FLASH_SIZE - flash_hdr->data_len;
+
+	if (cur_entity_size > space_left) {
+		rc = CUDBG_STATUS_FLASH_FULL;
+		goto out;
+	}
+
+	while (cur_entity_size > 0) {
+		sec = find_empty_sec(psec_info);
+		if (psec_info->par_sec) {
+			sec_data_offset = psec_info->par_sec_offset;
+			set_sector_availability(psec_info, psec_info->par_sec, 0);
+			psec_info->par_sec = 0;
+			psec_info->par_sec_offset = 0;
+
+		} else {
+			psec_info->cur_seq_no++;
+			flash_hdr->sec_seq_no = psec_info->cur_seq_no;
+			sec_data_offset = 0;
+		}
+
+		if (cur_entity_size + sec_data_offset > sec_data_size) {
+			tmp_size = sec_data_size - sec_data_offset;
+		} else {
+			tmp_size = cur_entity_size;
+			psec_info->par_sec = sec;
+			psec_info->par_sec_offset = cur_entity_size +
+						  sec_data_offset;
+		}
+
+		memcpy((void *)((char *)psec_info->sec_data + sec_data_offset),
+		       (void *)((char *)dbg_buff->data + start_offset),
+		       tmp_size);
+
+		rc = write_flash(adap, sec, psec_info->sec_data,
+				CUDBG_SF_SECTOR_SIZE);
+		if (rc)
+			goto out;
+
+		cur_entity_size -= tmp_size;
+		set_sector_availability(psec_info, sec, 1);
+		start_offset += tmp_size;
+	}
+
+out:
+	return rc;
+}
+
+int write_flash(struct adapter *adap, u32 start_sec, void *data, u32 size)
+{
+	unsigned int addr;
+	unsigned int i, n;
+	unsigned int sf_sec_size;
+	int rc = 0;
+	u8 *ptr = (u8 *)data;
+
+	sf_sec_size = adap->params.sf_size/adap->params.sf_nsec;
+	addr =  start_sec * CUDBG_SF_SECTOR_SIZE;
+	i = DIV_ROUND_UP(size,/* # of sectors spanned */
+			sf_sec_size);
+
+	rc = t4_flash_erase_sectors(adap, start_sec,
+		   start_sec + i - 1);
+	/*
+	 * If size == 0 then we're simply erasing the FLASH sectors associated
+	 * with the on-adapter OptionROM Configuration File.
+	 */
+	if (rc || size == 0)
+		goto out;
+
+	/* this will write to the flash up to SF_PAGE_SIZE at a time */
+	for (i = 0; i < size; i += SF_PAGE_SIZE) {
+		if ((size - i) <  SF_PAGE_SIZE)
+			n = size - i;
+		else
+			n = SF_PAGE_SIZE;
+		rc = t4_write_flash(adap, addr, n, ptr, 0);
+		if (rc)
+			goto out;
+
+		addr += n;
+		ptr += n;
+	}
+
+	return 0;
+out:
+	return rc;
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_if.h b/drivers/net/ethernet/chelsio/cxgb4/cudbg_if.h
new file mode 100644
index 00000000..3bba8102
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_if.h
@@ -0,0 +1,452 @@
+/*
+ * Chelsio Unified Debug Interface header file.
+ * Version 1.1
+ */
+#ifndef _CUDBG_IF_H_
+#define _CUDBG_IF_H_
+
+#define ATTRIBUTE_UNUSED __attribute__ ((unused))
+
+#define OUT
+#define IN
+#define INOUT
+
+/* Error codes */
+#define CUDBG_STATUS_SUCCESS		     0
+#define CUDBG_STATUS_NOSPACE		    -2
+#define CUDBG_STATUS_FLASH_WRITE_FAIL	    -3
+#define CUDBG_STATUS_FLASH_READ_FAIL	    -4
+#define CUDBG_STATUS_UNDEFINED_OUT_BUF	    -5
+#define CUDBG_STATUS_UNDEFINED_CBFN	    -6
+#define CUDBG_STATUS_UNDEFINED_PRINTF_CBFN  -7
+#define CUDBG_STATUS_ADAP_INVALID	    -8
+#define CUDBG_STATUS_FLASH_EMPTY	    -9
+#define CUDBG_STATUS_NO_ADAPTER		    -10
+#define CUDBG_STATUS_NO_SIGNATURE	    -11
+#define CUDBG_STATUS_MULTIPLE_REG	    -12
+#define CUDBG_STATUS_UNREGISTERED	    -13
+#define CUDBG_STATUS_UNDEFINED_ENTITY	    -14
+#define CUDBG_STATUS_REG_FAIlED		    -15
+#define CUDBG_STATUS_DEVLOG_FAILED	    -16
+#define CUDBG_STATUS_SMALL_BUFF		    -17
+#define CUDBG_STATUS_CHKSUM_MISSMATCH	    -18
+#define CUDBG_STATUS_NO_SCRATCH_MEM	    -19
+#define CUDBG_STATUS_OUTBUFF_OVERFLOW	    -20
+#define CUDBG_STATUS_INVALID_BUFF	    -21  /* Invalid magic */
+#define CUDBG_STATUS_FILE_OPEN_FAIL	    -22
+#define CUDBG_STATUS_DEVLOG_INT_FAIL	    -23
+#define CUDBG_STATUS_ENTITY_NOT_FOUND	    -24
+#define CUDBG_STATUS_DECOMPRESS_FAIL	    -25
+#define CUDBG_STATUS_BUFFER_SHORT	    -26
+#define CUDBG_METADATA_VERSION_MISMATCH     -27
+#define CUDBG_STATUS_NOT_IMPLEMENTED	    -28
+#define CUDBG_SYSTEM_ERROR		    -29
+#define CUDBG_STATUS_MMAP_FAILED	    -30
+#define CUDBG_STATUS_FILE_WRITE_FAILED	    -31
+#define CUDBG_STATUS_CCLK_NOT_DEFINED	    -32
+#define CUDBG_STATUS_FLASH_FULL            -33
+#define CUDBG_STATUS_SECTOR_EMPTY          -34
+#define CUDBG_STATUS_ENTITY_NOT_REQUESTED  -35
+#define CUDBG_STATUS_NOT_SUPPORTED         -36
+#define CUDBG_STATUS_FILE_READ_FAILED      -37
+
+#define CUDBG_MAJOR_VERSION		    1
+#define CUDBG_MINOR_VERSION		    14
+#define CUDBG_BUILD_VERSION		    0
+
+#define CUDBG_MAX_PARAMS		    16
+
+#define CUDBG_MAX_BITMAP_LEN 16
+
+static char ATTRIBUTE_UNUSED * err_msg[] = {
+	"Success",
+	"Unknown",
+	"No space",
+	"Flash write fail",
+	"Flash read fail",
+	"Undefined out buf",
+	"Callback function undefined",
+	"Print callback function undefined",
+	"ADAP invalid",
+	"Flash empty",
+	"No adapter",
+	"No signature",
+	"Multiple registration",
+	"Unregistered",
+	"Undefined entity",
+	"Reg failed",
+	"Devlog failed",
+	"Small buff",
+	"Checksum mismatch",
+	"No scratch memory",
+	"Outbuff overflow",
+	"Invalid buffer",
+	"File open fail",
+	"Devlog int fail",
+	"Entity not found",
+	"Decompress fail",
+	"Buffer short",
+	"Version mismatch",
+	"Not implemented",
+	"System error",
+	"Mmap failed",
+	"File write failed",
+	"cclk not defined",
+	"Flash full",
+	"Sector empty",
+	"Entity not requested",
+	"Not supported"
+	"File read fail"
+};
+
+enum CUDBG_DBG_ENTITY_TYPE {
+	CUDBG_ALL	   = 0,
+	CUDBG_REG_DUMP	   = 1,
+	CUDBG_DEV_LOG	   = 2,
+	CUDBG_CIM_LA	   = 3,
+	CUDBG_CIM_MA_LA    = 4,
+	CUDBG_CIM_QCFG	   = 5,
+	CUDBG_CIM_IBQ_TP0  = 6,
+	CUDBG_CIM_IBQ_TP1  = 7,
+	CUDBG_CIM_IBQ_ULP  = 8,
+	CUDBG_CIM_IBQ_SGE0 = 9,
+	CUDBG_CIM_IBQ_SGE1 = 10,
+	CUDBG_CIM_IBQ_NCSI = 11,
+	CUDBG_CIM_OBQ_ULP0 = 12,
+	CUDBG_CIM_OBQ_ULP1 = 13,
+	CUDBG_CIM_OBQ_ULP2 = 14,
+	CUDBG_CIM_OBQ_ULP3 = 15,
+	CUDBG_CIM_OBQ_SGE  = 16,
+	CUDBG_CIM_OBQ_NCSI = 17,
+	CUDBG_EDC0	   = 18,
+	CUDBG_EDC1	   = 19,
+	CUDBG_MC0	   = 20,
+	CUDBG_MC1	   = 21,
+	CUDBG_RSS	   = 22,
+	CUDBG_RSS_PF_CONF  = 23,
+	CUDBG_RSS_KEY	   = 24,
+	CUDBG_RSS_VF_CONF  = 25,
+	CUDBG_RSS_CONF	   = 26,
+	CUDBG_PATH_MTU	   = 27,
+	CUDBG_SW_STATE	   = 28,
+	CUDBG_WTP	   = 29,
+	CUDBG_PM_STATS	   = 30,
+	CUDBG_HW_SCHED	   = 31,
+	CUDBG_TCP_STATS    = 32,
+	CUDBG_TP_ERR_STATS = 33,
+	CUDBG_FCOE_STATS   = 34,
+	CUDBG_RDMA_STATS   = 35,
+	CUDBG_TP_INDIRECT  = 36,
+	CUDBG_SGE_INDIRECT = 37,
+	CUDBG_CPL_STATS    = 38,
+	CUDBG_DDP_STATS    = 39,
+	CUDBG_WC_STATS	   = 40,
+	CUDBG_ULPRX_LA	   = 41,
+	CUDBG_LB_STATS	   = 42,
+	CUDBG_TP_LA	   = 43,
+	CUDBG_MEMINFO	   = 44,
+	CUDBG_CIM_PIF_LA   = 45,
+	CUDBG_CLK	   = 46,
+	CUDBG_CIM_OBQ_RXQ0 = 47,
+	CUDBG_CIM_OBQ_RXQ1 = 48,
+	CUDBG_MAC_STATS    = 49,
+	CUDBG_PCIE_INDIRECT = 50,
+	CUDBG_PM_INDIRECT  = 51,
+	CUDBG_FULL	   = 52,
+	CUDBG_TX_RATE	   = 53,
+	CUDBG_TID_INFO	   = 54,
+	CUDBG_PCIE_CONFIG  = 55,
+	CUDBG_DUMP_CONTEXT = 56,
+	CUDBG_MPS_TCAM	   = 57,
+	CUDBG_VPD_DATA	   = 58,
+	CUDBG_LE_TCAM	   = 59,
+	CUDBG_CCTRL	   = 60,
+	CUDBG_MA_INDIRECT  = 61,
+	CUDBG_ULPTX_LA	   = 62,
+	CUDBG_EXT_ENTITY   = 63,
+	CUDBG_UP_CIM_INDIRECT = 64,
+	CUDBG_PBT_TABLE    = 65,
+	CUDBG_MBOX_LOG     = 66,
+	CUDBG_HMA_INDIRECT = 67,
+	CUDBG_HMA          = 68,
+	CUDBG_UPLOAD       = 69,
+	CUDBG_MAX_ENTITY   = 70,
+};
+
+#define ENTITY_FLAG_NULL 0
+#define ENTITY_FLAG_REGISTER 1
+#define ENTITY_FLAG_BINARY 2
+#define ENTITY_FLAG_FW_NO_ATTACH    3
+
+/* file_name matches Linux cxgb4 debugfs entry names. */
+struct el {char *name; char *file_name; int bit; u32 flag; };
+static struct el ATTRIBUTE_UNUSED entity_list[] = {
+	{"all", "all", CUDBG_ALL, ENTITY_FLAG_NULL},
+	{"regdump", "regdump", CUDBG_REG_DUMP, 1 << ENTITY_FLAG_REGISTER},
+	/* {"reg", CUDBG_REG_DUMP},*/
+	{"devlog", "devlog", CUDBG_DEV_LOG, ENTITY_FLAG_NULL},
+	{"cimla", "cim_la", CUDBG_CIM_LA, ENTITY_FLAG_NULL},
+	{"cimmala", "cim_ma_la", CUDBG_CIM_MA_LA, ENTITY_FLAG_NULL},
+	{"cimqcfg", "cim_qcfg", CUDBG_CIM_QCFG, ENTITY_FLAG_NULL},
+	{"ibqtp0", "ibq_tp0", CUDBG_CIM_IBQ_TP0, ENTITY_FLAG_NULL},
+	{"ibqtp1", "ibq_tp1", CUDBG_CIM_IBQ_TP1, ENTITY_FLAG_NULL},
+	{"ibqulp", "ibq_ulp", CUDBG_CIM_IBQ_ULP, ENTITY_FLAG_NULL},
+	{"ibqsge0", "ibq_sge0", CUDBG_CIM_IBQ_SGE0, ENTITY_FLAG_NULL},
+	{"ibqsge1", "ibq_sge1", CUDBG_CIM_IBQ_SGE1, ENTITY_FLAG_NULL},
+	{"ibqncsi", "ibq_ncsi", CUDBG_CIM_IBQ_NCSI, ENTITY_FLAG_NULL},
+	{"obqulp0", "obq_ulp0", CUDBG_CIM_OBQ_ULP0, ENTITY_FLAG_NULL},
+	/* {"cimobqulp1", CUDBG_CIM_OBQ_ULP1},*/
+	{"obqulp1", "obq_ulp1", CUDBG_CIM_OBQ_ULP1, ENTITY_FLAG_NULL},
+	{"obqulp2", "obq_ulp2", CUDBG_CIM_OBQ_ULP2, ENTITY_FLAG_NULL},
+	{"obqulp3", "obq_ulp3", CUDBG_CIM_OBQ_ULP3, ENTITY_FLAG_NULL},
+	{"obqsge", "obq_sge", CUDBG_CIM_OBQ_SGE, ENTITY_FLAG_NULL},
+	{"obqncsi", "obq_ncsi", CUDBG_CIM_OBQ_NCSI, ENTITY_FLAG_NULL},
+	{"edc0", "edc0", CUDBG_EDC0, (1 << ENTITY_FLAG_BINARY)},
+	{"edc1", "edc1", CUDBG_EDC1, (1 << ENTITY_FLAG_BINARY)},
+	{"mc0", "mc0", CUDBG_MC0, (1 << ENTITY_FLAG_BINARY)},
+	{"mc1", "mc1", CUDBG_MC1, (1 << ENTITY_FLAG_BINARY)},
+	{"rss", "rss", CUDBG_RSS, ENTITY_FLAG_NULL},
+	{"rss_pf_config", "rss_pf_config", CUDBG_RSS_PF_CONF, ENTITY_FLAG_NULL},
+	{"rss_key", "rss_key", CUDBG_RSS_KEY, ENTITY_FLAG_NULL},
+	{"rss_vf_config", "rss_vf_config", CUDBG_RSS_VF_CONF, ENTITY_FLAG_NULL},
+	{"rss_config", "rss_config", CUDBG_RSS_CONF, ENTITY_FLAG_NULL},
+	{"pathmtu", "path_mtus", CUDBG_PATH_MTU, ENTITY_FLAG_NULL},
+	{"swstate", "sw_state", CUDBG_SW_STATE, ENTITY_FLAG_NULL},
+	{"wtp", "wtp", CUDBG_WTP, ENTITY_FLAG_NULL},
+	{"pmstats", "pm_stats", CUDBG_PM_STATS, ENTITY_FLAG_NULL},
+	{"hwsched", "hw_sched", CUDBG_HW_SCHED, ENTITY_FLAG_NULL},
+	{"tcpstats", "tcp_stats", CUDBG_TCP_STATS, ENTITY_FLAG_NULL},
+	{"tperrstats", "tp_err_stats", CUDBG_TP_ERR_STATS, ENTITY_FLAG_NULL},
+	{"fcoestats", "fcoe_stats", CUDBG_FCOE_STATS, ENTITY_FLAG_NULL},
+	{"rdmastats", "rdma_stats", CUDBG_RDMA_STATS, ENTITY_FLAG_NULL},
+	{"tpindirect", "tp_indirect", CUDBG_TP_INDIRECT,
+					1 << ENTITY_FLAG_REGISTER},
+	{"sgeindirect", "sge_indirect", CUDBG_SGE_INDIRECT,
+					1 << ENTITY_FLAG_REGISTER},
+	{"cplstats", "cpl_stats", CUDBG_CPL_STATS, ENTITY_FLAG_NULL},
+	{"ddpstats", "ddp_stats", CUDBG_DDP_STATS, ENTITY_FLAG_NULL},
+	{"wcstats", "wc_stats", CUDBG_WC_STATS, ENTITY_FLAG_NULL},
+	{"ulprxla", "ulprx_la", CUDBG_ULPRX_LA, ENTITY_FLAG_NULL},
+	{"lbstats", "lb_stats", CUDBG_LB_STATS, ENTITY_FLAG_NULL},
+	{"tpla", "tp_la", CUDBG_TP_LA, ENTITY_FLAG_NULL},
+	{"meminfo", "meminfo", CUDBG_MEMINFO, ENTITY_FLAG_NULL},
+	{"cimpifla", "cim_pif_la", CUDBG_CIM_PIF_LA, ENTITY_FLAG_NULL},
+	{"clk", "clk", CUDBG_CLK, ENTITY_FLAG_NULL},
+	{"obq_sge_rx_q0", "obq_sge_rx_q0", CUDBG_CIM_OBQ_RXQ0,
+					ENTITY_FLAG_NULL},
+	{"obq_sge_rx_q1", "obq_sge_rx_q1", CUDBG_CIM_OBQ_RXQ1,
+					ENTITY_FLAG_NULL},
+	{"macstats", "mac_stats", CUDBG_MAC_STATS, ENTITY_FLAG_NULL},
+	{"pcieindirect", "pcie_indirect", CUDBG_PCIE_INDIRECT,
+					1 << ENTITY_FLAG_REGISTER},
+	{"pmindirect", "pm_indirect", CUDBG_PM_INDIRECT,
+				1 << ENTITY_FLAG_REGISTER},
+	{"full", "full", CUDBG_FULL, ENTITY_FLAG_NULL},
+	{"txrate", "tx_rate", CUDBG_TX_RATE, ENTITY_FLAG_NULL},
+	{"tidinfo", "tids", CUDBG_TID_INFO, ENTITY_FLAG_NULL |
+				(1 << ENTITY_FLAG_FW_NO_ATTACH)},
+	{"pcieconfig", "pcie_config", CUDBG_PCIE_CONFIG, ENTITY_FLAG_NULL},
+	{"dumpcontext", "dump_context", CUDBG_DUMP_CONTEXT, ENTITY_FLAG_NULL},
+	{"mpstcam", "mps_tcam", CUDBG_MPS_TCAM, ENTITY_FLAG_NULL},
+	{"vpddata", "vpd_data", CUDBG_VPD_DATA, ENTITY_FLAG_NULL},
+	{"letcam", "le_tcam", CUDBG_LE_TCAM, ENTITY_FLAG_NULL},
+	{"cctrl", "cctrl", CUDBG_CCTRL, ENTITY_FLAG_NULL},
+	{"maindirect", "ma_indirect", CUDBG_MA_INDIRECT,
+				1 << ENTITY_FLAG_REGISTER},
+	{"ulptxla", "ulptx_la", CUDBG_ULPTX_LA, ENTITY_FLAG_NULL},
+	{"extentity", "ext_entity", CUDBG_EXT_ENTITY, ENTITY_FLAG_NULL},
+	{"upcimindirect", "up_cim_indirect", CUDBG_UP_CIM_INDIRECT,
+					1 << ENTITY_FLAG_REGISTER},
+	{"pbttables", "pbt_tables", CUDBG_PBT_TABLE, ENTITY_FLAG_NULL},
+	{"mboxlog", "mboxlog", CUDBG_MBOX_LOG, ENTITY_FLAG_NULL},
+	{"hmaindirect", "hma_indirect", CUDBG_HMA_INDIRECT,
+				1 << ENTITY_FLAG_REGISTER},
+	{"hma", "hma", CUDBG_HMA, (1 << ENTITY_FLAG_BINARY)},
+	{"upload", "upload", CUDBG_UPLOAD, ENTITY_FLAG_NULL},
+};
+
+typedef int (*cudbg_print_cb) (char *str, ...);
+
+struct cudbg_init_hdr {
+	u8   major_ver;
+	u8   minor_ver;
+	u8   build_ver;
+	u8   res;
+	u16  init_struct_size;
+};
+
+struct cudbg_flash_hdr {
+	u32 signature;
+	u8 major_ver;
+	u8 minor_ver;
+	u8 build_ver;
+	u8 res;
+	u64 timestamp;
+	u64 time_res;
+	u32 hdr_len;
+	u32 data_len;
+	u32 hdr_flags;
+	u32 sec_seq_no;
+	u32 reserved[22];
+};
+
+struct cudbg_param {
+	u16			 param_type;
+	u16			 reserved;
+	union {
+		struct {
+			u32 memtype;	/* which memory (EDC0, EDC1, MC) */
+			u32 start;	/* start of log in firmware memory */
+			u32 size;	/* size of log */
+		} devlog_param;
+		struct {
+			struct mbox_cmd_log *log;
+			u16 mbox_cmds;
+		} mboxlog_param;
+		struct {
+			u8 caller_string[100];
+			u8 os_type;
+		} sw_state_param;
+		u64 time;
+		u8 tcb_bit_param;
+		void *tcb_adap;
+	} u;
+};
+
+/*
+ * * What is OFFLINE_VIEW_ONLY mode?
+ *
+ * cudbg frame work will be used only to interpret previously collected
+ * data store in a file (i.e NOT hw flash)
+ */
+
+struct cudbg_init {
+	struct cudbg_init_hdr	 header;
+	cudbg_print_cb		 print;		 /* Platform dependent print
+						    function */
+	u32			 verbose:1;	 /* Turn on verbose print */
+	u32			 use_flash:1;	 /* Use flash to collect or view
+						    debug */
+	u32			 full_mode:1;	 /* If set, cudbg will pull in
+						    common code */
+	u32			 no_compress:1;  /* Dont compress will storing
+						    the collected debug */
+	u32			 info:1;	 /* Show just the info, Dont
+						    interpret */
+	u32			 reserved:27;
+	u8			 dbg_bitmap[CUDBG_MAX_BITMAP_LEN];
+						/* Bit map to select the dbg
+						    data type to be collect
+						    or viewed */
+	void			 *sw_state_buf;		/* */
+	u32			 sw_state_buflen;	  /* */
+
+	unsigned char		 *hash_table; /* hash table used in
+					       * fastlz compression */
+	/* Optional for OFFLINE_VIEW_ONLY mode. Set to NULL for
+	 * OFFLINE_VIEW_ONLY mode */
+	struct adapter		 *adap;		 /* Pointer to adapter structure
+						    with filled fields */
+	u16		   dbg_params_cnt;
+	u16		   dbg_reserved;
+	struct cudbg_param dbg_params[CUDBG_MAX_PARAMS];
+};
+
+enum {
+	CUDBG_DEVLOG_PARAM = 1,
+	CUDBG_TIMESTAMP_PARAM = 2,
+	CUDBG_FW_NO_ATTACH_PARAM = 3,
+	CUDBG_MBOX_LOG_PARAM = 4,
+	CUDBG_GET_PAYLOAD_PARAM = 7,
+	CUDBG_SW_STATE_PARAM = 8,
+};
+
+enum {
+	/* params for os_type */
+	CUDBG_OS_TYPE_LINUX = 2,
+	CUDBG_OS_TYPE_UNKNOWN = 4,
+};
+
+/********************************* Helper functions *************************/
+static inline void set_dbg_bitmap(u8 *bitmap, enum CUDBG_DBG_ENTITY_TYPE type)
+{
+	int index = type / 8;
+	int bit = type % 8;
+
+	bitmap[index] |= (1 << bit);
+}
+
+static inline void reset_dbg_bitmap(u8 *bitmap, enum CUDBG_DBG_ENTITY_TYPE type)
+{
+	int index = type / 8;
+	int bit = type % 8;
+
+	bitmap[index] &= ~(1 << bit);
+}
+
+static inline void init_cudbg_hdr(struct cudbg_init_hdr *hdr)
+{
+	hdr->major_ver = CUDBG_MAJOR_VERSION;
+	hdr->minor_ver = CUDBG_MINOR_VERSION;
+	hdr->build_ver = CUDBG_BUILD_VERSION;
+	hdr->init_struct_size = sizeof(struct cudbg_init);
+}
+
+/**************************** End of Helper functions *************************/
+
+/* API Prototypes */
+
+/**
+ *  cudbg_hello - To initialize cudbg framework. Needs to called
+ *  first before calling anyother function
+ *  ## Parameters ##
+ *  @dbg_init : A pointer to cudbg_init structure.
+ *  @handle : A pointer to void
+ *  ##	Return ##
+ *  If the function succeeds, returns 0 and a handle will be copied to @handle.
+ *  -ve value represent error.
+ */
+int cudbg_hello(IN struct cudbg_init *dbg_init, OUT void **handle);
+
+/*
+ *  cudbg_hello2 - Extended cudbg_hello. Caller has provide required memory
+ *                 buffer for library initialization.
+ *  ## Parameters ##
+ *  @dbg_init : Pointer to cudbg_init structure.
+ *  @handle : Pointer to the handle that will be returned by cudbglib.
+ *  @buf : Pointer to the buffer, for the use of cudbglib.
+ *  @buf_size : Pointer to the variable containing the size of buffer.
+ *              Cudbglib  sets the size of the required buffer if
+ *              CUDBG_STATUS_SMALL_BUFF is returned.
+ *  ##   Return ##
+ *  If the function succeeds, returns 0.
+ *  -ve value represent error.
+
+ * Caller can first pass buf_size as 0, to find the size of buffer required by cudbglib. Then
+ * call cudbg_hello2() with correct buf and buf_size, after buffer allocation.
+ */
+int cudbg_hello2(IN struct cudbg_init *dbg_init, OUT void **handle, IN u8 *buf,
+		 INOUT u32 *buf_size);
+
+/**
+ *  cudbg_collect - To collect and store debug information.
+ *  ## Parameters ##
+ *  @handle : A pointer returned by cudbg_init.
+ *  @outbuf : pointer to output buffer, to store the collected information
+ *	      or to use it as a scratch buffer in case HW flash is used to
+ *	      store the debug information.
+ *  @outbuf_size : Size of output buffer.
+ *  ##	Return ##
+ *  If the function succeeds, the return value will be size of debug information
+ *  collected and stored.
+ *  -ve value represent error.
+ */
+int cudbg_collect(IN void *handle, OUT void *outbuf, INOUT u32 *outbuf_size);
+
+/**
+ *  cudbg_bye - To exit cudbg framework.
+ *  ## Parameters ##
+ *  @handle : A pointer returned by cudbg_hello.
+ */
+int cudbg_bye(IN void *handle);
+#endif /* _CUDBG_IF_H_ */
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.c b/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.c
new file mode 100644
index 00000000..1ad52cf1
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.c
@@ -0,0 +1,3603 @@
+#include "t4_regs.h"
+
+#include "t4_hw.h"
+#include "t4_chip_type.h"
+#include "t4fw_api.h"
+#include "cxgb4.h"
+
+#include "cudbg_if.h"
+#include "cudbg_lib_common.h"
+#include "cudbg_lib.h"
+#include "cudbg_entity.h"
+
+#include "cudbg_utls.c"
+
+#define  BUFFER_WARN_LIMIT 10000000
+
+#define GET_SCRATCH_BUFF(dbg_buff, size, scratch_buff) \
+do { \
+	rc = get_scratch_buff(dbg_buff, size, scratch_buff); \
+	if (rc) \
+		return rc; \
+} while (0)
+
+#define WRITE_AND_COMPRESS_SCRATCH_BUFF(scratch_buff, dbg_buff) \
+do { \
+	rc = write_compression_hdr(scratch_buff, dbg_buff); \
+	if (rc) \
+		goto err1; \
+	rc = compress_buff(pdbg_init->hash_table, scratch_buff, dbg_buff); \
+} while (0)
+
+#define WRITE_AND_RELEASE_SCRATCH_BUFF(scratch_buff, dbg_buff) \
+do { \
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(scratch_buff, dbg_buff); \
+err1: \
+	release_scratch_buff(scratch_buff, dbg_buff); \
+} while (0)
+
+int is_fw_attached(struct cudbg_init *pdbg_init)
+{
+	if (pdbg_init->dbg_params[CUDBG_FW_NO_ATTACH_PARAM].param_type ==
+	    CUDBG_FW_NO_ATTACH_PARAM)
+		return 0;
+	return 1;
+}
+
+/* This function will add additional padding bytes into debug_buffer to make it
+ * 4 byte aligned.*/
+void align_debug_buffer(struct cudbg_buffer *dbg_buff,
+			struct cudbg_entity_hdr *entity_hdr)
+{
+	u8 zero_buf[4] = {0};
+	u8 padding, remain;
+
+	remain = (dbg_buff->offset - entity_hdr->start_offset) % 4;
+	padding = 4 - remain;
+	if (remain) {
+		memcpy(((u8 *) dbg_buff->data) + dbg_buff->offset, &zero_buf,
+		       padding);
+		dbg_buff->offset += padding;
+		entity_hdr->num_pad = padding;
+	}
+	entity_hdr->size = dbg_buff->offset - entity_hdr->start_offset;
+}
+
+static void read_sge_ctxt(struct cudbg_init *pdbg_init, u32 cid,
+			  enum ctxt_type ctype, u32 *data)
+{
+	struct adapter *padap = pdbg_init->adap;
+	int rc = -1;
+
+	if (is_fw_attached(pdbg_init))
+		rc = t4_sge_ctxt_rd(padap, padap->mbox, cid, ctype,
+				    data);
+	if (rc)
+		t4_sge_ctxt_rd_bd(padap, cid, ctype, data);
+}
+
+int get_next_ext_entity_hdr(void *outbuf, u32 *ext_size,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_entity_hdr **entity_hdr)
+{
+	struct cudbg_hdr *cudbg_hdr = (struct cudbg_hdr *)outbuf;
+	u32 ext_offset = cudbg_hdr->data_len;
+	int rc = 0;
+
+	*ext_size = 0;
+	if (dbg_buff->size - dbg_buff->offset <=
+		 sizeof(struct cudbg_entity_hdr)) {
+		rc = CUDBG_STATUS_BUFFER_SHORT;
+		goto err;
+	}
+
+	*entity_hdr = (struct cudbg_entity_hdr *)
+		       ((char *)outbuf + cudbg_hdr->data_len);
+	/* Find the last extended entity header */
+	while ((*entity_hdr)->size) {
+
+		ext_offset += sizeof(struct cudbg_entity_hdr) +
+				     (*entity_hdr)->size;
+		*ext_size += (*entity_hdr)->size +
+			      sizeof(struct cudbg_entity_hdr);
+		if (dbg_buff->size - dbg_buff->offset + *ext_size  <=
+			sizeof(struct cudbg_entity_hdr)) {
+			rc = CUDBG_STATUS_BUFFER_SHORT;
+			goto err;
+		}
+
+		if (*ext_size != (*entity_hdr)->next_ext_offset) {
+			ext_offset -= sizeof(struct cudbg_entity_hdr) +
+				     (*entity_hdr)->size;
+			*ext_size -= (*entity_hdr)->size +
+				     sizeof(struct cudbg_entity_hdr);
+			break;
+		}
+		(*entity_hdr)->next_ext_offset = *ext_size;
+		*entity_hdr = (struct cudbg_entity_hdr *)
+					   ((char *)outbuf +
+					   ext_offset);
+	}
+	/* update the data offset */
+	dbg_buff->offset = ext_offset;
+err:
+	return rc;
+}
+
+int wr_entity_to_flash(void *handle, struct cudbg_buffer *dbg_buff,
+		       u32 cur_entity_data_offset,
+		       u32 cur_entity_size,
+		       int entity_nu, u32 ext_size)
+{
+	u32 cur_entity_hdr_offset = sizeof(struct cudbg_hdr);
+	struct cudbg_init *cudbg_init = NULL;
+	struct cudbg_flash_sec_info *psec_info;
+	struct cudbg_private *context;
+	u32 remain_flash_size;
+	u32 flash_data_offset;
+	u32 data_hdr_size;
+	u64 timestamp;
+	int rc = -1;
+
+	context = (struct cudbg_private *)handle;
+	cudbg_init = &(context->dbg_init);
+	psec_info = context->psec_info;
+
+	data_hdr_size = CUDBG_MAX_ENTITY * sizeof(struct cudbg_entity_hdr) +
+			sizeof(struct cudbg_hdr);
+	flash_data_offset = (FLASH_CUDBG_NSECS *
+			     (sizeof(struct cudbg_flash_hdr) +
+			      data_hdr_size)) +
+			    (cur_entity_data_offset - data_hdr_size) -
+			    get_skip_size(psec_info);
+
+	if (flash_data_offset > CUDBG_FLASH_SIZE) {
+		update_skip_size(psec_info, cur_entity_size);
+		cudbg_init->print("Large entity skipping...\n");
+		return rc;
+	}
+
+	remain_flash_size = CUDBG_FLASH_SIZE - flash_data_offset;
+	if (cur_entity_size > remain_flash_size) {
+		update_skip_size(psec_info, cur_entity_size);
+		cudbg_init->print("Large entity skipping...\n");
+	} else {
+		timestamp =
+			cudbg_init->dbg_params[CUDBG_TIMESTAMP_PARAM].
+			u.time;
+
+		cur_entity_hdr_offset +=
+			(sizeof(struct cudbg_entity_hdr) *
+			(entity_nu - 1));
+
+		rc = cudbg_write_flash(handle, timestamp, dbg_buff,
+				       cur_entity_data_offset,
+				       cur_entity_hdr_offset,
+				       cur_entity_size,
+				       ext_size);
+		if (rc == CUDBG_STATUS_FLASH_FULL)
+			cudbg_init->print("\n\tFLASH is full... "
+				"can not write in flash more\n\n");
+	}
+	return rc;
+}
+
+int cudbg_collect(void *handle, void *outbuf, u32 *outbuf_size)
+{
+	struct cudbg_private *context = (struct cudbg_private *)handle;
+	struct cudbg_init *cudbg_init = &(context->dbg_init);
+	struct cudbg_entity_hdr *ext_entity_hdr = NULL;
+	u8 *dbg_bitmap = context->dbg_init.dbg_bitmap;
+	int large_entity_code, large_entity_list_size;
+	struct cudbg_entity_hdr *entity_hdr = NULL;
+	struct large_entity large_entity_list[] = {
+		{CUDBG_EDC0, 0, 0},
+		{CUDBG_EDC1, 0, 0},
+		{CUDBG_MC0, 0, 0},
+		{CUDBG_MC1, 0, 0}
+	};
+	struct adapter *padap = cudbg_init->adap;
+	struct cudbg_param *dbg_param = NULL;
+	struct cudbg_error cudbg_err = {0};
+	u32 total_size, remaining_buf_size;
+	int j, entity_priority_list_size;
+	int index, bit, i, rc = -1, all;
+	struct cudbg_buffer dbg_buff;
+	struct cudbg_hdr *cudbg_hdr;
+	bool flag_ext = 0;
+	u32 ext_size = 0;
+
+	large_entity_list_size = ARRAY_SIZE(large_entity_list);
+	reset_skip_entity(large_entity_list, large_entity_list_size);
+
+	dbg_param = &cudbg_init->dbg_params[CUDBG_FW_NO_ATTACH_PARAM];
+
+	dbg_buff.data = outbuf;
+	dbg_buff.size = *outbuf_size;
+	dbg_buff.offset = 0;
+
+	cudbg_hdr = (struct cudbg_hdr *)dbg_buff.data;
+	cudbg_hdr->signature = CUDBG_SIGNATURE;
+	cudbg_hdr->hdr_len = sizeof(struct cudbg_hdr);
+	cudbg_hdr->major_ver = CUDBG_MAJOR_VERSION;
+	cudbg_hdr->minor_ver = CUDBG_MINOR_VERSION;
+	cudbg_hdr->max_entities = CUDBG_MAX_ENTITY;
+	cudbg_hdr->chip_ver = padap->params.chip;
+	if (cudbg_hdr->data_len)
+		flag_ext = 1;
+
+	if (cudbg_init->use_flash) {
+		rc = get_flash_params(padap);
+		if (rc) {
+			cudbg_init->print("\nGet flash params failed.\n\n");
+			cudbg_init->use_flash = 0;
+		}
+
+		/* Timestamp is mandatory. If it is not passed then disable
+		 * flash support
+		 */
+		if (!cudbg_init->dbg_params[CUDBG_TIMESTAMP_PARAM].u.time) {
+			cudbg_init->print("\nTimestamp param missing,"
+					  "so ignoring flash write request\n\n");
+			cudbg_init->use_flash = 0;
+		}
+	}
+
+	if (sizeof(struct cudbg_entity_hdr) * CUDBG_MAX_ENTITY >
+	    dbg_buff.size) {
+		rc = CUDBG_STATUS_SMALL_BUFF;
+		total_size = cudbg_hdr->hdr_len;
+		goto err;
+	}
+
+	/* If ext flag is set then move the offset to the end of the buf
+	 * so that we can add ext entities
+	 */
+	if (flag_ext) {
+		ext_entity_hdr = (struct cudbg_entity_hdr *)
+			      ((char *)outbuf + cudbg_hdr->hdr_len +
+			      (sizeof(struct cudbg_entity_hdr) *
+			      (CUDBG_EXT_ENTITY - 1)));
+		ext_entity_hdr->start_offset = cudbg_hdr->data_len;
+		ext_entity_hdr->entity_type = CUDBG_EXT_ENTITY;
+		ext_entity_hdr->size = 0;
+		dbg_buff.offset = cudbg_hdr->data_len;
+	} else {
+		dbg_buff.offset += cudbg_hdr->hdr_len; /* move 24 bytes*/
+		dbg_buff.offset += CUDBG_MAX_ENTITY *
+					sizeof(struct cudbg_entity_hdr);
+	}
+
+	total_size = dbg_buff.offset;
+	all = dbg_bitmap[0] & (1 << CUDBG_ALL);
+
+	entity_priority_list_size = sizeof(entity_priority_list) / sizeof(int);
+	/* entity_priority_list_size does not include CUDBG_ALL so
+	 * entity_priority_list_size + 1 */
+	if (entity_priority_list_size != (CUDBG_MAX_ENTITY - 1))
+		cudbg_init->print("WARNING: CUDBG_MAX_ENTITY(%d) and "\
+				  "entity_priority_list size(%d) mismatch\n",
+				  CUDBG_MAX_ENTITY,
+				  entity_priority_list_size + 1);
+
+	for( j = 0; j < entity_priority_list_size; j++) {
+		i = entity_priority_list[j];
+		index = i / 8;
+		bit = i % 8;
+
+		if (entity_list[i].bit == CUDBG_EXT_ENTITY)
+			continue;
+
+		if (all || (dbg_bitmap[index] & (1 << bit))) {
+
+			if (!flag_ext) {
+				rc = get_entity_hdr(outbuf, i, dbg_buff.size,
+						    &entity_hdr);
+				if (rc)
+					cudbg_hdr->hdr_flags = rc;
+			} else {
+				rc = get_next_ext_entity_hdr(outbuf, &ext_size,
+							     &dbg_buff,
+							     &entity_hdr);
+				if (rc)
+					goto err;
+
+				/* move the offset after the ext header */
+				dbg_buff.offset +=
+					sizeof(struct cudbg_entity_hdr);
+			}
+
+			entity_hdr->entity_type = i;
+			entity_hdr->start_offset = dbg_buff.offset;
+			/* process each entity by calling process_entity fp */
+			remaining_buf_size = dbg_buff.size - dbg_buff.offset;
+			if ((remaining_buf_size <= BUFFER_WARN_LIMIT) &&
+			    is_large_entity(large_entity_list,
+					    large_entity_list_size, i)) {
+				cudbg_init->print("Skipping %s\n",
+						  entity_list[i].name);
+				skip_entity(large_entity_list,
+					    large_entity_list_size, i);
+				continue;
+			} else {
+
+				/* If fw_attach is 0, then skip entities which
+				 * communicates with firmware
+				 */
+
+				if (dbg_param->param_type ==
+				    CUDBG_FW_NO_ATTACH_PARAM &&
+				    (entity_list[i].flag &
+				    (1 << ENTITY_FLAG_FW_NO_ATTACH))) {
+					cudbg_init->print("Skipping %s entity,"\
+							  "because fw_attach "\
+							  "is 0\n",
+							  entity_list[i].name);
+					continue;
+				}
+				cudbg_init->print("collecting debug entity: "\
+						  "%s\n", entity_list[i].name);
+				memset(&cudbg_err, 0,
+				       sizeof(struct cudbg_error));
+				rc = process_entity[i-1](cudbg_init, &dbg_buff,
+							 &cudbg_err);
+			}
+
+			if (rc) {
+				entity_hdr->size = 0;
+				dbg_buff.offset = entity_hdr->start_offset;
+			} else
+				align_debug_buffer(&dbg_buff, entity_hdr);
+
+			if (cudbg_err.sys_err)
+				rc = CUDBG_SYSTEM_ERROR;
+
+			entity_hdr->hdr_flags =  rc;
+			entity_hdr->sys_err = cudbg_err.sys_err;
+			entity_hdr->sys_warn =	cudbg_err.sys_warn;
+
+			/* We don't want to include ext entity size in global
+			 * header
+			 */
+			if (!flag_ext)
+				total_size += entity_hdr->size;
+
+			cudbg_hdr->data_len = total_size;
+			*outbuf_size = total_size;
+
+			/* consider the size of the ext entity header and data
+			 * also
+			 */
+			if (flag_ext) {
+				ext_size += (sizeof(struct cudbg_entity_hdr) +
+					     entity_hdr->size);
+				entity_hdr->start_offset -= cudbg_hdr->data_len;
+				ext_entity_hdr->size = ext_size;
+				entity_hdr->next_ext_offset = ext_size;
+				entity_hdr->flag |= CUDBG_EXT_DATA_VALID;
+			}
+
+			if (cudbg_init->use_flash) {
+				if (flag_ext) {
+					wr_entity_to_flash(handle,
+							   &dbg_buff,
+							   ext_entity_hdr->
+							   start_offset,
+							   entity_hdr->
+							   size,
+							   CUDBG_EXT_ENTITY,
+							   ext_size);
+				}
+				else
+					wr_entity_to_flash(handle,
+							   &dbg_buff,
+							   entity_hdr->\
+							   start_offset,
+							   entity_hdr->size,
+							   i, ext_size);
+			}
+		}
+	}
+
+	for (i = 0; i < sizeof(large_entity_list) / sizeof(struct large_entity);
+	     i++) {
+		large_entity_code = large_entity_list[i].entity_code;
+		if (large_entity_list[i].skip_flag) {
+			if (!flag_ext) {
+				rc = get_entity_hdr(outbuf, large_entity_code,
+						    dbg_buff.size, &entity_hdr);
+				if (rc)
+					cudbg_hdr->hdr_flags = rc;
+			} else {
+				rc = get_next_ext_entity_hdr(outbuf, &ext_size,
+							     &dbg_buff,
+							     &entity_hdr);
+				if (rc)
+					goto err;
+
+				dbg_buff.offset +=
+					sizeof(struct cudbg_entity_hdr);
+			}
+
+			/* If fw_attach is 0, then skip entities which
+			 * communicates with firmware
+			 */
+
+			if (dbg_param->param_type ==
+			    CUDBG_FW_NO_ATTACH_PARAM &&
+			    (entity_list[large_entity_code].flag &
+			    (1 << ENTITY_FLAG_FW_NO_ATTACH))) {
+				cudbg_init->print("Skipping %s entity,"\
+						  "because fw_attach "\
+						  "is 0\n",
+						  entity_list[large_entity_code]
+						  .name);
+				continue;
+			}
+
+			entity_hdr->entity_type = large_entity_code;
+			entity_hdr->start_offset = dbg_buff.offset;
+			cudbg_init->print("Re-trying debug entity: %s\n",
+					  entity_list[large_entity_code].name);
+
+			memset(&cudbg_err, 0, sizeof(struct cudbg_error));
+			rc = process_entity[large_entity_code - 1](cudbg_init,
+								   &dbg_buff,
+								   &cudbg_err);
+			if (rc) {
+				entity_hdr->size = 0;
+				dbg_buff.offset = entity_hdr->start_offset;
+			} else
+				align_debug_buffer(&dbg_buff, entity_hdr);
+
+			if (cudbg_err.sys_err)
+				rc = CUDBG_SYSTEM_ERROR;
+
+			entity_hdr->hdr_flags = rc;
+			entity_hdr->sys_err = cudbg_err.sys_err;
+			entity_hdr->sys_warn =	cudbg_err.sys_warn;
+
+			/* We don't want to include ext entity size in global
+			 * header
+			 */
+			if (!flag_ext)
+				total_size += entity_hdr->size;
+
+			cudbg_hdr->data_len = total_size;
+			*outbuf_size = total_size;
+
+			/* consider the size of the ext entity header and
+			 * data also
+			 */
+			if (flag_ext) {
+				ext_size += (sizeof(struct cudbg_entity_hdr) +
+						   entity_hdr->size);
+				entity_hdr->start_offset -=
+							cudbg_hdr->data_len;
+				ext_entity_hdr->size = ext_size;
+				entity_hdr->flag |= CUDBG_EXT_DATA_VALID;
+			}
+
+			if (cudbg_init->use_flash) {
+				if (flag_ext)
+					wr_entity_to_flash(handle,
+							   &dbg_buff,
+							   ext_entity_hdr->
+							   start_offset,
+							   entity_hdr->size,
+							   CUDBG_EXT_ENTITY,
+							   ext_size);
+				else
+					wr_entity_to_flash(handle,
+							   &dbg_buff,
+							   entity_hdr->
+							   start_offset,
+							   entity_hdr->
+							   size,
+							   large_entity_list[i].
+							   entity_code,
+							   ext_size);
+			}
+		}
+	}
+
+	cudbg_hdr->data_len = total_size;
+	*outbuf_size = total_size;
+
+	if (flag_ext)
+		*outbuf_size += ext_size;
+
+	return 0;
+err:
+	return rc;
+}
+
+void reset_skip_entity(struct large_entity *large_entity_list,
+		       int large_entity_list_size)
+{
+	int i;
+
+	for (i = 0; i < large_entity_list_size; i++)
+		large_entity_list[i].skip_flag = 0;
+}
+
+void skip_entity(struct large_entity *large_entity_list,
+		 int large_entity_list_size, int entity_code)
+{
+	int i;
+
+	for (i = 0; i < large_entity_list_size; i++)
+		if (large_entity_list[i].entity_code == entity_code)
+			large_entity_list[i].skip_flag = 1;
+}
+
+int is_large_entity(struct large_entity *large_entity_list,
+		    int large_entity_list_size, int entity_code)
+{
+	int i;
+
+	for (i = 0; i < large_entity_list_size; i++)
+		if (large_entity_list[i].entity_code == entity_code)
+			return 1;
+	return 0;
+}
+
+int get_entity_hdr(void *outbuf, int i, u32 size,
+		   struct cudbg_entity_hdr **entity_hdr)
+{
+	struct cudbg_hdr *cudbg_hdr = (struct cudbg_hdr *)outbuf;
+	int rc = 0;
+
+	if (cudbg_hdr->hdr_len + (sizeof(struct cudbg_entity_hdr)*i) > size)
+		return CUDBG_STATUS_SMALL_BUFF;
+
+	*entity_hdr = (struct cudbg_entity_hdr *)
+		      ((char *)outbuf+cudbg_hdr->hdr_len +
+		       (sizeof(struct cudbg_entity_hdr)*(i-1)));
+	return rc;
+}
+
+static int collect_rss(struct cudbg_init *pdbg_init,
+		       struct cudbg_buffer *dbg_buff,
+		       struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, RSS_NENTRIES * sizeof(u16), &scratch_buff);
+	rc = t4_read_rss(padap, (u16 *)scratch_buff.data);
+	if (rc) {
+		pdbg_init->print("%s(), t4_read_rss failed!, rc: %d\n",
+				 __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_sw_state(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_param *dbg_param = NULL;
+	struct cudbg_buffer scratch_buff;
+	struct sw_state *swstate;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*swstate), &scratch_buff);
+	swstate = (struct sw_state *) scratch_buff.data;
+	swstate->fw_state = t4_read_reg(padap, PCIE_FW_A);
+	dbg_param = &pdbg_init->dbg_params[CUDBG_SW_STATE_PARAM];
+	if (dbg_param->param_type == CUDBG_SW_STATE_PARAM) {
+		strcpy((char *)swstate->caller_string,
+		       (char *)dbg_param->u.sw_state_param.caller_string);
+		swstate->os_type = dbg_param->u.sw_state_param.os_type;
+	} else {
+		strcpy((char *)swstate->caller_string, "Unknown");
+		swstate->os_type = CUDBG_OS_TYPE_UNKNOWN;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_ddp_stats(struct cudbg_init *pdbg_init,
+			     struct cudbg_buffer *dbg_buff,
+			     struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct tp_usm_stats  *tp_usm_stats_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tp_usm_stats_buff), &scratch_buff);
+	tp_usm_stats_buff = (struct tp_usm_stats *) scratch_buff.data;
+	t4_get_usm_stats(padap, tp_usm_stats_buff, true);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_ulptx_la(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_ulptx_la *ulptx_la_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+	u32 i, j;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*ulptx_la_buff), &scratch_buff);
+	ulptx_la_buff = (struct struct_ulptx_la *) scratch_buff.data;
+	for (i = 0; i < CUDBG_NUM_ULPTX; i++) {
+		ulptx_la_buff->rdptr[i] = t4_read_reg(padap,
+						      ULP_TX_LA_RDPTR_0 +
+						      0x10 * i);
+		ulptx_la_buff->wrptr[i] = t4_read_reg(padap,
+						      ULP_TX_LA_WRPTR_0 +
+						      0x10 * i);
+		ulptx_la_buff->rddata[i] = t4_read_reg(padap,
+						       ULP_TX_LA_RDDATA_0 +
+						       0x10 * i);
+		for (j = 0; j < CUDBG_NUM_ULPTX_READ; j++) {
+			ulptx_la_buff->rd_data[i][j] =
+				t4_read_reg(padap,
+					    ULP_TX_LA_RDDATA_0 + 0x10 * i);
+		}
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_ulprx_la(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_ulprx_la *ulprx_la_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*ulprx_la_buff), &scratch_buff);
+	ulprx_la_buff = (struct struct_ulprx_la *) scratch_buff.data;
+	t4_ulprx_read_la(padap, (u32 *)ulprx_la_buff->data);
+	ulprx_la_buff->size = ULPRX_LA_SIZE;
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_cpl_stats(struct cudbg_init *pdbg_init,
+			     struct cudbg_buffer *dbg_buff,
+			     struct cudbg_error *cudbg_err)
+{
+	struct struct_tp_cpl_stats *tp_cpl_stats_buff;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tp_cpl_stats_buff), &scratch_buff);
+	tp_cpl_stats_buff = (struct struct_tp_cpl_stats *) scratch_buff.data;
+	tp_cpl_stats_buff->nchan = padap->params.arch.nchan;
+	t4_tp_get_cpl_stats(padap, &(tp_cpl_stats_buff->stats), true);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_wc_stats(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_wc_stats *wc_stats_buff;
+	struct cudbg_buffer scratch_buff;
+	u32 val1, val2;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*wc_stats_buff), &scratch_buff);
+	wc_stats_buff = (struct struct_wc_stats *) scratch_buff.data;
+	if (!is_t4(padap->params.chip)) {
+		val1 = t4_read_reg(padap, SGE_STAT_TOTAL_A);
+		val2 = t4_read_reg(padap, SGE_STAT_MATCH_A);
+		wc_stats_buff->wr_cl_success = val1 - val2;
+		wc_stats_buff->wr_cl_fail = val2;
+	} else {
+		wc_stats_buff->wr_cl_success = 0;
+		wc_stats_buff->wr_cl_fail = 0;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int fill_meminfo(struct adapter *padap,
+			struct struct_meminfo *meminfo_buff)
+{
+	struct struct_mem_desc *md;
+	u32 size, lo, hi;
+	int n, i, rc = 0;
+	u32 used, alloc;
+
+	size = sizeof(struct struct_meminfo);
+
+	memset(meminfo_buff->avail, 0,
+	       ARRAY_SIZE(meminfo_buff->avail) *
+	       sizeof(struct struct_mem_desc));
+	memset(meminfo_buff->mem, 0,
+	       (ARRAY_SIZE(region) + 3) * sizeof(struct struct_mem_desc));
+	md  = meminfo_buff->mem;
+
+	for (i = 0; i < ARRAY_SIZE(meminfo_buff->mem); i++) {
+		meminfo_buff->mem[i].limit = 0;
+		meminfo_buff->mem[i].idx = i;
+	}
+
+	i = 0;
+	lo = t4_read_reg(padap, MA_TARGET_MEM_ENABLE_A);
+	if (lo & EDRAM0_ENABLE_F) {
+		hi = t4_read_reg(padap, MA_EDRAM0_BAR_A);
+		meminfo_buff->avail[i].base = EDRAM0_BASE_G(hi) << 20;
+		meminfo_buff->avail[i].limit = meminfo_buff->avail[i].base +
+					       (EDRAM0_SIZE_G(hi) << 20);
+		meminfo_buff->avail[i].idx = 0;
+		i++;
+	}
+
+	if (lo & EDRAM1_ENABLE_F) {
+		hi =  t4_read_reg(padap, MA_EDRAM1_BAR_A);
+		meminfo_buff->avail[i].base = EDRAM1_BASE_G(hi) << 20;
+		meminfo_buff->avail[i].limit = meminfo_buff->avail[i].base +
+					       (EDRAM1_SIZE_G(hi) << 20);
+		meminfo_buff->avail[i].idx = 1;
+		i++;
+	}
+
+	if (is_t5(padap->params.chip)) {
+		if (lo & EXT_MEM0_ENABLE_F) {
+			hi = t4_read_reg(padap, MA_EXT_MEMORY0_BAR_A);
+			meminfo_buff->avail[i].base = EXT_MEM_BASE_G(hi) << 20;
+			meminfo_buff->avail[i].limit =
+				meminfo_buff->avail[i].base +
+				(EXT_MEM_SIZE_G(hi) << 20);
+			meminfo_buff->avail[i].idx = 3;
+			i++;
+		}
+
+		if (lo & EXT_MEM1_ENABLE_F) {
+			hi = t4_read_reg(padap, MA_EXT_MEMORY1_BAR_A);
+			meminfo_buff->avail[i].base = EXT_MEM1_BASE_G(hi) << 20;
+			meminfo_buff->avail[i].limit =
+				meminfo_buff->avail[i].base +
+				(EXT_MEM1_SIZE_G(hi) << 20);
+			meminfo_buff->avail[i].idx = 4;
+			i++;
+		}
+	} else if (is_t6(padap->params.chip)) {
+		if (lo & EXT_MEM_ENABLE_F) {
+			hi = t4_read_reg(padap, MA_EXT_MEMORY_BAR_A);
+			meminfo_buff->avail[i].base = EXT_MEM_BASE_G(hi) << 20;
+			meminfo_buff->avail[i].limit =
+				meminfo_buff->avail[i].base +
+				(EXT_MEM_SIZE_G(hi) << 20);
+			meminfo_buff->avail[i].idx = 2;
+			i++;
+		}
+
+		if (lo & HMA_MUX_F) {
+			hi = t4_read_reg(padap, MA_EXT_MEMORY1_BAR_A);
+			meminfo_buff->avail[i].base = EXT_MEM1_BASE_G(hi) << 20;
+			meminfo_buff->avail[i].limit =
+				meminfo_buff->avail[i].base +
+				(EXT_MEM1_SIZE_G(hi) << 20);
+			meminfo_buff->avail[i].idx = 5;
+			i++;
+		}
+	}
+
+	if (!i) {				   /* no memory available */
+		rc = CUDBG_STATUS_ENTITY_NOT_FOUND;
+		goto err;
+	}
+
+	meminfo_buff->avail_c = i;
+	sort_t(meminfo_buff->avail, i, sizeof(struct struct_mem_desc),
+	       mem_desc_cmp, NULL);
+	(md++)->base = t4_read_reg(padap, SGE_DBQ_CTXT_BADDR_A);
+	(md++)->base = t4_read_reg(padap, SGE_IMSG_CTXT_BADDR_A);
+	(md++)->base = t4_read_reg(padap, SGE_FLM_CACHE_BADDR_A);
+	(md++)->base = t4_read_reg(padap, TP_CMM_TCB_BASE_A);
+	(md++)->base = t4_read_reg(padap, TP_CMM_MM_BASE_A);
+	(md++)->base = t4_read_reg(padap, TP_CMM_TIMER_BASE_A);
+	(md++)->base = t4_read_reg(padap, TP_CMM_MM_RX_FLST_BASE_A);
+	(md++)->base = t4_read_reg(padap, TP_CMM_MM_TX_FLST_BASE_A);
+	(md++)->base = t4_read_reg(padap, TP_CMM_MM_PS_FLST_BASE_A);
+
+	/* the next few have explicit upper bounds */
+	md->base = t4_read_reg(padap, TP_PMM_TX_BASE_A);
+	md->limit = md->base - 1 +
+		    t4_read_reg(padap,
+				TP_PMM_TX_PAGE_SIZE_A) *
+				PMTXMAXPAGE_G(t4_read_reg(padap,
+							  TP_PMM_TX_MAX_PAGE_A)
+					     );
+	md++;
+
+	md->base = t4_read_reg(padap, TP_PMM_RX_BASE_A);
+	md->limit = md->base - 1 +
+		    t4_read_reg(padap,
+				TP_PMM_RX_PAGE_SIZE_A) *
+				PMRXMAXPAGE_G(t4_read_reg(padap,
+							  TP_PMM_RX_MAX_PAGE_A)
+					      );
+	md++;
+	if (t4_read_reg(padap, LE_DB_CONFIG_A) & HASHEN_F) {
+		if (CHELSIO_CHIP_VERSION(padap->params.chip) <= CHELSIO_T5) {
+			hi = t4_read_reg(padap, LE_DB_TID_HASHBASE_A) / 4;
+			md->base = t4_read_reg(padap, LE_DB_HASH_TID_BASE_A);
+		} else {
+			hi = t4_read_reg(padap, LE_DB_HASH_TID_BASE_A);
+			md->base = t4_read_reg(padap,
+					       LE_DB_HASH_TBL_BASE_ADDR_A);
+		}
+		md->limit = 0;
+	} else {
+		md->base = 0;
+		md->idx = ARRAY_SIZE(region);  /* hide it */
+	}
+	md++;
+#define ulp_region(reg) \
+	{\
+		md->base = t4_read_reg(padap, ULP_ ## reg ## _LLIMIT_A);\
+		(md++)->limit = t4_read_reg(padap, ULP_ ## reg ## _ULIMIT_A);\
+	}
+
+	ulp_region(RX_ISCSI);
+	ulp_region(RX_TDDP);
+	ulp_region(TX_TPT);
+	ulp_region(RX_STAG);
+	ulp_region(RX_RQ);
+	ulp_region(RX_RQUDP);
+	ulp_region(RX_PBL);
+	ulp_region(TX_PBL);
+#undef ulp_region
+	md->base = 0;
+	md->idx = ARRAY_SIZE(region);
+	if (!is_t4(padap->params.chip)) {
+		u32 sge_ctrl = t4_read_reg(padap, SGE_CONTROL2_A);
+		u32 fifo_size = t4_read_reg(padap, SGE_DBVFIFO_SIZE_A);
+
+		if (is_t5(padap->params.chip)) {
+			if (sge_ctrl & VFIFO_ENABLE_F)
+				size = DBVFIFO_SIZE_G(fifo_size);
+		} else
+			size = T6_DBVFIFO_SIZE_G(fifo_size);
+
+		if (size) {
+			md->base = BASEADDR_G(t4_read_reg(padap,
+							  SGE_DBVFIFO_BADDR_A));
+			md->limit = md->base + (size << 2) - 1;
+		}
+	}
+
+	md++;
+
+	md->base = t4_read_reg(padap, ULP_RX_CTX_BASE_A);
+	md->limit = 0;
+	md++;
+	md->base = t4_read_reg(padap, ULP_TX_ERR_TABLE_BASE_A);
+	md->limit = 0;
+	md++;
+#ifndef __NO_DRIVER_OCQ_SUPPORT__
+	md->idx = ARRAY_SIZE(region);  /* hide it */
+	md++;
+#endif
+
+	/* add any address-space holes, there can be up to 3 */
+	for (n = 0; n < i - 1; n++)
+		if (meminfo_buff->avail[n].limit <
+		    meminfo_buff->avail[n + 1].base)
+			(md++)->base = meminfo_buff->avail[n].limit;
+
+	if (meminfo_buff->avail[n].limit)
+		(md++)->base = meminfo_buff->avail[n].limit;
+
+	n = (int) (md - meminfo_buff->mem);
+	meminfo_buff->mem_c = n;
+
+	sort_t(meminfo_buff->mem, n, sizeof(struct struct_mem_desc),
+	       mem_desc_cmp, NULL);
+
+	lo = t4_read_reg(padap, CIM_SDRAM_BASE_ADDR_A);
+	hi = t4_read_reg(padap, CIM_SDRAM_ADDR_SIZE_A) + lo - 1;
+	meminfo_buff->up_ram_lo = lo;
+	meminfo_buff->up_ram_hi = hi;
+
+	lo = t4_read_reg(padap, CIM_EXTMEM2_BASE_ADDR_A);
+	hi = t4_read_reg(padap, CIM_EXTMEM2_ADDR_SIZE_A) + lo - 1;
+	meminfo_buff->up_extmem2_lo = lo;
+	meminfo_buff->up_extmem2_hi = hi;
+
+	lo = t4_read_reg(padap, TP_PMM_RX_MAX_PAGE_A);
+	meminfo_buff->rx_pages_data[0] =  PMRXMAXPAGE_G(lo);
+	meminfo_buff->rx_pages_data[1] =
+		t4_read_reg(padap, TP_PMM_RX_PAGE_SIZE_A) >> 10;
+	meminfo_buff->rx_pages_data[2] = (lo & PMRXNUMCHN_F) ? 2 : 1 ;
+
+	lo = t4_read_reg(padap, TP_PMM_TX_MAX_PAGE_A);
+	hi = t4_read_reg(padap, TP_PMM_TX_PAGE_SIZE_A);
+	meminfo_buff->tx_pages_data[0] = PMTXMAXPAGE_G(lo);
+	meminfo_buff->tx_pages_data[1] =
+		hi >= (1 << 20) ? (hi >> 20) : (hi >> 10);
+	meminfo_buff->tx_pages_data[2] =
+		hi >= (1 << 20) ? 'M' : 'K';
+	meminfo_buff->tx_pages_data[3] = 1 << PMTXNUMCHN_G(lo);
+
+	for (i = 0; i < 4; i++) {
+		if (CHELSIO_CHIP_VERSION(padap->params.chip) > CHELSIO_T5)
+			lo = t4_read_reg(padap,
+					 MPS_RX_MAC_BG_PG_CNT0_A + i * 4);
+		else
+			lo = t4_read_reg(padap, MPS_RX_PG_RSV0_A + i * 4);
+		if (is_t5(padap->params.chip)) {
+			used = T5_USED_G(lo);
+			alloc = T5_ALLOC_G(lo);
+		} else {
+			used = USED_G(lo);
+			alloc = ALLOC_G(lo);
+		}
+		meminfo_buff->port_used[i] = used;
+		meminfo_buff->port_alloc[i] = alloc;
+	}
+
+	for (i = 0; i < padap->params.arch.nchan; i++) {
+		if (CHELSIO_CHIP_VERSION(padap->params.chip) > CHELSIO_T5)
+			lo = t4_read_reg(padap,
+					 MPS_RX_LPBK_BG_PG_CNT0_A + i * 4);
+		else
+			lo = t4_read_reg(padap, MPS_RX_PG_RSV4_A + i * 4);
+		if (is_t5(padap->params.chip)) {
+			used = T5_USED_G(lo);
+			alloc = T5_ALLOC_G(lo);
+		} else {
+			used = USED_G(lo);
+			alloc = ALLOC_G(lo);
+		}
+		meminfo_buff->loopback_used[i] = used;
+		meminfo_buff->loopback_alloc[i] = alloc;
+	}
+err:
+	return rc;
+}
+
+static int collect_meminfo(struct cudbg_init *pdbg_init,
+			   struct cudbg_buffer *dbg_buff,
+			   struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_meminfo *meminfo_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*meminfo_buff), &scratch_buff);
+	meminfo_buff = (struct struct_meminfo *)scratch_buff.data;
+	rc = fill_meminfo(padap, meminfo_buff);
+	if (rc)
+		goto err1;
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int get_port_count(struct cudbg_init *pdbg_init)
+{
+	struct adapter *padap = pdbg_init->adap;
+	u8 port_type[PORT_TYPE_LEN + 1] = { 0 };
+	u32 v, port_vec, port_count;
+	unsigned int i;
+	int rc = 0;
+	char *tmp;
+
+	v = FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DEV) |
+	    FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DEV_PORTVEC);
+	port_count = 0;
+	if (is_fw_attached(pdbg_init)) {
+		rc = t4_query_params(padap, padap->mbox, padap->pf, 0, 1, &v,
+				     &port_vec);
+		if (rc >= 0)
+			port_count = count_set_bits(port_vec);
+	}
+
+	if (port_count == 0) {
+		rc = read_vpd_reg(padap, PORT_TYPE_ADDR, PORT_TYPE_LEN,
+				  port_type);
+		if (rc)
+			return rc;
+
+		/* port_vec will be like 0x0e0effff
+		 * ff means not active
+		 * rather than ff means active port
+		 */
+		tmp = (char *)port_type;
+		for (i = 0; i < PORT_TYPE_LEN; i += 2) {
+			if (!strncmp(&tmp[i], "FF", 2))
+				continue;
+			port_count++;
+		}
+	}
+	return port_count;
+}
+
+static int collect_lb_stats(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_lb_stats *lb_stats_buff;
+	struct cudbg_buffer scratch_buff;
+	struct lb_port_stats *tmp_stats;
+	u32 i, n, size;
+	int rc = 0;
+
+	rc = get_port_count(pdbg_init);
+	if (rc < 0)
+		return rc;
+
+	n = rc;
+	size = sizeof(struct struct_lb_stats) +
+	       n * sizeof(struct lb_port_stats);
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	lb_stats_buff = (struct struct_lb_stats *) scratch_buff.data;
+	lb_stats_buff->nchan = n;
+	tmp_stats = lb_stats_buff->s;
+	for (i = 0; i < n; i += 2, tmp_stats += 2) {
+		t4_get_lb_stats(padap, i, tmp_stats);
+		t4_get_lb_stats(padap, i + 1, tmp_stats+1);
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_rdma_stats(struct cudbg_init *pdbg_init,
+			      struct cudbg_buffer *dbg_buff,
+			      struct cudbg_error *cudbg_er)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct tp_rdma_stats *rdma_stats_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*rdma_stats_buff), &scratch_buff);
+	rdma_stats_buff = (struct tp_rdma_stats *) scratch_buff.data;
+	t4_tp_get_rdma_stats(padap, rdma_stats_buff, true);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_clk_info(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_clk_info *clk_info_buff;
+	struct cudbg_buffer scratch_buff;
+	u64 tp_tick_us;
+	int rc = 0;
+
+	if (!padap->params.vpd.cclk)
+		return CUDBG_STATUS_CCLK_NOT_DEFINED;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*clk_info_buff), &scratch_buff);
+	clk_info_buff = (struct struct_clk_info *) scratch_buff.data;
+	clk_info_buff->cclk_ps = 1000000000 / padap->params.vpd.cclk;  /* in ps
+	*/
+	clk_info_buff->res = t4_read_reg(padap, TP_TIMER_RESOLUTION_A);
+	clk_info_buff->tre = TIMERRESOLUTION_G(clk_info_buff->res);
+	clk_info_buff->dack_re = DELAYEDACKRESOLUTION_G(clk_info_buff->res);
+	tp_tick_us = (clk_info_buff->cclk_ps << clk_info_buff->tre) / 1000000;
+	/* in us */
+	clk_info_buff->dack_timer = ((clk_info_buff->cclk_ps <<
+				      clk_info_buff->dack_re) / 1000000) *
+				     t4_read_reg(padap, TP_DACK_TIMER_A);
+	clk_info_buff->retransmit_min =
+		tp_tick_us * t4_read_reg(padap, TP_RXT_MIN_A);
+	clk_info_buff->retransmit_max =
+		tp_tick_us * t4_read_reg(padap, TP_RXT_MAX_A);
+	clk_info_buff->persist_timer_min =
+		tp_tick_us * t4_read_reg(padap, TP_PERS_MIN_A);
+	clk_info_buff->persist_timer_max =
+		tp_tick_us * t4_read_reg(padap, TP_PERS_MAX_A);
+	clk_info_buff->keepalive_idle_timer =
+		tp_tick_us * t4_read_reg(padap, TP_KEEP_IDLE_A);
+	clk_info_buff->keepalive_interval =
+		tp_tick_us * t4_read_reg(padap, TP_KEEP_INTVL_A);
+	clk_info_buff->initial_srtt =
+		tp_tick_us * INITSRTT_G(t4_read_reg(padap, TP_INIT_SRTT_A));
+	clk_info_buff->finwait2_timer =
+		tp_tick_us * t4_read_reg(padap, TP_FINWAIT2_TIMER_A);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_macstats(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct struct_mac_stats_rev1 *mac_stats_buff;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+	u32 i, n;
+
+	rc = get_port_count(pdbg_init);
+	if (rc < 0)
+		return rc;
+
+	n = rc;
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*mac_stats_buff), &scratch_buff);
+	mac_stats_buff = (struct struct_mac_stats_rev1 *) scratch_buff.data;
+	mac_stats_buff->ver_hdr.signature = CUDBG_ENTITY_SIGNATURE;
+	mac_stats_buff->ver_hdr.revision = CUDBG_MAC_STATS_REV;
+	mac_stats_buff->ver_hdr.size = sizeof(struct struct_mac_stats_rev1) -
+				       sizeof(struct cudbg_ver_hdr);
+	mac_stats_buff->port_count = n;
+	for (i = 0; i <  mac_stats_buff->port_count; i++)
+		t4_get_port_stats(padap, i, &mac_stats_buff->stats[i]);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_cim_pif_la(struct cudbg_init *pdbg_init,
+			      struct cudbg_buffer *dbg_buff,
+			      struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cim_pif_la *cim_pif_la_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+	u32 size;
+
+	size = sizeof(*cim_pif_la_buff) + 2 * CIM_PIFLA_SIZE * 6 * sizeof(u32);
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	cim_pif_la_buff = (struct cim_pif_la *) scratch_buff.data;
+	cim_pif_la_buff->size = CIM_PIFLA_SIZE;
+	t4_cim_read_pif_la(padap, (u32 *)cim_pif_la_buff->data,
+			   (u32 *)cim_pif_la_buff->data + 6 * CIM_PIFLA_SIZE,
+			   NULL, NULL);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_tp_la(struct cudbg_init *pdbg_init,
+			 struct cudbg_buffer *dbg_buff,
+			 struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct struct_tp_la *tp_la_buff;
+	int rc = 0;
+	u32 size;
+
+	size = sizeof(struct struct_tp_la) + TPLA_SIZE *  sizeof(u64);
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	tp_la_buff = (struct struct_tp_la *) scratch_buff.data;
+	tp_la_buff->mode = DBGLAMODE_G(t4_read_reg(padap, TP_DBG_LA_CONFIG_A));
+	t4_tp_read_la(padap, (u64 *)tp_la_buff->data, NULL);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_fcoe_stats(struct cudbg_init *pdbg_init,
+			      struct cudbg_buffer *dbg_buff,
+			      struct cudbg_error *cudbg_err)
+{
+	struct struct_tp_fcoe_stats  *tp_fcoe_stats_buff;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tp_fcoe_stats_buff), &scratch_buff);
+	tp_fcoe_stats_buff = (struct struct_tp_fcoe_stats *) scratch_buff.data;
+	t4_get_fcoe_stats(padap, 0, &(tp_fcoe_stats_buff->stats[0]), true);
+	t4_get_fcoe_stats(padap, 1, &(tp_fcoe_stats_buff->stats[1]), true);
+	if (padap->params.arch.nchan == NCHAN) {
+		t4_get_fcoe_stats(padap, 2, &(tp_fcoe_stats_buff->stats[2]),
+				  true);
+		t4_get_fcoe_stats(padap, 3, &(tp_fcoe_stats_buff->stats[3]),
+				  true);
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_tp_err_stats(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	struct struct_tp_err_stats *tp_err_stats_buff;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tp_err_stats_buff), &scratch_buff);
+	tp_err_stats_buff = (struct struct_tp_err_stats *) scratch_buff.data;
+	t4_tp_get_err_stats(padap, &(tp_err_stats_buff->stats), true);
+	tp_err_stats_buff->nchan = padap->params.arch.nchan;
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_tcp_stats(struct cudbg_init *pdbg_init,
+			     struct cudbg_buffer *dbg_buff,
+			     struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_tcp_stats *tcp_stats_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tcp_stats_buff), &scratch_buff);
+	tcp_stats_buff = (struct struct_tcp_stats *) scratch_buff.data;
+	t4_tp_get_tcp_stats(padap, &(tcp_stats_buff->v4),
+			    &(tcp_stats_buff->v6), true);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_hw_sched(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_hw_sched *hw_sched_buff;
+	struct cudbg_buffer scratch_buff;
+	int i, rc = 0;
+
+	if (!padap->params.vpd.cclk)
+		return CUDBG_STATUS_CCLK_NOT_DEFINED;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*hw_sched_buff), &scratch_buff);
+	hw_sched_buff = (struct struct_hw_sched *) scratch_buff.data;
+	hw_sched_buff->map = t4_read_reg(padap, TP_TX_MOD_QUEUE_REQ_MAP_A);
+	hw_sched_buff->mode = TIMERMODE_G(t4_read_reg(padap, TP_MOD_CONFIG_A));
+	t4_read_pace_tbl(padap, hw_sched_buff->pace_tab);
+	for (i = 0; i < NTX_SCHED; ++i) {
+		t4_get_tx_sched(padap, i, &(hw_sched_buff->kbps[i]),
+				&(hw_sched_buff->ipg[i]), true);
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_pm_stats(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_pm_stats *pm_stats_buff;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*pm_stats_buff), &scratch_buff);
+	pm_stats_buff = (struct struct_pm_stats *) scratch_buff.data;
+	t4_pmtx_get_stats(padap, pm_stats_buff->tx_cnt, pm_stats_buff->tx_cyc);
+	t4_pmrx_get_stats(padap, pm_stats_buff->rx_cnt, pm_stats_buff->rx_cyc);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_path_mtu(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, NMTUS * sizeof(u16), &scratch_buff);
+	t4_read_mtu_tbl(padap, (u16 *)scratch_buff.data, NULL);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_rss_key(struct cudbg_init *pdbg_init,
+			   struct cudbg_buffer *dbg_buff,
+			   struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	GET_SCRATCH_BUFF(dbg_buff, 10 * sizeof(u32), &scratch_buff);
+	t4_read_rss_key(padap, (u32 *)scratch_buff.data, true);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_rss_config(struct cudbg_init *pdbg_init,
+			      struct cudbg_buffer *dbg_buff,
+			      struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct rss_config *rss_conf;
+	int rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(struct rss_config), &scratch_buff);
+	rss_conf =  (struct rss_config *)scratch_buff.data;
+	rss_conf->tp_rssconf = t4_read_reg(padap, TP_RSS_CONFIG_A);
+	rss_conf->tp_rssconf_tnl = t4_read_reg(padap, TP_RSS_CONFIG_TNL_A);
+	rss_conf->tp_rssconf_ofd = t4_read_reg(padap, TP_RSS_CONFIG_OFD_A);
+	rss_conf->tp_rssconf_syn = t4_read_reg(padap, TP_RSS_CONFIG_SYN_A);
+	rss_conf->tp_rssconf_vrt = t4_read_reg(padap, TP_RSS_CONFIG_VRT_A);
+	rss_conf->tp_rssconf_cng = t4_read_reg(padap, TP_RSS_CONFIG_CNG_A);
+	rss_conf->chip = padap->params.chip;
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_rss_vf_config(struct cudbg_init *pdbg_init,
+				 struct cudbg_buffer *dbg_buff,
+				 struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct rss_vf_conf *vfconf;
+	int vf, rc, vf_count;
+
+	vf_count = padap->params.arch.vfcount;
+	GET_SCRATCH_BUFF(dbg_buff, vf_count * sizeof(*vfconf), &scratch_buff);
+	vfconf =  (struct rss_vf_conf *)scratch_buff.data;
+	for (vf = 0; vf < vf_count; vf++) {
+		t4_read_rss_vf_config(padap, vf, &vfconf[vf].rss_vf_vfl,
+				      &vfconf[vf].rss_vf_vfh, true);
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_rss_pf_config(struct cudbg_init *pdbg_init,
+				 struct cudbg_buffer *dbg_buff,
+				 struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	u32 rss_pf_map, rss_pf_mask;
+	struct rss_pf_conf *pfconf;
+	int pf, rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, 8 * sizeof(*pfconf), &scratch_buff);
+	pfconf =  (struct rss_pf_conf *)scratch_buff.data;
+	rss_pf_map = t4_read_rss_pf_map(padap, true);
+	rss_pf_mask = t4_read_rss_pf_mask(padap, true);
+	for (pf = 0; pf < 8; pf++) {
+		pfconf[pf].rss_pf_map = rss_pf_map;
+		pfconf[pf].rss_pf_mask = rss_pf_mask;
+		t4_read_rss_pf_config(padap, pf, &pfconf[pf].rss_pf_config,
+				      true);
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int check_valid(u32 *buf, int type)
+{
+	int index, bit, bit_pos = 0;
+
+	switch (type) {
+	case CTXT_EGRESS:
+		bit_pos = 176;
+		break;
+	case CTXT_INGRESS:
+		bit_pos = 141;
+		break;
+	case CTXT_FLM:
+		bit_pos = 89;
+		break;
+	}
+	index = bit_pos / 32;
+	bit =  bit_pos % 32;
+	return buf[index] & (1U << bit);
+}
+
+/**
+ * Get EGRESS, INGRESS, FLM, and CNM max qid.
+ *
+ * For EGRESS and INGRESS, do the following calculation.
+ * max_qid = (DBQ/IMSG context region size in bytes) /
+ *	     (size of context in bytes).
+ *
+ * For FLM, do the following calculation.
+ * max_qid = (FLM cache region size in bytes) /
+ *	     ((number of pointers cached in EDRAM) * 8 (bytes per pointer)).
+ *
+ * There's a 1-to-1 mapping between FLM and CNM if there's no header splitting
+ * enabled; i.e., max CNM qid is equal to max FLM qid. However, if header
+ * splitting is enabled, then max CNM qid is half of max FLM qid.
+ */
+static int get_max_ctxt_qid(struct adapter *padap,
+			    struct struct_meminfo *meminfo,
+			    u32 *max_ctx_qid, u8 nelem)
+{
+	u32 i, idx, found = 0;
+
+	if (nelem != (CTXT_CNM + 1))
+		return -EINVAL;
+
+	for (i = 0; i < meminfo->mem_c; i++) {
+		if (meminfo->mem[i].idx >= ARRAY_SIZE(region))
+			continue;                        /* skip holes */
+
+		idx = meminfo->mem[i].idx;
+		/* Get DBQ, IMSG, and FLM context region size */
+		if (idx <= CTXT_FLM) {
+			if (!(meminfo->mem[i].limit))
+				meminfo->mem[i].limit =
+					i < meminfo->mem_c - 1 ?
+					meminfo->mem[i + 1].base - 1 : ~0;
+
+			if (idx < CTXT_FLM) {
+				/* Get EGRESS and INGRESS max qid. */
+				max_ctx_qid[idx] = (meminfo->mem[i].limit -
+						    meminfo->mem[i].base + 1) /
+						   CUDBG_CTXT_SIZE_BYTES;
+				found++;
+			} else {
+				/* Get FLM and CNM max qid. */
+				u32 value, edram_ptr_count;
+				u8 bytes_per_ptr = 8;
+				u8 nohdr;
+
+				value = t4_read_reg(padap, SGE_FLM_CFG_A);
+
+				/* Check if header splitting is enabled. */
+				nohdr = (value >> NOHDR_S) & 1U;
+
+				/* Get the number of pointers in EDRAM per
+				 * qid in units of 32.
+				 */
+				edram_ptr_count = 32 *
+						  (1U << EDRAMPTRCNT_G(value));
+
+				/* EDRAMPTRCNT value of 3 is reserved.
+				 * So don't exceed 128.
+				 */
+				if (edram_ptr_count > 128)
+					edram_ptr_count = 128;
+
+				max_ctx_qid[idx] = (meminfo->mem[i].limit -
+						    meminfo->mem[i].base + 1) /
+						   (edram_ptr_count *
+						    bytes_per_ptr);
+				found++;
+
+				/* CNM has 1-to-1 mapping with FLM.
+				 * However, if header splitting is enabled,
+				 * then max CNM qid is half of max FLM qid.
+				 */
+				max_ctx_qid[CTXT_CNM] = nohdr ?
+							max_ctx_qid[idx] :
+							max_ctx_qid[idx] >> 1;
+
+				/* One more increment for CNM */
+				found++;
+			}
+		}
+		if (found == nelem)
+			break;
+	}
+
+	/* Sanity check. Ensure the values are within known max. */
+	max_ctx_qid[CTXT_EGRESS] = min_t(u32, max_ctx_qid[CTXT_EGRESS],
+					 CTXTQID_M);
+	max_ctx_qid[CTXT_INGRESS] = min_t(u32, max_ctx_qid[CTXT_INGRESS],
+					  CUDBG_MAX_INGRESS_QIDS);
+	max_ctx_qid[CTXT_FLM] = min_t(u32, max_ctx_qid[CTXT_FLM],
+				      CUDBG_MAX_FL_QIDS);
+	max_ctx_qid[CTXT_CNM] = min_t(u32, max_ctx_qid[CTXT_CNM],
+				      CUDBG_MAX_CNM_QIDS);
+	return 0;
+}
+
+static int collect_dump_context(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	u32 size = 0, next_offset = 0, total_size = 0;
+	struct cudbg_buffer scratch_buff, temp_buff;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_ch_cntxt *buff = NULL;
+	struct struct_meminfo meminfo;
+	u32 max_ctx_qid[CTXT_CNM + 1];
+	u32 i, j, qid_count = 0;
+	bool limit_qid = false;
+	int bytes = 0, rc = 0;
+
+	rc = fill_meminfo(padap, &meminfo);
+	if (rc)
+		goto err;
+
+	/* Get max valid qid for each type of queue */
+	rc = get_max_ctxt_qid(padap, &meminfo, max_ctx_qid, CTXT_CNM + 1);
+	if (rc)
+		goto err;
+
+	/* There are four types of queues. Collect context upto max
+	 * qid of each type of queue.
+	 */
+	for (i = CTXT_EGRESS; i <= CTXT_CNM; i++)
+		size += sizeof(struct cudbg_ch_cntxt) * max_ctx_qid[i];
+
+	rc = get_scratch_buff(dbg_buff, size, &scratch_buff);
+	if (rc == CUDBG_STATUS_NO_SCRATCH_MEM) {
+		/* Not enough scratch Memory available.
+		 * Collect context of at least CUDBG_LOWMEM_MAX_CTXT_QIDS
+		 * for each queue type.
+		 */
+		size = 0;
+		for (i = CTXT_EGRESS; i <= CTXT_CNM; i++)
+			size += sizeof(struct cudbg_ch_cntxt) *
+				CUDBG_LOWMEM_MAX_CTXT_QIDS;
+
+		limit_qid = true;
+		GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	}
+
+	buff = (struct cudbg_ch_cntxt *)scratch_buff.data;
+	/* Collect context data */
+	for (i = CTXT_EGRESS; i <= CTXT_FLM; i++) {
+		qid_count = 0;
+		for (j = 0; j < max_ctx_qid[i]; j++) {
+			read_sge_ctxt(pdbg_init, j, i, buff->data);
+
+			rc = check_valid(buff->data, i);
+			if (rc) {
+				buff->cntxt_type = i;
+				buff->cntxt_id = j;
+				buff++;
+				total_size += sizeof(struct cudbg_ch_cntxt);
+
+				if (i == CTXT_FLM) {
+					read_sge_ctxt(pdbg_init, j, CTXT_CNM,
+						      buff->data);
+					buff->cntxt_type = CTXT_CNM;
+					buff->cntxt_id = j;
+					buff++;
+					total_size +=
+						sizeof(struct cudbg_ch_cntxt);
+				}
+				qid_count++;
+			}
+
+			/* If there's not enough space to collect more qids,
+			 * then bail and move on to next queue type.
+			 */
+			if (limit_qid &&
+			    qid_count >= CUDBG_LOWMEM_MAX_CTXT_QIDS)
+				break;
+		}
+	}
+
+	scratch_buff.size = total_size;
+	rc = write_compression_hdr(&scratch_buff, dbg_buff);
+	if (rc)
+		goto err1;
+
+	/* Splitting buffer and writing in terms of CUDBG_CHUNK_SIZE */
+	while (total_size > 0) {
+		bytes = min_t(unsigned long, (unsigned long)total_size,
+			      (unsigned long)CUDBG_CHUNK_SIZE);
+		temp_buff.size = bytes;
+		temp_buff.data = (void *)((char *)scratch_buff.data +
+					  next_offset);
+
+		rc = compress_buff(pdbg_init->hash_table, &temp_buff, dbg_buff);
+		if (rc)
+			goto err1;
+
+		total_size -= bytes;
+		next_offset += bytes;
+	}
+
+err1:
+	scratch_buff.size = size;
+	release_scratch_buff(&scratch_buff, dbg_buff);
+err:
+	return rc;
+}
+
+static int collect_fw_devlog(struct cudbg_init *pdbg_init,
+			     struct cudbg_buffer *dbg_buff,
+			     struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct devlog_params *dparams = &padap->params.devlog;
+	struct cudbg_param *params = NULL;
+	struct cudbg_buffer scratch_buff;
+	u32 offset;
+	int rc = 0, i;
+
+	rc = t4_init_devlog_params(padap);
+	if (rc < 0) {
+		pdbg_init->print("%s(), t4_init_devlog_params failed!, rc: "\
+				 "%d\n", __func__, rc);
+		for (i = 0; i < pdbg_init->dbg_params_cnt; i++) {
+			if (pdbg_init->dbg_params[i].param_type ==
+			    CUDBG_DEVLOG_PARAM) {
+				params = &pdbg_init->dbg_params[i];
+				break;
+			}
+		}
+
+		if (params) {
+			dparams->memtype = params->u.devlog_param.memtype;
+			dparams->start = params->u.devlog_param.start;
+			dparams->size = params->u.devlog_param.size;
+		} else {
+			cudbg_err->sys_err = rc;
+			goto err;
+		}
+	}
+
+	GET_SCRATCH_BUFF(dbg_buff, dparams->size, &scratch_buff);
+	/* Collect FW devlog */
+	if (dparams->start != 0) {
+		offset = scratch_buff.offset;
+		rc = t4_memory_rw(padap, padap->params.drv_memwin,
+				  dparams->memtype, dparams->start,
+				  dparams->size,
+				  (__be32 *)((char *)scratch_buff.data +
+					     offset), 1);
+
+		if (rc) {
+			pdbg_init->print("%s(), t4_memory_rw failed!, rc: "\
+					 "%d\n", __func__, rc);
+			cudbg_err->sys_err = rc;
+			goto err1;
+		}
+	}
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+err:
+	return rc;
+}
+
+/* CIM OBQ */
+static int collect_cim_obq_ulp0(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 0);
+}
+
+static int collect_cim_obq_ulp1(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 1);
+}
+
+static int collect_cim_obq_ulp2(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 2);
+}
+
+static int collect_cim_obq_ulp3(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 3);
+}
+
+static int collect_cim_obq_sge(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 4);
+}
+
+static int collect_cim_obq_ncsi(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 5);
+}
+
+static int collect_obq_sge_rx_q0(struct cudbg_init *pdbg_init,
+				 struct cudbg_buffer *dbg_buff,
+				 struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 6);
+}
+
+static int collect_obq_sge_rx_q1(struct cudbg_init *pdbg_init,
+				 struct cudbg_buffer *dbg_buff,
+				 struct cudbg_error *cudbg_err)
+{
+	return read_cim_obq(pdbg_init, dbg_buff, cudbg_err, 7);
+}
+
+static int read_cim_obq(struct cudbg_init *pdbg_init,
+			struct cudbg_buffer *dbg_buff,
+			struct cudbg_error *cudbg_err, int qid)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int no_of_read_words, rc = 0;
+	u32 qsize;
+
+	/* collect CIM OBQ */
+	qsize =  6 * CIM_OBQ_SIZE * 4 *  sizeof(u32);
+	GET_SCRATCH_BUFF(dbg_buff, qsize, &scratch_buff);
+	/* t4_read_cim_obq will return no. of read words or error */
+	no_of_read_words = t4_read_cim_obq(padap, qid,
+					   (u32 *)((u32 *)scratch_buff.data +
+					   scratch_buff.offset), qsize);
+	/* no_of_read_words is less than or equal to 0 means error */
+	if (no_of_read_words <= 0) {
+		if (no_of_read_words == 0)
+			rc = CUDBG_SYSTEM_ERROR;
+		else
+			rc = no_of_read_words;
+		pdbg_init->print("%s(), t4_read_cim_obq failed!, rc: %d\n",
+				 __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+	scratch_buff.size = no_of_read_words * 4;
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+/* CIM IBQ */
+static int collect_cim_ibq_tp0(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	return read_cim_ibq(pdbg_init, dbg_buff, cudbg_err, 0);
+}
+
+static int collect_cim_ibq_tp1(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	return read_cim_ibq(pdbg_init, dbg_buff, cudbg_err, 1);
+}
+
+static int collect_cim_ibq_ulp(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	return read_cim_ibq(pdbg_init, dbg_buff, cudbg_err, 2);
+}
+
+static int collect_cim_ibq_sge0(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_ibq(pdbg_init, dbg_buff, cudbg_err, 3);
+}
+
+static int collect_cim_ibq_sge1(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_ibq(pdbg_init, dbg_buff, cudbg_err, 4);
+}
+
+static int collect_cim_ibq_ncsi(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return read_cim_ibq(pdbg_init, dbg_buff, cudbg_err, 5);
+}
+
+static int read_cim_ibq(struct cudbg_init *pdbg_init,
+			struct cudbg_buffer *dbg_buff,
+			struct cudbg_error *cudbg_err, int qid)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int no_of_read_words, rc = 0;
+	u32 qsize;
+
+	/* collect CIM IBQ */
+	qsize = CIM_IBQ_SIZE * 4 *  sizeof(u32);
+	GET_SCRATCH_BUFF(dbg_buff, qsize, &scratch_buff);
+	/* t4_read_cim_ibq will return no. of read words or error */
+	no_of_read_words = t4_read_cim_ibq(padap, qid,
+					   (u32 *)((u32 *)scratch_buff.data +
+					   scratch_buff.offset), qsize);
+	/* no_of_read_words is less than or equal to 0 means error */
+	if (no_of_read_words <= 0) {
+		if (no_of_read_words == 0)
+			rc = CUDBG_SYSTEM_ERROR;
+		else
+			rc = no_of_read_words;
+		pdbg_init->print("%s(), t4_read_cim_ibq failed!, rc: %d\n",
+				 __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_cim_ma_la(struct cudbg_init *pdbg_init,
+			     struct cudbg_buffer *dbg_buff,
+			     struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc = 0;
+
+	/* collect CIM MA LA */
+	scratch_buff.size =  2 * CIM_MALA_SIZE * 5 * sizeof(u32);
+	GET_SCRATCH_BUFF(dbg_buff, scratch_buff.size, &scratch_buff);
+	t4_cim_read_ma_la(padap,
+			  (u32 *) ((char *)scratch_buff.data +
+				   scratch_buff.offset),
+			  (u32 *) ((char *)scratch_buff.data +
+				   scratch_buff.offset + 5 * CIM_MALA_SIZE));
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_cim_la(struct cudbg_init *pdbg_init,
+			  struct cudbg_buffer *dbg_buff,
+			  struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int size, rc = 0;
+	u32 cfg = 0;
+
+	/* collect CIM LA */
+	if (is_t6(padap->params.chip)) {
+		size = padap->params.cim_la_size / 10 + 1;
+		size *= 11 * sizeof(u32);
+	} else {
+		size = padap->params.cim_la_size / 8;
+		size *= 8 * sizeof(u32);
+	}
+
+	size += sizeof(cfg);
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	rc = t4_cim_read(padap, UP_UP_DBG_LA_CFG_A, 1, &cfg);
+	if (rc) {
+		pdbg_init->print("%s(), t4_cim_read failed!, rc: %d\n",
+				 __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+
+	memcpy((char *)scratch_buff.data + scratch_buff.offset, &cfg,
+	       sizeof(cfg));
+	rc = t4_cim_read_la(padap,
+			    (u32 *) ((char *)scratch_buff.data +
+				     scratch_buff.offset + sizeof(cfg)), NULL);
+	if (rc < 0) {
+		pdbg_init->print("%s(), t4_cim_read_la failed!, rc: %d\n",
+				 __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_cim_qcfg(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct struct_cim_qcfg *cim_qcfg_data = NULL;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int cim_num_obq, rc = 0;
+	u32 offset;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*cim_qcfg_data), &scratch_buff);
+	offset = scratch_buff.offset;
+	cim_num_obq = is_t4(padap->params.chip) ? CIM_NUM_OBQ : CIM_NUM_OBQ_T5;
+	cim_qcfg_data =
+		(struct struct_cim_qcfg *)((u8 *)((char *)scratch_buff.data +
+					   offset));
+	rc = t4_cim_read(padap, UP_IBQ_0_RDADDR_A,
+			 ARRAY_SIZE(cim_qcfg_data->stat), cim_qcfg_data->stat);
+	if (rc) {
+		pdbg_init->print("%s(), t4_cim_read IBQ_0_RDADDR failed!, rc: "\
+				 "%d\n", __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+
+	rc = t4_cim_read(padap, UP_OBQ_0_REALADDR_A,
+			 ARRAY_SIZE(cim_qcfg_data->obq_wr),
+			 cim_qcfg_data->obq_wr);
+	if (rc) {
+		pdbg_init->print("%s(), t4_cim_read OBQ_0_REALADDR failed!, "\
+				 "rc: %d\n", __func__, rc);
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+
+	t4_read_cimq_cfg(padap,
+			cim_qcfg_data->base,
+			cim_qcfg_data->size,
+			cim_qcfg_data->thres);
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+/**
+ * Fetch the TX/RX payload regions start and end.
+ *
+ * @padap (IN): adapter handle.
+ * @mem_type (IN): EDC0, EDC1, MC/MC0/MC1.
+ * @mem_tot_len (IN): total length of @mem_type memory region to read.
+ * @payload_type (IN): TX or RX Payload.
+ * @reg_info (OUT): store the payload region info.
+ *
+ * Fetch the TX/RX payload region information from meminfo.
+ * However, reading from the @mem_type region starts at 0 and not
+ * from whatever base info is stored in meminfo.  Hence, if the
+ * payload region exists, then calculate the payload region
+ * start and end wrt 0 and @mem_tot_len, respectively, and set
+ * @reg_info->exist to true. Otherwise, set @reg_info->exist to false.
+ */
+static int get_payload_range(struct adapter *padap, u8 mem_type,
+			     unsigned long mem_tot_len, u8 payload_type,
+			     struct struct_region_info *reg_info)
+{
+	struct struct_mem_desc mem_region;
+	struct struct_mem_desc payload;
+	struct struct_meminfo meminfo;
+	u32 i, idx, found = 0;
+	u8 mc_type;
+	int rc;
+
+	/* Get meminfo of all regions */
+	rc = fill_meminfo(padap, &meminfo);
+	if (rc)
+		return rc;
+
+	/* Extract the specified TX or RX Payload region range */
+	memset(&payload, 0, sizeof(struct struct_mem_desc));
+	for (i = 0; i < meminfo.mem_c; i++) {
+		if (meminfo.mem[i].idx >= ARRAY_SIZE(region))
+			continue;                        /* skip holes */
+
+		idx = meminfo.mem[i].idx;
+		/* Get TX or RX Payload region start and end */
+		if (idx == payload_type) {
+			if (!(meminfo.mem[i].limit))
+				meminfo.mem[i].limit =
+					i < meminfo.mem_c - 1 ?
+					meminfo.mem[i + 1].base - 1 : ~0;
+
+			memcpy(&payload, &meminfo.mem[i], sizeof(payload));
+			found = 1;
+			break;
+		}
+	}
+
+	/* If TX or RX Payload region is not found return error. */
+	if (!found)
+		return -EINVAL;
+
+	if (mem_type < MEM_MC) {
+		memcpy(&mem_region, &meminfo.avail[mem_type],
+		       sizeof(mem_region));
+	} else {
+		/* Check if both MC0 and MC1 exist by checking if a
+		 * base address for the specified @mem_type exists.
+		 * If a base address exists, then there is MC1 and
+		 * hence use the base address stored at index 3.
+		 * Otherwise, use the base address stored at index 2.
+		 */
+		mc_type = meminfo.avail[mem_type].base ?
+			  mem_type : mem_type - 1;
+		memcpy(&mem_region, &meminfo.avail[mc_type],
+		       sizeof(mem_region));
+	}
+
+	/* Check if payload region exists in current memory */
+	if (payload.base < mem_region.base && payload.limit < mem_region.base) {
+		reg_info->exist = false;
+		return 0;
+	}
+
+	/* Get Payload region start and end with respect to 0 and
+	 * mem_tot_len, respectively.  This is because reading from the
+	 * memory region starts at 0 and not at base info stored in meminfo.
+	 */
+	if (payload.base < mem_region.limit) {
+		reg_info->exist = true;
+		if (payload.base >= mem_region.base)
+			reg_info->start = payload.base - mem_region.base;
+		else
+			reg_info->start = 0;
+
+		if (payload.limit < mem_region.limit)
+			reg_info->end = payload.limit - mem_region.base;
+		else
+			reg_info->end = mem_tot_len;
+	}
+
+	return 0;
+}
+
+static int read_fw_mem(struct cudbg_init *pdbg_init,
+			struct cudbg_buffer *dbg_buff, u8 mem_type,
+			unsigned long tot_len, struct cudbg_error *cudbg_err)
+{
+	struct struct_region_info payload[2]; /* TX and RX Payload Region */
+	unsigned long bytes, bytes_left, bytes_read = 0;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	u16 get_payload_flag;
+	int rc = 0;
+	u8 i;
+
+	get_payload_flag =
+		pdbg_init->dbg_params[CUDBG_GET_PAYLOAD_PARAM].param_type;
+
+	/* If explicitly asked to get TX/RX Payload data,
+	 * then don't zero out the payload data. Otherwise,
+	 * zero out the payload data.
+	 */
+	if (!get_payload_flag) {
+		u8 region_index[2];
+		u8 j = 0;
+
+		/* Find the index of TX and RX Payload regions in meminfo */
+		for (i = 0; i < ARRAY_SIZE(region); i++) {
+			if (!strcmp(region[i], "Tx payload:") ||
+			    !strcmp(region[i], "Rx payload:")) {
+				region_index[j] = i;
+				j++;
+				if (j == 2)
+					break;
+			}
+		}
+
+		/* Get TX/RX Payload region range if they exist */
+		memset(payload, 0, ARRAY_SIZE(payload) * sizeof(payload[0]));
+		for (i = 0; i < ARRAY_SIZE(payload); i++) {
+			rc = get_payload_range(padap, mem_type, tot_len,
+					       region_index[i],
+					       &payload[i]);
+			if (rc)
+				goto err;
+
+			if (payload[i].exist) {
+				/* Align start and end to avoid wrap around */
+				payload[i].start =
+					cudbg_round_up(payload[i].start,
+						       CUDBG_CHUNK_SIZE);
+				payload[i].end =
+					cudbg_round_down(payload[i].end,
+							 CUDBG_CHUNK_SIZE);
+			}
+		}
+	}
+
+	bytes_left = tot_len;
+	scratch_buff.size = tot_len;
+	rc = write_compression_hdr(&scratch_buff, dbg_buff);
+	if (rc)
+		goto err;
+
+	while (bytes_left > 0) {
+		bytes = min_t(unsigned long, bytes_left, (unsigned long)CUDBG_CHUNK_SIZE);
+		rc = get_scratch_buff(dbg_buff, bytes, &scratch_buff);
+		if (rc) {
+			rc = CUDBG_STATUS_NO_SCRATCH_MEM;
+			goto err;
+		}
+
+		if (!get_payload_flag) {
+			for (i = 0; i < ARRAY_SIZE(payload); i++) {
+				if (payload[i].exist &&
+				    bytes_read >= payload[i].start &&
+				    (bytes_read + bytes) <= payload[i].end) {
+					memset(scratch_buff.data, 0, bytes);
+					/* TX and RX Payload regions
+					 * can't overlap.
+					 */
+					goto skip_read;
+				}
+			}
+		}
+
+		/* Read from file */
+		/*fread(scratch_buff.data, 1, Bytes, in);*/
+		rc = t4_memory_rw(padap, MEMWIN_NIC, mem_type, bytes_read,
+				  bytes, (__be32 *)(scratch_buff.data), 1);
+		if (rc) {
+			pdbg_init->print("%s(), t4_memory_rw failed!, rc: "\
+					 "%d\n", __func__, rc);
+			cudbg_err->sys_err = rc;
+			goto err1;
+		}
+
+skip_read:
+		rc = compress_buff(pdbg_init->hash_table, &scratch_buff,
+				   dbg_buff);
+		if (rc)
+			goto err1;
+
+		bytes_left -= bytes;
+		bytes_read += bytes;
+		release_scratch_buff(&scratch_buff, dbg_buff);
+	}
+
+err1:
+	if (rc)
+		release_scratch_buff(&scratch_buff, dbg_buff);
+err:
+	return rc;
+}
+
+static void collect_mem_info(struct cudbg_init *pdbg_init,
+			     struct card_mem *mem_info)
+{
+	struct adapter *padap = pdbg_init->adap;
+	int t4 = 0;
+	u32 value;
+
+	if (is_t4(padap->params.chip))
+		t4 = 1;
+
+	if (t4) {
+		value = t4_read_reg(padap, MA_EXT_MEMORY_BAR_A);
+		value = EXT_MEM_SIZE_G(value);
+		mem_info->size_mc0 = (u16)value;  /* size in MB */
+
+		value = t4_read_reg(padap, MA_TARGET_MEM_ENABLE_A);
+		if (value & EXT_MEM_ENABLE_F)
+			mem_info->mem_flag |= (1 << MC0_FLAG); /* set mc0 flag
+								  bit */
+	} else {
+		value = t4_read_reg(padap, MA_EXT_MEMORY0_BAR_A);
+		value = EXT_MEM0_SIZE_G(value);
+		mem_info->size_mc0 = (u16)value;
+
+		value = t4_read_reg(padap, MA_EXT_MEMORY1_BAR_A);
+		value = EXT_MEM1_SIZE_G(value);
+		mem_info->size_mc1 = (u16)value;
+
+		value = t4_read_reg(padap, MA_TARGET_MEM_ENABLE_A);
+		if (value & EXT_MEM0_ENABLE_F)
+			mem_info->mem_flag |= (1 << MC0_FLAG);
+		if (value & EXT_MEM1_ENABLE_F)
+			mem_info->mem_flag |= (1 << MC1_FLAG);
+	}
+
+	value = t4_read_reg(padap, MA_EDRAM0_BAR_A);
+	value = EDRAM0_SIZE_G(value);
+	mem_info->size_edc0 = (u16)value;
+
+	value = t4_read_reg(padap, MA_EDRAM1_BAR_A);
+	value = EDRAM1_SIZE_G(value);
+	mem_info->size_edc1 = (u16)value;
+
+	value = t4_read_reg(padap, MA_TARGET_MEM_ENABLE_A);
+	if (value & EDRAM0_ENABLE_F)
+		mem_info->mem_flag |= (1 << EDC0_FLAG);
+	if (value & EDRAM1_ENABLE_F)
+		mem_info->mem_flag |= (1 << EDC1_FLAG);
+}
+
+static void cudbg_t4_fwcache(struct cudbg_init *pdbg_init,
+				struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	int rc;
+
+	if (is_fw_attached(pdbg_init)) {
+
+		/* Flush uP dcache before reading edcX/mcX  */
+		rc = t4_fwcache(padap, FW_PARAM_DEV_FWCACHE_FLUSH);
+
+		if (rc) {
+			pdbg_init->print("%s(), Warning: t4_fwcache failed!, rc: %d\n",
+				 __func__, rc);
+			cudbg_err->sys_warn = rc;
+		}
+	}
+}
+
+static int collect_mem_region(struct cudbg_init *pdbg_init,
+			      struct cudbg_buffer *dbg_buff,
+			      struct cudbg_error *cudbg_err,
+			      u8 mem_type)
+{
+	struct card_mem mem_info = {0};
+	unsigned long flag, size;
+	int rc;
+
+	cudbg_t4_fwcache(pdbg_init, cudbg_err);
+	collect_mem_info(pdbg_init, &mem_info);
+	switch (mem_type) {
+	case MEM_EDC0:
+		flag = (1 << EDC0_FLAG);
+		size = (((unsigned long)mem_info.size_edc0) * 1024 * 1024);
+		break;
+	case MEM_EDC1:
+		flag = (1 << EDC1_FLAG);
+		size = (((unsigned long)mem_info.size_edc1) * 1024 * 1024);
+		break;
+	case MEM_MC0:
+		flag = (1 << MC0_FLAG);
+		size = (((unsigned long)mem_info.size_mc0) * 1024 * 1024);
+		break;
+	case MEM_MC1:
+		flag = (1 << MC1_FLAG);
+		size = (((unsigned long)mem_info.size_mc1) * 1024 * 1024);
+		break;
+	default:
+		rc = CUDBG_STATUS_ENTITY_NOT_FOUND;
+		goto err;
+	}
+
+	if (mem_info.mem_flag & flag) {
+		rc = read_fw_mem(pdbg_init, dbg_buff, mem_type,
+				 size, cudbg_err);
+		if (rc)
+			goto err;
+	} else {
+		rc = CUDBG_STATUS_ENTITY_NOT_FOUND;
+		pdbg_init->print("%s(), collect_mem_info failed!, %s\n",
+				 __func__, err_msg[-rc]);
+		goto err;
+	}
+err:
+	return rc;
+}
+
+static int collect_edc0_meminfo(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return collect_mem_region(pdbg_init, dbg_buff, cudbg_err, MEM_EDC0);
+}
+
+static int collect_edc1_meminfo(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	return collect_mem_region(pdbg_init, dbg_buff, cudbg_err, MEM_EDC1);
+}
+
+static int collect_mc0_meminfo(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	return collect_mem_region(pdbg_init, dbg_buff, cudbg_err, MEM_MC0);
+}
+
+static int collect_mc1_meminfo(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	return collect_mem_region(pdbg_init, dbg_buff, cudbg_err, MEM_MC1);
+}
+
+static int collect_hma_meminfo(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	struct card_mem mem_info = {0};
+	unsigned long hma_size;
+	int rc;
+
+	cudbg_t4_fwcache(pdbg_init, cudbg_err);
+	collect_mem_info(pdbg_init, &mem_info);
+	if (mem_info.mem_flag & (1 << HMA_FLAG)) {
+		hma_size = (((unsigned long)mem_info.size_hma) * 1024 * 1024);
+		rc = read_fw_mem(pdbg_init, dbg_buff, MEM_HMA,
+				 hma_size, cudbg_err);
+	} else {
+		rc = CUDBG_STATUS_ENTITY_NOT_FOUND;
+		pdbg_init->print("%s(), collect_mem_info failed!, %s\n",
+				 __func__, err_msg[-rc]);
+	}
+
+	return rc;
+}
+
+static int collect_reg_dump(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct cudbg_buffer tmp_scratch_buff, scratch_buff;
+	unsigned long bytes, bytes_left, bytes_read = 0;
+	struct adapter *padap = pdbg_init->adap;
+	u32 buf_size = 0;
+	int rc = 0;
+
+	if (is_t4(padap->params.chip))
+		buf_size = T4_REGMAP_SIZE ;/*+ sizeof(unsigned int);*/
+	else if (is_t5(padap->params.chip) || is_t6(padap->params.chip))
+		buf_size = T5_REGMAP_SIZE;
+
+	scratch_buff.size = buf_size;
+	tmp_scratch_buff = scratch_buff;
+	GET_SCRATCH_BUFF(dbg_buff, scratch_buff.size, &scratch_buff);
+	t4_get_regs(padap, (void *)scratch_buff.data, scratch_buff.size);
+	bytes_left = scratch_buff.size;
+	rc = write_compression_hdr(&scratch_buff, dbg_buff);
+	if (rc)
+		goto err1;
+
+	while (bytes_left > 0) {
+		tmp_scratch_buff.data =
+			((char *)scratch_buff.data) + bytes_read;
+		bytes = min_t(unsigned long, bytes_left, (unsigned long)CUDBG_CHUNK_SIZE);
+		tmp_scratch_buff.size = bytes;
+		compress_buff(pdbg_init->hash_table, &tmp_scratch_buff,
+			      dbg_buff);
+		bytes_left -= bytes;
+		bytes_read += bytes;
+	}
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_cctrl(struct cudbg_init *pdbg_init,
+			 struct cudbg_buffer *dbg_buff,
+			 struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	u32 size;
+	int rc;
+
+	size = sizeof(u16) * NMTUS * NCCTRL_WIN;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	t4_read_cong_tbl(padap, (void *)scratch_buff.data);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int check_busy_bit(struct adapter *padap)
+{
+	int status = 0, retry = 10, i = 0;
+	u32 val, busy = 1;
+
+	while (busy & (1 < retry)) {
+		val = t4_read_reg(padap, CIM_HOST_ACC_CTRL_A);
+		busy = (0 != (val & CUDBG_CIM_BUSY_BIT));
+		i++;
+	}
+	if (busy)
+		status = -1;
+	return status;
+}
+
+static int cim_ha_rreg(struct adapter *padap, u32 addr, u32 *val)
+{
+	int rc = 0;
+
+	/* write register address into the CIM_HOST_ACC_CTRL_A */
+	t4_write_reg(padap, CIM_HOST_ACC_CTRL_A, addr);
+	/* Poll HOSTBUSY */
+	rc = check_busy_bit(padap);
+	if (rc)
+		goto err;
+	/* Read value from CIM_HOST_ACC_DATA_A */
+	*val = t4_read_reg(padap, CIM_HOST_ACC_DATA_A);
+err:
+	return rc;
+}
+
+static int dump_up_cim(struct adapter *padap, struct cudbg_init *pdbg_init,
+		       struct ireg_field *up_cim_reg, u32 *buff)
+{
+	int rc = 0;
+	u32 i;
+
+	for (i = 0; i < up_cim_reg->ireg_offset_range; i++) {
+		rc = cim_ha_rreg(padap, up_cim_reg->ireg_local_offset + (i * 4),
+				 buff);
+		if (rc) {
+			pdbg_init->print("BUSY timeout reading"
+					 "CIM_HOST_ACC_CTRL\n");
+			goto err;
+		}
+		buff++;
+	}
+err:
+	return rc;
+}
+
+static int collect_up_cim_indirect(struct cudbg_init *pdbg_init,
+				   struct cudbg_buffer *dbg_buff,
+				   struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *up_cim;
+	int i, rc, n;
+	u32 size;
+
+	n = sizeof(t5_up_cim_reg_array) / (4 * sizeof(u32));
+	size = sizeof(struct ireg_buf) * n;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	up_cim = (struct ireg_buf *)scratch_buff.data;
+	for (i = 0; i < n; i++) {
+		struct ireg_field *up_cim_reg = &up_cim->tp_pio;
+		u32 *buff = up_cim->outbuf;
+
+		if (is_t5(padap->params.chip)) {
+			up_cim_reg->ireg_addr = t5_up_cim_reg_array[i][0];
+			up_cim_reg->ireg_data = t5_up_cim_reg_array[i][1];
+			up_cim_reg->ireg_local_offset =
+						t5_up_cim_reg_array[i][2];
+			up_cim_reg->ireg_offset_range =
+						t5_up_cim_reg_array[i][3];
+		} else if (is_t6(padap->params.chip)) {
+			up_cim_reg->ireg_addr = t6_up_cim_reg_array[i][0];
+			up_cim_reg->ireg_data = t6_up_cim_reg_array[i][1];
+			up_cim_reg->ireg_local_offset =
+						t6_up_cim_reg_array[i][2];
+			up_cim_reg->ireg_offset_range =
+						t6_up_cim_reg_array[i][3];
+		}
+
+		rc = dump_up_cim(padap, pdbg_init, up_cim_reg, buff);
+		up_cim++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_mbox_log(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct cudbg_mbox_log *mboxlog = NULL;
+	struct cudbg_buffer scratch_buff;
+	struct mbox_cmd_log *log = NULL;
+	struct mbox_cmd *entry;
+	unsigned int entry_idx;
+	u16 mbox_cmds;
+	int i, k, rc;
+	u64 flit;
+	u32 size;
+
+	if (pdbg_init->dbg_params[CUDBG_MBOX_LOG_PARAM].u.mboxlog_param.log) {
+		log = pdbg_init->dbg_params[CUDBG_MBOX_LOG_PARAM].u.
+			mboxlog_param.log;
+		mbox_cmds = pdbg_init->dbg_params[CUDBG_MBOX_LOG_PARAM].u.
+				mboxlog_param.mbox_cmds;
+	} else {
+		pdbg_init->print("Mbox log is not requested\n");
+		return CUDBG_STATUS_ENTITY_NOT_REQUESTED;
+	}
+
+	size = sizeof(struct cudbg_mbox_log) * mbox_cmds;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	mboxlog = (struct cudbg_mbox_log *)scratch_buff.data;
+	for (k = 0; k < mbox_cmds; k++) {
+		entry_idx = log->cursor + k;
+		if (entry_idx >= log->size)
+			entry_idx -= log->size;
+
+		entry = mbox_cmd_log_entry(log, entry_idx);
+		/* skip over unused entries */
+		if (entry->timestamp == 0)
+			continue;
+
+		memcpy(&mboxlog->entry, entry, sizeof(struct mbox_cmd));
+		for (i = 0; i < MBOX_LEN / 8; i++) {
+			flit = entry->cmd[i];
+			mboxlog->hi[i] = (u32)(flit >> 32);
+			mboxlog->lo[i] = (u32)flit;
+		}
+		mboxlog++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_pbt_tables(struct cudbg_init *pdbg_init,
+			      struct cudbg_buffer *dbg_buff,
+			      struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_pbt_tables *pbt = NULL;
+	struct cudbg_buffer scratch_buff;
+	int i, rc;
+	u32 addr;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*pbt), &scratch_buff);
+	pbt = (struct cudbg_pbt_tables *)scratch_buff.data;
+	/* PBT dynamic entries */
+	addr = CUDBG_CHAC_PBT_ADDR;
+	for (i = 0; i < CUDBG_PBT_DYNAMIC_ENTRIES; i++) {
+		rc = cim_ha_rreg(padap, addr + (i * 4), &pbt->pbt_dynamic[i]);
+		if (rc) {
+			pdbg_init->print("BUSY timeout reading"
+					 "CIM_HOST_ACC_CTRL\n");
+			goto err1;
+		}
+	}
+
+	/* PBT static entries */
+	/* static entries start when bit 6 is set */
+	addr = CUDBG_CHAC_PBT_ADDR + (1 << 6);
+	for (i = 0; i < CUDBG_PBT_STATIC_ENTRIES; i++) {
+		rc = cim_ha_rreg(padap, addr + (i * 4), &pbt->pbt_static[i]);
+		if (rc) {
+			pdbg_init->print("BUSY timeout reading"
+					 "CIM_HOST_ACC_CTRL\n");
+			goto err1;
+		}
+	}
+
+	/* LRF entries */
+	addr = CUDBG_CHAC_PBT_LRF;
+	for (i = 0; i < CUDBG_LRF_ENTRIES; i++) {
+		rc = cim_ha_rreg(padap, addr + (i * 4), &pbt->lrf_table[i]);
+		if (rc) {
+			pdbg_init->print("BUSY timeout reading"
+					 "CIM_HOST_ACC_CTRL\n");
+			goto err1;
+		}
+	}
+
+	/* PBT data entries */
+	addr = CUDBG_CHAC_PBT_DATA;
+	for (i = 0; i < CUDBG_PBT_DATA_ENTRIES; i++) {
+		rc = cim_ha_rreg(padap, addr + (i * 4), &pbt->pbt_data[i]);
+		if (rc) {
+			pdbg_init->print("BUSY timeout reading"
+					 "CIM_HOST_ACC_CTRL\n");
+			goto err1;
+		}
+	}
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_pm_indirect(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *ch_pm;
+	int i, rc, n;
+	u32 size;
+
+	n = sizeof(t5_pm_rx_array) / (4 * sizeof(u32));
+	size = sizeof(struct ireg_buf) * n * 2;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	ch_pm = (struct ireg_buf *)scratch_buff.data;
+	/*PM_RX*/
+	for (i = 0; i < n; i++) {
+		struct ireg_field *pm_pio = &ch_pm->tp_pio;
+		u32 *buff = ch_pm->outbuf;
+
+		pm_pio->ireg_addr = t5_pm_rx_array[i][0];
+		pm_pio->ireg_data = t5_pm_rx_array[i][1];
+		pm_pio->ireg_local_offset = t5_pm_rx_array[i][2];
+		pm_pio->ireg_offset_range = t5_pm_rx_array[i][3];
+		t4_read_indirect(padap,
+				pm_pio->ireg_addr,
+				pm_pio->ireg_data,
+				buff,
+				pm_pio->ireg_offset_range,
+				pm_pio->ireg_local_offset);
+		ch_pm++;
+	}
+
+	/*PM_Tx*/
+	n = sizeof(t5_pm_tx_array) / (4 * sizeof(u32));
+	for (i = 0; i < n; i++) {
+		struct ireg_field *pm_pio = &ch_pm->tp_pio;
+		u32 *buff = ch_pm->outbuf;
+
+		pm_pio->ireg_addr = t5_pm_tx_array[i][0];
+		pm_pio->ireg_data = t5_pm_tx_array[i][1];
+		pm_pio->ireg_local_offset = t5_pm_tx_array[i][2];
+		pm_pio->ireg_offset_range = t5_pm_tx_array[i][3];
+		t4_read_indirect(padap,
+				pm_pio->ireg_addr,
+				pm_pio->ireg_data,
+				buff,
+				pm_pio->ireg_offset_range,
+				pm_pio->ireg_local_offset);
+		ch_pm++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_tid(struct cudbg_init *pdbg_init,
+		       struct cudbg_buffer *dbg_buff,
+		       struct cudbg_error *cudbg_err)
+{
+
+	struct adapter *padap = pdbg_init->adap;
+	struct tid_info_region_rev1 *tid1;
+	struct cudbg_buffer scratch_buff;
+	struct tid_info_region *tid;
+	u32 para[7], val[7];
+	u32 mbox, pf;
+	int rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tid1), &scratch_buff);
+
+#define FW_PARAM_DEV_A(param) \
+	(FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DEV) | \
+	 FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DEV_##param))
+#define FW_PARAM_PFVF_A(param) \
+	(FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_PFVF) | \
+	 FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_PFVF_##param)|  \
+	 FW_PARAMS_PARAM_Y_V(0) | \
+	 FW_PARAMS_PARAM_Z_V(0))
+#define MAX_ATIDS_A 8192U
+
+	tid1 = (struct tid_info_region_rev1 *)scratch_buff.data;
+	tid = &(tid1->tid);
+	tid1->ver_hdr.signature = CUDBG_ENTITY_SIGNATURE;
+	tid1->ver_hdr.revision = CUDBG_TID_INFO_REV;
+	tid1->ver_hdr.size = sizeof(struct tid_info_region_rev1) -
+			     sizeof(struct cudbg_ver_hdr);
+
+	if (is_t5(padap->params.chip)) {
+		tid->hash_base = t4_read_reg(padap, LE_DB_TID_HASHBASE_A);
+		tid1->tid_start = 0;
+	} else if (is_t6(padap->params.chip)) {
+		tid->hash_base = t4_read_reg(padap, T6_LE_DB_HASH_TID_BASE_A);
+		tid1->tid_start = t4_read_reg(padap, LE_DB_ACTIVE_TABLE_START_INDEX_A);
+	}
+
+	tid->le_db_conf = t4_read_reg(padap, LE_DB_CONFIG_A);
+
+	para[0] = FW_PARAM_PFVF_A(FILTER_START);
+	para[1] = FW_PARAM_PFVF_A(FILTER_END);
+	para[2] = FW_PARAM_PFVF_A(ACTIVE_FILTER_START);
+	para[3] = FW_PARAM_PFVF_A(ACTIVE_FILTER_END);
+	para[4] = FW_PARAM_DEV_A(NTID);
+	para[5] = FW_PARAM_PFVF_A(SERVER_START);
+	para[6] = FW_PARAM_PFVF_A(SERVER_END);
+
+	mbox = padap->mbox;
+	pf = padap->pf;
+	rc = t4_query_params(padap, mbox, pf, 0, 7, para, val);
+	if (rc <  0) {
+		if (rc == -FW_EPERM) {
+			/* It looks like we don't have permission to use
+			 * padap->mbox.
+			 *
+			 * Try mbox 4.  If it works, we'll continue to
+			 * collect the rest of tid info from mbox 4.
+			 * Else, quit trying to collect tid info.
+			 */
+			mbox = 4;
+			pf = 4;
+			rc = t4_query_params(padap, mbox, pf, 0, 7, para, val);
+			if (rc < 0) {
+				cudbg_err->sys_err = rc;
+				goto err1;
+			}
+		} else {
+			cudbg_err->sys_err = rc;
+			goto err1;
+		}
+	}
+
+	tid->ftid_base = val[0];
+	tid->nftids = val[1] - val[0] + 1;
+	/*active filter region*/
+	if (val[2] != val[3]) {
+		tid->flags |= FW_OFLD_CONN;
+		tid->aftid_base = val[2];
+		tid->aftid_end = val[3];
+	}
+	tid->ntids = val[4];
+	tid->natids = min_t(u32, tid->ntids / 2, MAX_ATIDS_A);
+	tid->stid_base = val[5];
+	tid->nstids = val[6] - val[5] + 1;
+
+	if (CHELSIO_CHIP_VERSION(padap->params.chip) >= CHELSIO_T6) {
+		para[0] = FW_PARAM_PFVF_A(HPFILTER_START);
+		para[1] = FW_PARAM_PFVF_A(HPFILTER_END);
+		rc = t4_query_params(padap, mbox, pf, 0, 2, para, val);
+		if (rc < 0) {
+			cudbg_err->sys_err = rc;
+			goto err1;
+		}
+
+		tid->hpftid_base = val[0];
+		tid->nhpftids = val[1] - val[0] + 1;
+	}
+
+	if (CHELSIO_CHIP_VERSION(padap->params.chip) <= CHELSIO_T5) {
+		tid->sb = t4_read_reg(padap, LE_DB_SERVER_INDEX_A) / 4;
+		tid->hash_base /= 4;
+	} else
+		tid->sb = t4_read_reg(padap, LE_DB_SRVR_START_INDEX_A);
+
+	/*UO context range*/
+	para[0] = FW_PARAM_PFVF_A(ETHOFLD_START);
+	para[1] = FW_PARAM_PFVF_A(ETHOFLD_END);
+
+	rc = t4_query_params(padap, mbox, pf, 0, 2, para, val);
+	if (rc <  0) {
+		cudbg_err->sys_err = rc;
+		goto err1;
+	}
+
+	if (val[0] != val[1]) {
+		tid->uotid_base = val[0];
+		tid->nuotids = val[1] - val[0] + 1;
+	}
+	tid->IP_users = t4_read_reg(padap, LE_DB_ACT_CNT_IPV4_A);
+	tid->IPv6_users = t4_read_reg(padap, LE_DB_ACT_CNT_IPV6_A);
+
+#undef FW_PARAM_PFVF_A
+#undef FW_PARAM_DEV_A
+#undef MAX_ATIDS_A
+
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_tx_rate(struct cudbg_init *pdbg_init,
+			   struct cudbg_buffer *dbg_buff,
+			   struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct tx_rate *tx_rate;
+	int rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*tx_rate), &scratch_buff);
+	tx_rate = (struct tx_rate *)scratch_buff.data;
+	t4_get_chan_txrate(padap, tx_rate->nrate, tx_rate->orate);
+	tx_rate->nchan = padap->params.arch.nchan;
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static inline void cudbg_tcamxy2valmask(u64 x, u64 y, u8 *addr, u64 *mask)
+{
+	*mask = x | y;
+	y = (__force u64)cpu_to_be64(y);
+	memcpy(addr, (char *)&y + 2, ETH_ALEN);
+}
+
+void mps_rpl_backdoor(struct adapter *padap, struct fw_ldst_mps_rplc *mps_rplc)
+{
+	if (is_t5(padap->params.chip)) {
+		mps_rplc->rplc255_224 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP3_A));
+		mps_rplc->rplc223_192 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP2_A));
+		mps_rplc->rplc191_160 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP1_A));
+		mps_rplc->rplc159_128 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP0_A));
+	} else {
+		mps_rplc->rplc255_224 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP7_A));
+		mps_rplc->rplc223_192 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP6_A));
+		mps_rplc->rplc191_160 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP5_A));
+		mps_rplc->rplc159_128 = htonl(t4_read_reg(padap,
+							  MPS_VF_RPLCT_MAP4_A));
+	}
+	mps_rplc->rplc127_96 = htonl(t4_read_reg(padap, MPS_VF_RPLCT_MAP3_A));
+	mps_rplc->rplc95_64 = htonl(t4_read_reg(padap, MPS_VF_RPLCT_MAP2_A));
+	mps_rplc->rplc63_32 = htonl(t4_read_reg(padap, MPS_VF_RPLCT_MAP1_A));
+	mps_rplc->rplc31_0 = htonl(t4_read_reg(padap, MPS_VF_RPLCT_MAP0_A));
+}
+
+static int collect_mps_tcam(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_mps_tcam *tcam = NULL;
+	u32 size = 0, i, n, total_size = 0;
+	struct cudbg_buffer scratch_buff;
+	u64 tcamy, tcamx, val;
+	u32 ctl, data2;
+	int rc;
+
+	n = padap->params.arch.mps_tcam_size;
+	size = sizeof(struct cudbg_mps_tcam) * n;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	memset(scratch_buff.data, 0, size);
+	tcam = (struct cudbg_mps_tcam *)scratch_buff.data;
+	for (i = 0; i < n; i++) {
+		if (CHELSIO_CHIP_VERSION(padap->params.chip) >= CHELSIO_T6) {
+			/* CtlReqID   - 1: use Host Driver Requester ID
+			 * CtlCmdType - 0: Read, 1: Write
+			 * CtlTcamSel - 0: TCAM0, 1: TCAM1
+			 * CtlXYBitSel- 0: Y bit, 1: X bit
+			 */
+
+			/* Read tcamy */
+			ctl = (CTLREQID_V(1) |
+			       CTLCMDTYPE_V(0) | CTLXYBITSEL_V(0));
+			if (i < 256)
+				ctl |= CTLTCAMINDEX_V(i) | CTLTCAMSEL_V(0);
+			else
+				ctl |= CTLTCAMINDEX_V(i - 256) |
+				       CTLTCAMSEL_V(1);
+
+			t4_write_reg(padap, MPS_CLS_TCAM_DATA2_CTL_A, ctl);
+			val = t4_read_reg(padap, MPS_CLS_TCAM_RDATA1_REQ_ID1_A);
+			tcamy = DMACH_G(val) << 32;
+			tcamy |= t4_read_reg(padap, MPS_CLS_TCAM_RDATA0_REQ_ID1_A);
+			data2 = t4_read_reg(padap, MPS_CLS_TCAM_RDATA2_REQ_ID1_A);
+			tcam->lookup_type = DATALKPTYPE_G(data2);
+
+			/* 0 - Outer header, 1 - Inner header
+			 * [71:48] bit locations are overloaded for
+			 * outer vs. inner lookup types.
+			 */
+
+			if (tcam->lookup_type &&
+			    (tcam->lookup_type != DATALKPTYPE_M)) {
+				/* Inner header VNI */
+				tcam->vniy = (((data2 & DATAVIDH2_F) |
+					     (DATAVIDH1_G(data2))) << 16) |
+					     VIDL_G(val);
+				tcam->dip_hit = data2 & DATADIPHIT_F;
+			} else {
+				tcam->vlan_vld = data2 & DATAVIDH2_F;
+				tcam->ivlan = VIDL_G(val);
+			}
+
+			tcam->port_num = DATAPORTNUM_G(data2);
+
+			/* Read tcamx. Change the control param */
+			ctl |= CTLXYBITSEL_V(1);
+			t4_write_reg(padap, MPS_CLS_TCAM_DATA2_CTL_A, ctl);
+			val = t4_read_reg(padap, MPS_CLS_TCAM_RDATA1_REQ_ID1_A);
+			tcamx = DMACH_G(val) << 32;
+			tcamx |= t4_read_reg(padap, MPS_CLS_TCAM_RDATA0_REQ_ID1_A);
+			data2 = t4_read_reg(padap, MPS_CLS_TCAM_RDATA2_REQ_ID1_A);
+			if (tcam->lookup_type &&
+			    (tcam->lookup_type != DATALKPTYPE_M)) {
+				/* Inner header VNI mask */
+				tcam->vnix = (((data2 & DATAVIDH2_F) |
+					     (DATAVIDH1_G(data2))) << 16) |
+					     VIDL_G(val);
+			}
+		} else {
+			tcamy = t4_read_reg64(padap, MPS_CLS_TCAM_Y_L(i));
+			tcamx = t4_read_reg64(padap, MPS_CLS_TCAM_X_L(i));
+		}
+
+		if (tcamx & tcamy)
+			continue;
+
+		tcam->cls_lo = t4_read_reg(padap, MPS_CLS_SRAM_L(i));
+		tcam->cls_hi = t4_read_reg(padap, MPS_CLS_SRAM_H(i));
+
+		if (is_t5(padap->params.chip))
+			tcam->repli = (tcam->cls_lo & REPLICATE_F);
+		else if (is_t6(padap->params.chip))
+			tcam->repli = (tcam->cls_lo & T6_REPLICATE_F);
+
+		if (tcam->repli) {
+			struct fw_ldst_cmd ldst_cmd;
+			struct fw_ldst_mps_rplc mps_rplc;
+
+			memset(&ldst_cmd, 0, sizeof(ldst_cmd));
+			ldst_cmd.op_to_addrspace =
+				htonl(FW_CMD_OP_V(FW_LDST_CMD) |
+				      FW_CMD_REQUEST_F |
+				      FW_CMD_READ_F |
+				      FW_LDST_CMD_ADDRSPACE_V(
+					      FW_LDST_ADDRSPC_MPS));
+
+			ldst_cmd.cycles_to_len16 = htonl(FW_LEN16(ldst_cmd));
+
+			ldst_cmd.u.mps.rplc.fid_idx =
+				htons(FW_LDST_CMD_FID_V(FW_LDST_MPS_RPLC) |
+				      FW_LDST_CMD_IDX_V(i));
+
+			rc = t4_wr_mbox(padap, padap->mbox, &ldst_cmd,
+					sizeof(ldst_cmd), &ldst_cmd);
+
+			if (rc)
+				mps_rpl_backdoor(padap, &mps_rplc);
+			else
+				mps_rplc = ldst_cmd.u.mps.rplc;
+
+			tcam->rplc[0] = ntohl(mps_rplc.rplc31_0);
+			tcam->rplc[1] = ntohl(mps_rplc.rplc63_32);
+			tcam->rplc[2] = ntohl(mps_rplc.rplc95_64);
+			tcam->rplc[3] = ntohl(mps_rplc.rplc127_96);
+			if (padap->params.arch.mps_rplc_size >
+					CUDBG_MAX_RPLC_SIZE) {
+				tcam->rplc[4] = ntohl(mps_rplc.rplc159_128);
+				tcam->rplc[5] = ntohl(mps_rplc.rplc191_160);
+				tcam->rplc[6] = ntohl(mps_rplc.rplc223_192);
+				tcam->rplc[7] = ntohl(mps_rplc.rplc255_224);
+			}
+		}
+		cudbg_tcamxy2valmask(tcamx, tcamy, tcam->addr, &tcam->mask);
+
+		tcam->idx = i;
+		tcam->rplc_size = padap->params.arch.mps_rplc_size;
+
+		total_size += sizeof(struct cudbg_mps_tcam);
+
+		tcam++;
+	}
+
+	if (total_size == 0) {
+		rc = CUDBG_SYSTEM_ERROR;
+		goto err1;
+	}
+
+	scratch_buff.size = total_size;
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	scratch_buff.size = size;
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_pcie_config(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	u32 size, *value, j;
+	int i, rc, n;
+
+	size = sizeof(u32) * NUM_PCIE_CONFIG_REGS;
+	n = sizeof(t5_pcie_config_array) / (2 * sizeof(u32));
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	value = (u32 *)scratch_buff.data;
+	for (i = 0; i < n; i++) {
+		for (j = t5_pcie_config_array[i][0];
+		     j <= t5_pcie_config_array[i][1]; j += 4) {
+			t4_hw_pci_read_cfg4(padap, j, value);
+			value++;
+		}
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int cudbg_read_tid(struct cudbg_init *pdbg_init, u32 tid,
+			  struct cudbg_tid_data *tid_data)
+{
+	struct adapter *padap = pdbg_init->adap;
+	int i, cmd_retry = 8;
+	u32 val;
+
+	/* Fill REQ_DATA regs with 0's */
+	for (i = 0; i < CUDBG_NUM_REQ_REGS; i++)
+		t4_write_reg(padap, LE_DB_DBGI_REQ_DATA_A + (i << 2), 0);
+
+	/* Write DBIG command */
+	val = (0x4 << DBGICMD_S) | tid;
+	t4_write_reg(padap, LE_DB_DBGI_REQ_TCAM_CMD_A, val);
+	tid_data->dbig_cmd = val;
+
+	val = 0;
+	val |= 1 << DBGICMDSTRT_S;
+	val |= 1;  /* LE mode */
+	t4_write_reg(padap, LE_DB_DBGI_CONFIG_A, val);
+	tid_data->dbig_conf = val;
+
+	/* Poll the DBGICMDBUSY bit */
+	val = 1;
+	while (val) {
+		val = t4_read_reg(padap, LE_DB_DBGI_CONFIG_A);
+		val = (val >> DBGICMDBUSY_S) & 1;
+		cmd_retry--;
+		if (!cmd_retry) {
+			pdbg_init->print("%s(): Timeout waiting for non-busy\n",
+					 __func__);
+			return CUDBG_SYSTEM_ERROR;
+		}
+	}
+
+	/* Check RESP status */
+	val = 0;
+	val = t4_read_reg(padap, LE_DB_DBGI_RSP_STATUS_A);
+	tid_data->dbig_rsp_stat = val;
+	if (!(val & 1)) {
+		pdbg_init->print("%s(): DBGI command failed\n", __func__);
+		return CUDBG_SYSTEM_ERROR;
+	}
+
+	/* Read RESP data */
+	for (i = 0; i < CUDBG_NUM_REQ_REGS; i++)
+		tid_data->data[i] = t4_read_reg(padap,
+						LE_DB_DBGI_RSP_DATA_A +
+						(i << 2));
+
+	tid_data->tid = tid;
+	return 0;
+}
+
+static int collect_le_tcam(struct cudbg_init *pdbg_init,
+			   struct cudbg_buffer *dbg_buff,
+			   struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_tid_data *tid_data = NULL;
+	u32 value, bytes = 0, bytes_left  = 0;
+	struct cudbg_tcam tcam_region = {0};
+	struct cudbg_buffer scratch_buff;
+	int rc, size;
+	u32 i;
+
+	/* Get the LE regions */
+	value = t4_read_reg(padap, LE_DB_TID_HASHBASE_A); /* Get hash base
+							     index */
+	tcam_region.tid_hash_base = value;
+
+	/* Get routing table index */
+	value = t4_read_reg(padap, LE_DB_ROUTING_TABLE_INDEX_A);
+	tcam_region.routing_start = value;
+
+	/*Get clip table index */
+	value = t4_read_reg(padap, LE_DB_CLIP_TABLE_INDEX_A);
+	tcam_region.clip_start = value;
+
+	/* Get filter table index */
+	value = t4_read_reg(padap, LE_DB_FILTER_TABLE_INDEX_A);
+	tcam_region.filter_start = value;
+
+	/* Get server table index */
+	value = t4_read_reg(padap, LE_DB_SERVER_INDEX_A);
+	tcam_region.server_start = value;
+
+	/* Check whether hash is enabled and calculate the max tids */
+	value = t4_read_reg(padap, LE_DB_CONFIG_A);
+	if ((value >> HASHEN_S) & 1) {
+		value = t4_read_reg(padap, LE_DB_HASH_CONFIG_A);
+		if (CHELSIO_CHIP_VERSION(padap->params.chip) > CHELSIO_T5)
+			tcam_region.max_tid = (value & 0xFFFFF) +
+					      tcam_region.tid_hash_base;
+		else {	    /* for T5 */
+			value = HASHTIDSIZE_G(value);
+			value = 1 << value;
+			tcam_region.max_tid = value +
+				tcam_region.tid_hash_base;
+		}
+	} else	 /* hash not enabled */
+		tcam_region.max_tid = CUDBG_MAX_TCAM_TID;
+
+	size = sizeof(struct cudbg_tid_data) * tcam_region.max_tid;
+	size += sizeof(struct cudbg_tcam);
+	scratch_buff.size = size;
+
+	rc = write_compression_hdr(&scratch_buff, dbg_buff);
+	if (rc)
+		goto err;
+
+	GET_SCRATCH_BUFF(dbg_buff, CUDBG_CHUNK_SIZE, &scratch_buff);
+	memcpy(scratch_buff.data, &tcam_region, sizeof(struct cudbg_tcam));
+	tid_data = (struct cudbg_tid_data *)(((struct cudbg_tcam *)
+					     scratch_buff.data) + 1);
+	bytes_left = CUDBG_CHUNK_SIZE - sizeof(struct cudbg_tcam);
+	bytes = sizeof(struct cudbg_tcam);
+	/* read all tid */
+	for (i = 0; i < tcam_region.max_tid; i++) {
+		if (bytes_left < sizeof(struct cudbg_tid_data)) {
+			scratch_buff.size = bytes;
+			rc = compress_buff(pdbg_init->hash_table,
+					   &scratch_buff, dbg_buff);
+			if (rc)
+				goto err1;
+			scratch_buff.size = CUDBG_CHUNK_SIZE;
+			release_scratch_buff(&scratch_buff, dbg_buff);
+
+			/* new alloc */
+			GET_SCRATCH_BUFF(dbg_buff, CUDBG_CHUNK_SIZE,
+					 &scratch_buff);
+			tid_data = (struct cudbg_tid_data *)(scratch_buff.data);
+			bytes_left = CUDBG_CHUNK_SIZE;
+			bytes = 0;
+		}
+
+		rc = cudbg_read_tid(pdbg_init, i, tid_data);
+		if (rc) {
+			cudbg_err->sys_err = rc;
+			goto err1;
+		}
+		tid_data++;
+		bytes_left -= sizeof(struct cudbg_tid_data);
+		bytes += sizeof(struct cudbg_tid_data);
+	}
+
+	if (bytes) {
+		scratch_buff.size = bytes;
+		rc = compress_buff(pdbg_init->hash_table, &scratch_buff,
+				   dbg_buff);
+	}
+
+err1:
+	scratch_buff.size = CUDBG_CHUNK_SIZE;
+	release_scratch_buff(&scratch_buff, dbg_buff);
+err:
+	return rc;
+}
+
+static int collect_ma_indirect(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *ma_indr = NULL;
+	int i, rc, n;
+	u32 size, j;
+
+	if (CHELSIO_CHIP_VERSION(padap->params.chip) < CHELSIO_T6) {
+		pdbg_init->print("MA indirect available only in T6\n");
+		return CUDBG_STATUS_ENTITY_NOT_FOUND;
+	}
+
+	n = sizeof(t6_ma_ireg_array) / (4 * sizeof(u32));
+	size = sizeof(struct ireg_buf) * n * 2;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	ma_indr = (struct ireg_buf *)scratch_buff.data;
+	for (i = 0; i < n; i++) {
+		struct ireg_field *ma_fli = &ma_indr->tp_pio;
+		u32 *buff = ma_indr->outbuf;
+
+		ma_fli->ireg_addr = t6_ma_ireg_array[i][0];
+		ma_fli->ireg_data = t6_ma_ireg_array[i][1];
+		ma_fli->ireg_local_offset = t6_ma_ireg_array[i][2];
+		ma_fli->ireg_offset_range = t6_ma_ireg_array[i][3];
+		t4_read_indirect(padap, ma_fli->ireg_addr, ma_fli->ireg_data,
+				 buff, ma_fli->ireg_offset_range,
+				 ma_fli->ireg_local_offset);
+		ma_indr++;
+	}
+
+	n = sizeof(t6_ma_ireg_array2) / (4 * sizeof(u32));
+	for (i = 0; i < n; i++) {
+		struct ireg_field *ma_fli = &ma_indr->tp_pio;
+		u32 *buff = ma_indr->outbuf;
+
+		ma_fli->ireg_addr = t6_ma_ireg_array2[i][0];
+		ma_fli->ireg_data = t6_ma_ireg_array2[i][1];
+		ma_fli->ireg_local_offset = t6_ma_ireg_array2[i][2];
+		for (j = 0; j < t6_ma_ireg_array2[i][3]; j++) {
+			t4_read_indirect(padap, ma_fli->ireg_addr,
+					 ma_fli->ireg_data, buff, 1,
+					 ma_fli->ireg_local_offset);
+			buff++;
+			ma_fli->ireg_local_offset += 0x20;
+		}
+		ma_indr++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_hma_indirect(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *hma_indr = NULL;
+	int i, rc, n;
+	u32 size;
+
+	if (CHELSIO_CHIP_VERSION(padap->params.chip) < CHELSIO_T6) {
+		pdbg_init->print("HMA indirect available only in T6\n");
+		return CUDBG_STATUS_ENTITY_NOT_FOUND;
+	}
+
+	n = sizeof(t6_hma_ireg_array) / (4 * sizeof(u32));
+	size = sizeof(struct ireg_buf) * n;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	hma_indr = (struct ireg_buf *)scratch_buff.data;
+	for (i = 0; i < n; i++) {
+		struct ireg_field *hma_fli = &hma_indr->tp_pio;
+		u32 *buff = hma_indr->outbuf;
+
+		hma_fli->ireg_addr = t6_hma_ireg_array[i][0];
+		hma_fli->ireg_data = t6_hma_ireg_array[i][1];
+		hma_fli->ireg_local_offset = t6_hma_ireg_array[i][2];
+		hma_fli->ireg_offset_range = t6_hma_ireg_array[i][3];
+		t4_read_indirect(padap, hma_fli->ireg_addr, hma_fli->ireg_data,
+				 buff, hma_fli->ireg_offset_range,
+				 hma_fli->ireg_local_offset);
+		hma_indr++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_pcie_indirect(struct cudbg_init *pdbg_init,
+				 struct cudbg_buffer *dbg_buff,
+				 struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *ch_pcie;
+	int i, rc, n;
+	u32 size;
+
+	n = sizeof(t5_pcie_pdbg_array) / (4 * sizeof(u32));
+	size = sizeof(struct ireg_buf) * n * 2;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	ch_pcie = (struct ireg_buf *)scratch_buff.data;
+	/*PCIE_PDBG*/
+	for (i = 0; i < n; i++) {
+		struct ireg_field *pcie_pio = &ch_pcie->tp_pio;
+		u32 *buff = ch_pcie->outbuf;
+
+		pcie_pio->ireg_addr = t5_pcie_pdbg_array[i][0];
+		pcie_pio->ireg_data = t5_pcie_pdbg_array[i][1];
+		pcie_pio->ireg_local_offset = t5_pcie_pdbg_array[i][2];
+		pcie_pio->ireg_offset_range = t5_pcie_pdbg_array[i][3];
+		t4_read_indirect(padap,
+				pcie_pio->ireg_addr,
+				pcie_pio->ireg_data,
+				buff,
+				pcie_pio->ireg_offset_range,
+				pcie_pio->ireg_local_offset);
+		ch_pcie++;
+	}
+
+	/*PCIE_CDBG*/
+	n = sizeof(t5_pcie_cdbg_array) / (4 * sizeof(u32));
+	for (i = 0; i < n; i++) {
+		struct ireg_field *pcie_pio = &ch_pcie->tp_pio;
+		u32 *buff = ch_pcie->outbuf;
+
+		pcie_pio->ireg_addr = t5_pcie_cdbg_array[i][0];
+		pcie_pio->ireg_data = t5_pcie_cdbg_array[i][1];
+		pcie_pio->ireg_local_offset = t5_pcie_cdbg_array[i][2];
+		pcie_pio->ireg_offset_range = t5_pcie_cdbg_array[i][3];
+		t4_read_indirect(padap,
+				pcie_pio->ireg_addr,
+				pcie_pio->ireg_data,
+				buff,
+				pcie_pio->ireg_offset_range,
+				pcie_pio->ireg_local_offset);
+		ch_pcie++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_tp_indirect(struct cudbg_init *pdbg_init,
+			       struct cudbg_buffer *dbg_buff,
+			       struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *ch_tp_pio;
+	int i, rc, n = 0;
+	u32 size;
+
+	if (is_t5(padap->params.chip))
+		n = sizeof(t5_tp_pio_array) / (4 * sizeof(u32));
+	else if (is_t6(padap->params.chip))
+		n = sizeof(t6_tp_pio_array) / (4 * sizeof(u32));
+
+	size = sizeof(struct ireg_buf) * n * 3;
+	GET_SCRATCH_BUFF(dbg_buff, size, &scratch_buff);
+	ch_tp_pio = (struct ireg_buf *)scratch_buff.data;
+	/* TP_PIO*/
+	for (i = 0; i < n; i++) {
+		struct ireg_field *tp_pio = &ch_tp_pio->tp_pio;
+		u32 *buff = ch_tp_pio->outbuf;
+
+		if (is_t5(padap->params.chip)) {
+			tp_pio->ireg_addr = t5_tp_pio_array[i][0];
+			tp_pio->ireg_data = t5_tp_pio_array[i][1];
+			tp_pio->ireg_local_offset = t5_tp_pio_array[i][2];
+			tp_pio->ireg_offset_range = t5_tp_pio_array[i][3];
+		} else if (is_t6(padap->params.chip)) {
+			tp_pio->ireg_addr = t6_tp_pio_array[i][0];
+			tp_pio->ireg_data = t6_tp_pio_array[i][1];
+			tp_pio->ireg_local_offset = t6_tp_pio_array[i][2];
+			tp_pio->ireg_offset_range = t6_tp_pio_array[i][3];
+		}
+		t4_tp_pio_read(padap, buff, tp_pio->ireg_offset_range,
+			       tp_pio->ireg_local_offset, true);
+		ch_tp_pio++;
+	}
+
+	/* TP_TM_PIO*/
+	if (is_t5(padap->params.chip))
+		n = sizeof(t5_tp_tm_pio_array) / (4 * sizeof(u32));
+	else if (is_t6(padap->params.chip))
+		n = sizeof(t6_tp_tm_pio_array) / (4 * sizeof(u32));
+
+	for (i = 0; i < n; i++) {
+		struct ireg_field *tp_pio = &ch_tp_pio->tp_pio;
+		u32 *buff = ch_tp_pio->outbuf;
+
+		if (is_t5(padap->params.chip)) {
+			tp_pio->ireg_addr = t5_tp_tm_pio_array[i][0];
+			tp_pio->ireg_data = t5_tp_tm_pio_array[i][1];
+			tp_pio->ireg_local_offset = t5_tp_tm_pio_array[i][2];
+			tp_pio->ireg_offset_range = t5_tp_tm_pio_array[i][3];
+		} else if (is_t6(padap->params.chip)) {
+			tp_pio->ireg_addr = t6_tp_tm_pio_array[i][0];
+			tp_pio->ireg_data = t6_tp_tm_pio_array[i][1];
+			tp_pio->ireg_local_offset = t6_tp_tm_pio_array[i][2];
+			tp_pio->ireg_offset_range = t6_tp_tm_pio_array[i][3];
+		}
+		t4_tp_tm_pio_read(padap, buff, tp_pio->ireg_offset_range,
+				  tp_pio->ireg_local_offset, true);
+		ch_tp_pio++;
+	}
+
+	/* TP_MIB_INDEX*/
+	if (is_t5(padap->params.chip))
+		n = sizeof(t5_tp_mib_index_array) / (4 * sizeof(u32));
+	else if (is_t6(padap->params.chip))
+		n = sizeof(t6_tp_mib_index_array) / (4 * sizeof(u32));
+
+	for (i = 0; i < n ; i++) {
+		struct ireg_field *tp_pio = &ch_tp_pio->tp_pio;
+		u32 *buff = ch_tp_pio->outbuf;
+
+		if (is_t5(padap->params.chip)) {
+			tp_pio->ireg_addr = t5_tp_mib_index_array[i][0];
+			tp_pio->ireg_data = t5_tp_mib_index_array[i][1];
+			tp_pio->ireg_local_offset =
+				t5_tp_mib_index_array[i][2];
+			tp_pio->ireg_offset_range =
+				t5_tp_mib_index_array[i][3];
+		} else if (is_t6(padap->params.chip)) {
+			tp_pio->ireg_addr = t6_tp_mib_index_array[i][0];
+			tp_pio->ireg_data = t6_tp_mib_index_array[i][1];
+			tp_pio->ireg_local_offset =
+				t6_tp_mib_index_array[i][2];
+			tp_pio->ireg_offset_range =
+				t6_tp_mib_index_array[i][3];
+		}
+		t4_tp_mib_read(padap, buff, tp_pio->ireg_offset_range,
+			       tp_pio->ireg_local_offset, true);
+		ch_tp_pio++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_sge_indirect(struct cudbg_init *pdbg_init,
+				struct cudbg_buffer *dbg_buff,
+				struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	struct ireg_buf *ch_sge_dbg;
+	int i, rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*ch_sge_dbg) * 2, &scratch_buff);
+	ch_sge_dbg = (struct ireg_buf *)scratch_buff.data;
+	for (i = 0; i < 2; i++) {
+		struct ireg_field *sge_pio = &ch_sge_dbg->tp_pio;
+		u32 *buff = ch_sge_dbg->outbuf;
+
+		sge_pio->ireg_addr = t5_sge_dbg_index_array[i][0];
+		sge_pio->ireg_data = t5_sge_dbg_index_array[i][1];
+		sge_pio->ireg_local_offset = t5_sge_dbg_index_array[i][2];
+		sge_pio->ireg_offset_range = t5_sge_dbg_index_array[i][3];
+		t4_read_indirect(padap,
+				sge_pio->ireg_addr,
+				sge_pio->ireg_data,
+				buff,
+				sge_pio->ireg_offset_range,
+				sge_pio->ireg_local_offset);
+		ch_sge_dbg++;
+	}
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_full(struct cudbg_init *pdbg_init,
+			struct cudbg_buffer *dbg_buff,
+			struct cudbg_error *cudbg_err)
+{
+	u32 reg_addr, reg_data, reg_local_offset, reg_offset_range;
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	int rc, nreg = 0;
+	u32 *sp;
+
+	/* Collect Registers:
+	 * TP_DBG_SCHED_TX (0x7e40 + 0x6a),
+	 * TP_DBG_SCHED_RX (0x7e40 + 0x6b),
+	 * TP_DBG_CSIDE_INT (0x7e40 + 0x23f),
+	 * TP_DBG_ESIDE_INT (0x7e40 + 0x148),
+	 * PCIE_CDEBUG_INDEX[AppData0] (0x5a10 + 2),
+	 * PCIE_CDEBUG_INDEX[AppData1] (0x5a10 + 3)  This is for T6
+	 * SGE_DEBUG_DATA_HIGH_INDEX_10 (0x12a8)
+	 **/
+
+	if (is_t5(padap->params.chip))
+		nreg = 6;
+	else if (is_t6(padap->params.chip))
+		nreg = 7;
+
+	scratch_buff.size = nreg * sizeof(u32);
+	GET_SCRATCH_BUFF(dbg_buff, scratch_buff.size, &scratch_buff);
+	sp = (u32 *)scratch_buff.data;
+
+	/* TP_DBG_SCHED_TX */
+	reg_local_offset = t5_tp_pio_array[3][2] + 0xa;
+	reg_offset_range = 1;
+	t4_tp_pio_read(padap, sp, reg_offset_range, reg_local_offset, true);
+	sp++;
+
+	/* TP_DBG_SCHED_RX */
+	reg_local_offset = t5_tp_pio_array[3][2] + 0xb;
+	reg_offset_range = 1;
+	t4_tp_pio_read(padap, sp, reg_offset_range, reg_local_offset, true);
+	sp++;
+
+	/* TP_DBG_CSIDE_INT */
+	reg_local_offset = t5_tp_pio_array[9][2] + 0xf;
+	reg_offset_range = 1;
+	t4_tp_pio_read(padap, sp, reg_offset_range, reg_local_offset, true);
+	sp++;
+
+	/* TP_DBG_ESIDE_INT */
+	reg_local_offset = t5_tp_pio_array[8][2] + 3;
+	reg_offset_range = 1;
+	t4_tp_pio_read(padap, sp, reg_offset_range, reg_local_offset, true);
+	sp++;
+
+	/* PCIE_CDEBUG_INDEX[AppData0] */
+	reg_addr = t5_pcie_cdbg_array[0][0];
+	reg_data = t5_pcie_cdbg_array[0][1];
+	reg_local_offset = t5_pcie_cdbg_array[0][2] + 2;
+	reg_offset_range = 1;
+	t4_read_indirect(padap, reg_addr, reg_data, sp, reg_offset_range,
+			 reg_local_offset);
+	sp++;
+
+	if (is_t6(padap->params.chip)) {
+		/* PCIE_CDEBUG_INDEX[AppData1] */
+		reg_addr = t5_pcie_cdbg_array[0][0];
+		reg_data = t5_pcie_cdbg_array[0][1];
+		reg_local_offset = t5_pcie_cdbg_array[0][2] + 3;
+		reg_offset_range = 1;
+		t4_read_indirect(padap, reg_addr, reg_data, sp,
+				 reg_offset_range, reg_local_offset);
+		sp++;
+	}
+
+	/* SGE_DEBUG_DATA_HIGH_INDEX_10 */
+	*sp = t4_read_reg(padap, SGE_DEBUG_DATA_HIGH_INDEX_10_A);
+	WRITE_AND_RELEASE_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_vpd_data(struct cudbg_init *pdbg_init,
+			    struct cudbg_buffer *dbg_buff,
+			    struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct struct_vpd_data *vpd_data;
+	struct cudbg_buffer scratch_buff;
+	char vpd_ver[4];
+	u32 fw_vers;
+	int rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(*vpd_data), &scratch_buff);
+	vpd_data = (struct struct_vpd_data *)scratch_buff.data;
+	if (is_t5(padap->params.chip)) {
+		read_vpd_reg(padap, SN_REG_ADDR, SN_MAX_LEN, vpd_data->sn);
+		read_vpd_reg(padap, BN_REG_ADDR, BN_MAX_LEN, vpd_data->bn);
+		read_vpd_reg(padap, NA_REG_ADDR, NA_MAX_LEN, vpd_data->na);
+		read_vpd_reg(padap, MN_REG_ADDR, MN_MAX_LEN, vpd_data->mn);
+	} else if (is_t6(padap->params.chip)) {
+		read_vpd_reg(padap, SN_T6_ADDR, SN_MAX_LEN, vpd_data->sn);
+		read_vpd_reg(padap, BN_T6_ADDR, BN_MAX_LEN, vpd_data->bn);
+		read_vpd_reg(padap, NA_T6_ADDR, NA_MAX_LEN, vpd_data->na);
+		read_vpd_reg(padap, MN_T6_ADDR, MN_MAX_LEN, vpd_data->mn);
+	}
+
+	if (is_fw_attached(pdbg_init)) {
+	   rc = t4_get_scfg_version(padap, &vpd_data->scfg_vers);
+	} else {
+		rc = 1;
+	}
+
+	if (rc) {
+		/* Now trying with backdoor mechanism */
+		rc = read_vpd_reg(padap, SCFG_VER_ADDR, SCFG_VER_LEN,
+				  (u8 *)&vpd_data->scfg_vers);
+		if (rc)
+			goto err1;
+	}
+
+	if (is_fw_attached(pdbg_init)) {
+		rc = t4_get_vpd_version(padap, &vpd_data->vpd_vers);
+	} else {
+		rc = 1;
+	}
+
+	if (rc) {
+		/* Now trying with backdoor mechanism */
+		rc = read_vpd_reg(padap, VPD_VER_ADDR, VPD_VER_LEN,
+				  (u8 *)vpd_ver);
+		if (rc)
+			goto err1;
+		/* read_vpd_reg return string of stored hex
+		 * converting hex string to char string
+		 * vpd version is 2 bytes only */
+		sprintf(vpd_ver, "%c%c\n", vpd_ver[0], vpd_ver[1]);
+		vpd_data->vpd_vers = simple_strtoul(vpd_ver, NULL, 16);
+	}
+
+	/* Get FW version if it's not already filled in */
+	fw_vers = padap->params.fw_vers;
+	if (!fw_vers) {
+		rc = t4_get_fw_version(padap, &fw_vers);
+		if (rc)
+			goto err1;
+	}
+
+	vpd_data->fw_major = FW_HDR_FW_VER_MAJOR_G(fw_vers);
+	vpd_data->fw_minor = FW_HDR_FW_VER_MINOR_G(fw_vers);
+	vpd_data->fw_micro = FW_HDR_FW_VER_MICRO_G(fw_vers);
+	vpd_data->fw_build = FW_HDR_FW_VER_BUILD_G(fw_vers);
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
+
+static int collect_upload(struct cudbg_init *pdbg_init,
+			  struct cudbg_buffer *dbg_buff,
+			  struct cudbg_error *cudbg_err)
+{
+	struct adapter *padap = pdbg_init->adap;
+	struct cudbg_buffer scratch_buff;
+	u32 param, *value;
+	int rc;
+
+	GET_SCRATCH_BUFF(dbg_buff, sizeof(u32), &scratch_buff);
+	value = (u32 *)scratch_buff.data;
+	param = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DEV) |
+		 FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DEV_LOAD));
+	rc = t4_query_params(padap, padap->mbox, padap->pf, 0, 1,
+			     &param, value);
+	if (rc < 0)
+		goto err1;
+	WRITE_AND_COMPRESS_SCRATCH_BUFF(&scratch_buff, dbg_buff);
+err1:
+	release_scratch_buff(&scratch_buff, dbg_buff);
+	return rc;
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.h b/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.h
new file mode 100644
index 00000000..7ccf65d8
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib.h
@@ -0,0 +1,307 @@
+#ifndef __CUDBG_LIB_H__
+#define __CUDBG_LIB_H__
+
+#ifndef min_t
+#define min_t(type, _a, _b)   (((type)(_a) < (type)(_b)) ? (type)(_a) : (type)(_b))
+#endif
+
+static int collect_reg_dump(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_fw_devlog(struct cudbg_init *, struct cudbg_buffer *,
+			     struct cudbg_error *);
+static int collect_cim_qcfg(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_cim_la(struct cudbg_init *, struct cudbg_buffer *,
+			  struct cudbg_error *);
+static int collect_cim_ma_la(struct cudbg_init *, struct cudbg_buffer *,
+			     struct cudbg_error *);
+static int collect_cim_obq_ulp0(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_obq_ulp1(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_obq_ulp2(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_obq_ulp3(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_obq_sge(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_cim_obq_ncsi(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_ibq_tp0(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_cim_ibq_tp1(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_cim_ibq_ulp(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_cim_ibq_sge0(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_ibq_sge1(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cim_ibq_ncsi(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_edc0_meminfo(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_edc1_meminfo(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_mc0_meminfo(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_mc1_meminfo(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_rss(struct cudbg_init *, struct cudbg_buffer *,
+		       struct cudbg_error *);
+static int collect_rss_key(struct cudbg_init *, struct cudbg_buffer *,
+			   struct cudbg_error *);
+static int collect_rss_pf_config(struct cudbg_init *, struct cudbg_buffer *,
+				 struct cudbg_error *);
+static int collect_rss_vf_config(struct cudbg_init *, struct cudbg_buffer *,
+				 struct cudbg_error *);
+static int collect_rss_config(struct cudbg_init *, struct cudbg_buffer *,
+			      struct cudbg_error *);
+static int collect_path_mtu(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_sw_state(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+int collect_wtp_data(struct cudbg_init *, struct cudbg_buffer *,
+		     struct cudbg_error *);
+static int collect_pm_stats(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_hw_sched(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_tcp_stats(struct cudbg_init *, struct cudbg_buffer *,
+			     struct cudbg_error *);
+static int collect_tp_err_stats(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_fcoe_stats(struct cudbg_init *, struct cudbg_buffer *,
+			      struct cudbg_error *);
+static int collect_rdma_stats(struct cudbg_init *, struct cudbg_buffer *,
+			      struct cudbg_error *);
+static int collect_tp_indirect(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_sge_indirect(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_cpl_stats(struct cudbg_init *, struct cudbg_buffer *,
+			     struct cudbg_error *);
+static int collect_ddp_stats(struct cudbg_init *, struct cudbg_buffer *,
+			     struct cudbg_error *);
+static int collect_wc_stats(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_ulprx_la(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_lb_stats(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_tp_la(struct cudbg_init *, struct cudbg_buffer *,
+			 struct cudbg_error *);
+static int collect_meminfo(struct cudbg_init *, struct cudbg_buffer *,
+			   struct cudbg_error *);
+static int collect_cim_pif_la(struct cudbg_init *, struct cudbg_buffer *,
+			      struct cudbg_error *);
+static int collect_clk_info(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_obq_sge_rx_q0(struct cudbg_init *, struct cudbg_buffer *,
+				 struct cudbg_error *);
+static int collect_obq_sge_rx_q1(struct cudbg_init *, struct cudbg_buffer *,
+				 struct cudbg_error *);
+static int collect_macstats(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_pcie_indirect(struct cudbg_init *, struct cudbg_buffer *,
+				 struct cudbg_error *);
+static int collect_pm_indirect(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_full(struct cudbg_init *, struct cudbg_buffer *,
+			struct cudbg_error *);
+static int collect_tx_rate(struct cudbg_init *, struct cudbg_buffer *,
+			   struct cudbg_error *);
+static int collect_tid(struct cudbg_init *, struct cudbg_buffer *,
+		       struct cudbg_error *);
+static int collect_pcie_config(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_dump_context(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_mps_tcam(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_vpd_data(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_le_tcam(struct cudbg_init *, struct cudbg_buffer *,
+			   struct cudbg_error *);
+static int collect_cctrl(struct cudbg_init *, struct cudbg_buffer *,
+			 struct cudbg_error *);
+static int collect_ma_indirect(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_ulptx_la(struct cudbg_init *, struct cudbg_buffer *,
+		struct cudbg_error *);
+static int collect_up_cim_indirect(struct cudbg_init *, struct cudbg_buffer *,
+				   struct cudbg_error *);
+static int collect_pbt_tables(struct cudbg_init *, struct cudbg_buffer *,
+			      struct cudbg_error *);
+static int collect_mbox_log(struct cudbg_init *, struct cudbg_buffer *,
+			    struct cudbg_error *);
+static int collect_hma_indirect(struct cudbg_init *, struct cudbg_buffer *,
+				struct cudbg_error *);
+static int collect_hma_meminfo(struct cudbg_init *, struct cudbg_buffer *,
+			       struct cudbg_error *);
+static int collect_upload(struct cudbg_init *, struct cudbg_buffer *,
+			  struct cudbg_error *);
+
+
+static int (*process_entity[])
+	(struct cudbg_init *, struct cudbg_buffer *, struct cudbg_error *) = {
+		collect_reg_dump,
+		collect_fw_devlog,
+		collect_cim_la,		/*3*/
+		collect_cim_ma_la,
+		collect_cim_qcfg,
+		collect_cim_ibq_tp0,
+		collect_cim_ibq_tp1,
+		collect_cim_ibq_ulp,
+		collect_cim_ibq_sge0,
+		collect_cim_ibq_sge1,
+		collect_cim_ibq_ncsi,
+		collect_cim_obq_ulp0,
+		collect_cim_obq_ulp1,	/*13*/
+		collect_cim_obq_ulp2,
+		collect_cim_obq_ulp3,
+		collect_cim_obq_sge,
+		collect_cim_obq_ncsi,
+		collect_edc0_meminfo,
+		collect_edc1_meminfo,
+		collect_mc0_meminfo,
+		collect_mc1_meminfo,
+		collect_rss,		/*22*/
+		collect_rss_pf_config,
+		collect_rss_key,
+		collect_rss_vf_config,
+		collect_rss_config,	/*26*/
+		collect_path_mtu,	/*27*/
+		collect_sw_state,
+		collect_wtp_data,
+		collect_pm_stats,
+		collect_hw_sched,
+		collect_tcp_stats,
+		collect_tp_err_stats,
+		collect_fcoe_stats,
+		collect_rdma_stats,
+		collect_tp_indirect,
+		collect_sge_indirect,
+		collect_cpl_stats,
+		collect_ddp_stats,
+		collect_wc_stats,
+		collect_ulprx_la,
+		collect_lb_stats,
+		collect_tp_la,
+		collect_meminfo,
+		collect_cim_pif_la,
+		collect_clk_info,
+		collect_obq_sge_rx_q0,
+		collect_obq_sge_rx_q1,
+		collect_macstats,
+		collect_pcie_indirect,
+		collect_pm_indirect,
+		collect_full,
+		collect_tx_rate,
+		collect_tid,
+		collect_pcie_config,
+		collect_dump_context,
+		collect_mps_tcam,
+		collect_vpd_data,
+		collect_le_tcam,
+		collect_cctrl,
+		collect_ma_indirect,
+		collect_ulptx_la,
+		NULL,			/* ext entity */
+		collect_up_cim_indirect,
+		collect_pbt_tables,
+		collect_mbox_log,
+		collect_hma_indirect,
+		collect_hma_meminfo,
+		collect_upload,
+	};
+
+static int ATTRIBUTE_UNUSED entity_priority_list[] = {
+	CUDBG_MBOX_LOG,
+	CUDBG_REG_DUMP,
+	CUDBG_DEV_LOG,
+	CUDBG_CIM_LA,
+	CUDBG_CIM_MA_LA,
+	CUDBG_CIM_QCFG,
+	CUDBG_CIM_IBQ_TP0,
+	CUDBG_CIM_IBQ_TP1,
+	CUDBG_CIM_IBQ_ULP,
+	CUDBG_CIM_IBQ_SGE0,
+	CUDBG_CIM_IBQ_SGE1,
+	CUDBG_CIM_IBQ_NCSI,
+	CUDBG_CIM_OBQ_ULP0,
+	CUDBG_CIM_OBQ_ULP1,
+	CUDBG_CIM_OBQ_ULP2,
+	CUDBG_CIM_OBQ_ULP3,
+	CUDBG_CIM_OBQ_SGE,
+	CUDBG_CIM_OBQ_NCSI,
+	CUDBG_EDC0,
+	CUDBG_EDC1,
+	CUDBG_MC0,
+	CUDBG_MC1,
+	CUDBG_RSS,
+	CUDBG_RSS_PF_CONF,
+	CUDBG_RSS_KEY,
+	CUDBG_RSS_VF_CONF,
+	CUDBG_RSS_CONF,
+	CUDBG_PATH_MTU,
+	CUDBG_SW_STATE,
+	CUDBG_WTP,
+	CUDBG_PM_STATS,
+	CUDBG_HW_SCHED,
+	CUDBG_TCP_STATS,
+	CUDBG_TP_ERR_STATS,
+	CUDBG_FCOE_STATS,
+	CUDBG_RDMA_STATS,
+	CUDBG_TP_INDIRECT,
+	CUDBG_SGE_INDIRECT,
+	CUDBG_CPL_STATS,
+	CUDBG_DDP_STATS,
+	CUDBG_WC_STATS,
+	CUDBG_ULPRX_LA,
+	CUDBG_LB_STATS,
+	CUDBG_TP_LA,
+	CUDBG_MEMINFO,
+	CUDBG_CIM_PIF_LA,
+	CUDBG_CLK,
+	CUDBG_CIM_OBQ_RXQ0,
+	CUDBG_CIM_OBQ_RXQ1,
+	CUDBG_MAC_STATS,
+	CUDBG_PCIE_INDIRECT,
+	CUDBG_PM_INDIRECT,
+	CUDBG_FULL,
+	CUDBG_TX_RATE,
+	CUDBG_TID_INFO,
+	CUDBG_PCIE_CONFIG,
+	CUDBG_DUMP_CONTEXT,
+	CUDBG_MPS_TCAM,
+	CUDBG_VPD_DATA,
+	CUDBG_LE_TCAM,
+	CUDBG_CCTRL,
+	CUDBG_MA_INDIRECT,
+	CUDBG_ULPTX_LA,
+	CUDBG_EXT_ENTITY,
+	CUDBG_UP_CIM_INDIRECT,
+	CUDBG_PBT_TABLE,
+	CUDBG_HMA_INDIRECT,
+	CUDBG_HMA,
+	CUDBG_UPLOAD,
+};
+
+struct large_entity {
+	int entity_code;
+	int skip_flag;
+	int priority; /* 1 is high priority */
+};
+
+static int read_cim_ibq(struct cudbg_init *, struct cudbg_buffer *,
+			struct cudbg_error * , int);
+static int read_cim_obq(struct cudbg_init *, struct cudbg_buffer *,
+			struct cudbg_error *, int);
+int get_entity_hdr(void *outbuf, int i, u32 size, struct cudbg_entity_hdr **);
+void skip_entity(struct large_entity *, int large_entity_list_size,
+		 int entity_code);
+void reset_skip_entity(struct large_entity *, int large_entity_list_size);
+int is_large_entity(struct large_entity *, int large_entity_list_size,
+		    int entity_code);
+#endif
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib_common.h b/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib_common.h
new file mode 100644
index 00000000..9bd616ae
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_lib_common.h
@@ -0,0 +1,133 @@
+#ifndef __CUDBG_LIB_COMMON_H__
+#define __CUDBG_LIB_COMMON_H__
+
+/* Extended entity
+ *
+ * Layout of the cudbg dump file when extended entity is present.
+ *
+ *
+ *           ----------------
+ *           | Global header |
+ *           |---------------|
+ *           |entity headers |
+ *           |---------------|
+ *           | Entity data   |
+ *           |      *        |
+ *           |      *        |
+ *           |      *        |
+ *           |---------------|
+ *           |extended entity|
+ *           |    header     |
+ *           |---------------|
+ *           |extended entity|
+ *           |     data      |
+ *           -----------------
+ *
+ *
+ * Extended entity: This comes into picture only when cudbg_collect() is called
+ * multiple times.
+ */
+
+#include "t4_hw.h"
+
+#define CUDBG_SF_MAX_SECTOR         (FLASH_CUDBG_START_SEC + FLASH_CUDBG_NSECS)
+#define CUDBG_SF_SECTOR_SIZE        SF_SEC_SIZE
+#define CUDBG_START_SEC             FLASH_CUDBG_START_SEC
+#define CUDBG_FLASH_SIZE            FLASH_CUDBG_MAX_SIZE
+
+#define CUDBG_EXT_DATA_BIT          0
+#define CUDBG_EXT_DATA_VALID        (1 << CUDBG_EXT_DATA_BIT)
+
+struct cudbg_hdr {
+	u32 signature;
+	u32 hdr_len;
+	u16 major_ver;
+	u16 minor_ver;
+	u32 data_len;
+	u32 hdr_flags;
+	u16 max_entities;
+	u8 chip_ver;
+	u8 reserved1;
+	u32 reserved[8];
+};
+
+struct cudbg_entity_hdr {
+	u32 entity_type;
+	u32 start_offset;
+	u32 size;
+	int hdr_flags;
+	u32 sys_warn;
+	u32 sys_err;
+	u8 num_pad;
+	u8 flag;		/* bit 0 is used to indicate ext data */
+	u8 reserved1[2];
+	u32 next_ext_offset;	/* pointer to next extended entity meta data */
+	u32 reserved[5];
+};
+
+struct cudbg_buffer {
+	u32 size;
+	u32 offset;
+	char *data;
+};
+
+struct cudbg_error {
+	int sys_err;
+	int sys_warn;
+	int app_err;
+};
+
+struct cudbg_private {
+	struct cudbg_init  dbg_init;
+	struct cudbg_flash_sec_info *psec_info;
+};
+
+struct cudbg_flash_sec_info {
+	int par_sec;		   /* Represent partially filled sector no */
+	int par_sec_offset;	   /* Offset in partially filled sector */
+	int cur_seq_no;
+	u32 max_seq_no;
+	u32 max_seq_sec;
+	u32 hdr_data_len;	   /* Total data */
+	u32 skip_size;		   /* Total size of large entities. */
+	u64 max_timestamp;
+	char sec_data[CUDBG_SF_SECTOR_SIZE];
+	u8 sec_bitmap[8];
+};
+
+#define HTONL_NIBBLE(data) ( \
+			    (((uint32_t)(data) >> 28) & 0x0000000F) | \
+			    (((uint32_t)(data) >> 20)  & 0x000000F0) | \
+			    (((uint32_t)(data) >> 12)  & 0x00000F00) | \
+			    (((uint32_t)(data) >> 4) & 0x0000F000) | \
+			    (((uint32_t)(data) << 4) & 0x000F0000) | \
+			    (((uint32_t)(data) << 12)  & 0x00F00000) | \
+			    (((uint32_t)(data) << 20)  & 0x0F000000) | \
+			    (((uint32_t)(data) << 28) & 0xF0000000))
+
+#define CDUMP_MAX_COMP_BUF_SIZE    ((64 * 1024) - 1)
+#define CUDBG_CHUNK_SIZE  ((CDUMP_MAX_COMP_BUF_SIZE/1024) * 1024)
+
+#define CUDBG_LEGACY_SIGNATURE 123
+#define CUDBG_SIGNATURE 67856866 /* CUDB in ascii */
+#define CUDBG_FL_SIGNATURE 0x4355464c /* CUFL in ascii */
+
+#define CUDBG_FL_MAJOR_VERSION	    1
+#define CUDBG_FL_MINOR_VERSION	    1
+#define CUDBG_FL_BUILD_VERSION	    0
+
+u32 get_skip_size(struct cudbg_flash_sec_info *psec_info);
+void update_skip_size(struct cudbg_flash_sec_info *psec_info, u32 size);
+int write_compression_hdr(struct cudbg_buffer *, struct cudbg_buffer *);
+int compress_buff(unsigned char *hash_table, struct cudbg_buffer *,
+		  struct cudbg_buffer *);
+int get_scratch_buff(struct cudbg_buffer *, u32, struct cudbg_buffer *);
+void release_scratch_buff(struct cudbg_buffer *, struct cudbg_buffer *);
+void sort_t(void *base, int num, int size,
+	    int (*cmp_func)(const void *, const void *),
+	    void (*swap_func)(void *, void *, int size));
+int cudbg_write_flash(void *handle, u64 timestamp, void *data,
+		      u32 start_offset, u32 start_hdr_offset,
+		      u32 cur_entity_size,
+		      u32 ext_size);
+#endif
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_utls.c b/drivers/net/ethernet/chelsio/cxgb4/cudbg_utls.c
new file mode 100644
index 00000000..27339fca
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_utls.c
@@ -0,0 +1,226 @@
+#include "cudbg_lib_common.h"
+static int mem_desc_cmp(const void *a, const void *b);
+static void u32_swap(void *a, void *b, int size);
+static void generic_swap(void *a1, void *b1, int size);
+
+static int mem_desc_cmp(const void *a, const void *b)
+{
+	return ((const struct struct_mem_desc *)a)->base -
+		((const struct struct_mem_desc *)b)->base;
+}
+
+static void u32_swap(void *a, void *b, int size)
+{
+	u32 t = *(u32 *)a;
+
+	*(u32 *)a = *(u32 *)b;
+	*(u32 *)b = t;
+}
+
+static void generic_swap(void *a1, void *b1, int size)
+{
+	u8 t;
+	u8 *a = (u8 *)a1;
+	u8 *b = (u8 *)b1;
+
+	do {
+		t = *a;
+		*(a++) = *b;
+		*(b++) = t;
+	} while (--size > 0);
+}
+
+/**
+ * sort - sort an array of elements
+ * @base: pointer to data to sort
+ * @num: number of elements
+ * @size: size of each element
+ * @cmp_func: pointer to comparison function
+ * @swap_func: pointer to swap function or NULL
+ *
+ * This function does a heapsort on the given array. You may provide a
+ * swap_func function optimized to your element type.
+ *
+ * Sorting time is O(n log n) both on average and worst-case. While
+ * qsort is about 20% faster on average, it suffers from exploitable
+ * O(n*n) worst-case behavior and extra memory requirements that make
+ * it less suitable for kernel use.
+ */
+void sort_t(void *base_val, int num, int size,
+	    int (*cmp_func)(const void *, const void *),
+	    void (*swap_func)(void *, void *, int size))
+{
+	/* pre-scale counters for performance */
+	int i = (num / 2 - 1) * size;
+	int n = num * size;
+	int c, r;
+	u8 *base = (u8 *)base_val;
+
+	if (!swap_func)
+		swap_func = (size == 4 ? u32_swap : generic_swap);
+
+	/* heapify */
+	for (; i >= 0; i -= size) {
+		for (r = i; r * 2 + size < n; r  = c) {
+			c = r * 2 + size;
+			if (c < n - size &&
+			    cmp_func(base + c, base + c + size) < 0)
+				c += size;
+			if (cmp_func(base + r, base + c) >= 0)
+				break;
+			swap_func(base + r, base + c, size);
+		}
+	}
+
+	/* sort */
+	for (i = n - size; i > 0; i -= size) {
+		swap_func(base, base + i, size);
+		for (r = 0; r * 2 + size < i; r = c) {
+			c = r * 2 + size;
+			if (c < i - size &&
+			    cmp_func(base + c, base + c + size) < 0)
+				c += size;
+			if (cmp_func(base + r, base + c) >= 0)
+				break;
+			swap_func(base + r, base + c, size);
+		}
+	}
+}
+
+static inline uint8_t count_set_bits(uint32_t word32)
+{
+	uint8_t count = 0;
+
+	while (word32) {
+		count += word32 & 1;
+		word32 >>= 1;
+	}
+	return count;
+}
+
+/* Code to read VPD data */
+#define PCIE_BAR0_LENGTH	0x513FF
+#define S_ENABLE		30
+#define V_ENABLE(x)		((x) << S_ENABLE)
+#define F_ENABLE		V_ENABLE(1U)
+#define S_WRITE_BYTES		24
+#define V_WRITE_BYTES(x)	((x) << S_WRITE_BYTES)
+#define F_WRITE_BYTES		V_WRITE_BYTES(0xf)
+#define VPD_CAP			0xD0
+#define VPD_DATA		0xD4
+#define SEEPROM_SIZE		0x8000
+
+static unsigned int remap_se_addr(unsigned int addr)
+{
+	if (addr >= 0x400)
+		return addr - 0x400;
+	return 0x7C00 + addr;
+}
+
+static unsigned int read_pci(struct adapter *padap, unsigned addr)
+{
+	u32 req_mask = 0;
+
+	if (is_t5(padap->params.chip))
+		req_mask = F_ENABLE | F_WRITE_BYTES;
+	else if (is_t6(padap->params.chip))
+		req_mask = T6_ENABLE_F | T6_WRBE_V(0xFU);
+
+	t4_write_reg(padap, PCIE_CFG_SPACE_REQ_A, addr | req_mask);
+	return t4_read_reg(padap, PCIE_CFG_SPACE_DATA_A);
+}
+
+static unsigned wait_vpd_cap_not_busy(struct adapter *padap,
+				      unsigned busy_polarity, int timeout)
+{
+	unsigned busy = 1;
+	unsigned cnt = 0;
+
+	while (busy) {
+		unsigned raw_busy = read_pci(padap, VPD_CAP);
+
+		cnt++;
+		busy = (busy_polarity == ((raw_busy >> 31) & 1));
+	}
+	return busy;
+}
+
+static void write_pci(struct adapter *padap, unsigned addr, unsigned data)
+{
+	u32 req_mask = 0;
+
+	if (is_t5(padap->params.chip))
+		req_mask = F_ENABLE | F_WRITE_BYTES;
+	else if (is_t6(padap->params.chip))
+		req_mask = T6_ENABLE_F | T6_WRBE_V(0xFU);
+
+	t4_write_reg(padap, PCIE_CFG_SPACE_REQ_A, addr | req_mask);
+	t4_write_reg(padap, PCIE_CFG_SPACE_DATA_A, data);
+}
+
+static unsigned int se_read(struct adapter *padap, unsigned int addr)
+{
+	write_pci(padap, VPD_CAP, remap_se_addr(addr) << 16);
+	wait_vpd_cap_not_busy(padap, 0, 0);
+	return read_pci(padap, VPD_DATA);
+}
+
+static int read_vpd_reg(struct adapter *padap, int addr, int length,
+			u8 *read_out)
+{
+	int i;
+	int j = 0;
+	int data;
+
+	/* buffer to store data we read from VPD.*/
+	/* With extra buffer since se_read reads dword aligned.*/
+	u8 vpd_data[MAX_VPD_DATA_LEN];
+	int addr_diff;
+	int base_addr = addr & ~3;
+
+	/* Read next dword b/c if we read addr 0x2 for 4 bytes, we need to
+	 * read both addr 0x0 and addr 0x4.
+	 */
+	int max_addr;
+
+	/* checking (addr + length) is multiple of 4 or not */
+	if (((addr + length) & 3) == 0)
+		max_addr = addr + length;
+	else
+		max_addr = ((addr + length) & ~3) + 4;
+
+	/* Read from VPD and put each character into the buffer.
+	 * se_read() returns value that is byte swapped so we mask MSB starting
+	 * from right.
+	 */
+	for (i = base_addr; i < max_addr; i += 4) {
+		data = se_read(padap, i);
+		vpd_data[j++] = (data & 0x000000ff);
+		vpd_data[j++] = (data & 0x0000ff00) >> 8;
+		vpd_data[j++] = (data & 0x00ff0000) >> 16;
+		vpd_data[j++] = (data & 0xff000000) >> 24;
+	}
+	vpd_data[j] = '\0';  /* end string*/
+
+	/* Need to figure out where in our buffer to start printing our string
+	 * because se_read() reads from dword aligned addresses. For example,
+	 * reading 0xae2 will return the whole dword from 0xae0-0xae3.
+	 */
+	addr_diff = addr - base_addr;
+	for (i = addr_diff, j = 0; i < (addr_diff + length); i++)
+		read_out[j++] = vpd_data[i];
+
+	read_out[j] = '\0';
+
+	return 0;
+}
+
+static inline unsigned int cudbg_round_up(unsigned int x, unsigned int y)
+{
+	return ((x + y - 1) / y) * y;
+}
+
+static inline unsigned int cudbg_round_down(unsigned int x, unsigned int y)
+{
+	return (x / y) * y;
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cudbg_wtp.c b/drivers/net/ethernet/chelsio/cxgb4/cudbg_wtp.c
new file mode 100644
index 00000000..c078c5c3
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cudbg_wtp.c
@@ -0,0 +1,12 @@
+#include "cxgb4.h"
+
+#include "cudbg_if.h"
+#include "cudbg_lib_common.h"
+#include "cudbg_entity.h"
+
+int collect_wtp_data(struct cudbg_init *pdbg_init,
+		     struct cudbg_buffer *dbg_buff,
+		     struct cudbg_error *cudbg_err)
+{
+	return CUDBG_STATUS_NOT_IMPLEMENTED;
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index 2481bd58..f37c6ec5 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@ -1193,6 +1193,7 @@ int cxgb4_set_rspq_intr_params(struct sge_rspq *q, unsigned int us,
 			       unsigned int cnt);
 void cxgb4_set_ethtool_ops(struct net_device *netdev);
 int cxgb4_write_rss(const struct port_info *pi, const u16 *queues);
+void do_collect(struct adapter *adap, void *buf, unsigned long size);
 extern int dbfifo_int_thresh;
 
 #define for_each_port(adapter, iter) \
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_cudbg.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_cudbg.c
new file mode 100644
index 00000000..34079981
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_cudbg.c
@@ -0,0 +1,82 @@
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/time.h>
+#include "t4_hw.h"
+#include "t4_regs.h"
+#include "t4_msg.h"
+#include "cudbg_if.h"
+#include "cxgb4.h"
+
+static struct cudbg_init cudbg = {{0}};
+
+void do_collect(struct adapter *adap, void *buf, unsigned long size)
+{
+	struct devlog_params *dparams = NULL;
+	struct cudbg_param *dbg_param;
+	struct timespec ts;
+	void *handle = NULL;
+	int ret;
+
+	init_cudbg_hdr(&cudbg.header);
+	set_dbg_bitmap(cudbg.dbg_bitmap, CUDBG_ALL);
+	cudbg.adap = adap;
+	cudbg.print = (cudbg_print_cb) printk;
+	cudbg.sw_state_buf = NULL;
+	cudbg.sw_state_buflen = 0;
+	cudbg.use_flash = 1;
+
+	dparams = &adap->params.devlog;
+
+	cudbg.dbg_params[CUDBG_DEVLOG_PARAM].param_type = CUDBG_DEVLOG_PARAM;
+	cudbg.dbg_params[CUDBG_DEVLOG_PARAM].u.devlog_param.memtype =
+			dparams->memtype;
+	cudbg.dbg_params[CUDBG_DEVLOG_PARAM].u.devlog_param.start =
+			dparams->start;
+	cudbg.dbg_params[CUDBG_DEVLOG_PARAM].u.devlog_param.size =
+			dparams->size;
+	cudbg.dbg_params_cnt++;
+
+	getnstimeofday(&ts);
+	cudbg.dbg_params[CUDBG_TIMESTAMP_PARAM].u.time = ts.tv_sec;
+	cudbg.dbg_params[CUDBG_TIMESTAMP_PARAM].param_type =
+			CUDBG_TIMESTAMP_PARAM;
+
+#ifdef T4_OS_LOG_MBOX_CMDS
+	cudbg.dbg_params[CUDBG_MBOX_LOG_PARAM].param_type =
+			CUDBG_MBOX_LOG_PARAM;
+	cudbg.dbg_params[CUDBG_MBOX_LOG_PARAM].u.mboxlog_param.log =
+			adap->mbox_log;
+	cudbg.dbg_params[CUDBG_MBOX_LOG_PARAM].u.mboxlog_param.mbox_cmds =
+			T4_OS_LOG_MBOX_CMDS;
+	cudbg.dbg_params_cnt++;
+#endif
+
+	dbg_param = &(cudbg.dbg_params[CUDBG_SW_STATE_PARAM]);
+	dbg_param->param_type = CUDBG_SW_STATE_PARAM;
+	dbg_param->u.sw_state_param.os_type = CUDBG_OS_TYPE_LINUX;
+	strcpy((char *)dbg_param->u.sw_state_param.caller_string,
+	       "KERNEL PANIC");
+
+	ret = cudbg_hello(&cudbg, &handle);
+	if (ret) {
+		dev_err(adap->pdev_dev,
+			"cudbg failed to initialize, hello cmd failed, ret=%d",
+			 ret);
+		goto out;
+	}
+
+	ret = cudbg_collect(handle, buf, (u32 *)&size);
+	if (ret) {
+		dev_err(adap->pdev_dev,
+			"cudbg collect failed, ret=%d", ret);
+		goto out;
+	} else {
+		dev_info(adap->pdev_dev,
+			"cudbg collect success, size=%lu", size);
+		dev_info(adap->pdev_dev, "Dumping debug data to flash.");
+	}
+
+out:
+	if (handle)
+		cudbg_bye(handle);
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/fastlz.c b/drivers/net/ethernet/chelsio/cxgb4/fastlz.c
new file mode 100644
index 00000000..44acdd13
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/fastlz.c
@@ -0,0 +1,327 @@
+/*
+   FastLZ - lightning-fast lossless compression library
+
+   Copyright (C) 2007 Ariya Hidayat (ariya@kde.org)
+   Copyright (C) 2006 Ariya Hidayat (ariya@kde.org)
+   Copyright (C) 2005 Ariya Hidayat (ariya@kde.org)
+
+   Permission is hereby granted, free of charge, to any person obtaining a copy
+   of this software and associated documentation files (the "Software"), to deal
+   in the Software without restriction, including without limitation the rights
+   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+   copies of the Software, and to permit persons to whom the Software is
+   furnished to do so, subject to the following conditions:
+
+   The above copyright notice and this permission notice shall be included in
+   all copies or substantial portions of the Software.
+
+   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+   THE SOFTWARE.
+   */
+
+#include "fastlz_common.h"
+
+#if !defined(FASTLZ_COMPRESSOR)
+
+#undef FASTLZ_LEVEL
+#define FASTLZ_LEVEL 1
+
+#undef FASTLZ_COMPRESSOR
+#define FASTLZ_COMPRESSOR fastlz1_compress
+static FASTLZ_INLINE int FASTLZ_COMPRESSOR(unsigned char *hash_table,
+					   const void *input, int length,
+					   void *output);
+#include "fastlz.c"
+
+#undef FASTLZ_LEVEL
+#define FASTLZ_LEVEL 2
+
+#undef MAX_DISTANCE
+#define MAX_DISTANCE 8191
+#define MAX_FARDISTANCE (65535 + MAX_DISTANCE - 1)
+
+#undef FASTLZ_COMPRESSOR
+#define FASTLZ_COMPRESSOR fastlz2_compress
+static FASTLZ_INLINE int FASTLZ_COMPRESSOR(unsigned char *hash_table,
+					   const void *input, int length,
+					   void *output);
+#include "fastlz.c"
+
+int fastlz_compress(unsigned char *hash_table, const void *input, int length,
+		    void *output)
+{
+	/* for short block, choose fastlz1 */
+	if (length < 65536)
+		return fastlz1_compress(hash_table, input, length, output);
+
+	/* else... */
+	return fastlz2_compress(hash_table, input, length, output);
+}
+
+int fastlz_compress_level(unsigned char *hash_table, int level,
+			  const void *input, int length,
+			  void *output)
+{
+	if (level == 1)
+		return fastlz1_compress(hash_table, input, length, output);
+	if (level == 2)
+		return fastlz2_compress(hash_table, input, length, output);
+
+	return 0;
+}
+
+#else /* !defined(FASTLZ_COMPRESSOR) */
+
+static FASTLZ_INLINE int FASTLZ_COMPRESSOR(unsigned char *hash_table,
+					   const void *input, int length,
+					   void *output)
+{
+	const unsigned char *ip = (const unsigned char *) input;
+	const unsigned char *ip_bound = ip + length - 2;
+	const unsigned char *ip_limit = ip + length - 12;
+	unsigned char *op = (unsigned char *) output;
+
+	const unsigned char **htab = (const unsigned char **)hash_table;
+	const unsigned char **hslot;
+	unsigned int hval;
+
+	unsigned int copy;
+
+	/* sanity check */
+	if (FASTLZ_UNEXPECT_CONDITIONAL(length < 4)) {
+		if (length) {
+			/* create literal copy only */
+			*op++ = length - 1;
+			ip_bound++;
+			while (ip <= ip_bound)
+				*op++ = *ip++;
+			return length + 1;
+		} else
+			return 0;
+	}
+
+	/* initializes hash table */
+	for (hslot = htab; hslot < htab + FASTLZ_HASH_SIZE; hslot++)
+		*hslot = ip;
+
+	/* we start with literal copy */
+	copy = 2;
+	*op++ = MAX_COPY - 1;
+	*op++ = *ip++;
+	*op++ = *ip++;
+
+	/* main loop */
+	while (FASTLZ_EXPECT_CONDITIONAL(ip < ip_limit)) {
+		const unsigned char *ref;
+		unsigned int distance;
+
+		/* minimum match length */
+		unsigned int len = 3;
+
+		/* comparison starting-point */
+		const unsigned char *anchor = ip;
+
+		/* check for a run */
+#if FASTLZ_LEVEL == 2
+		if (ip[0] == ip[-1] &&
+		    FASTLZ_READU16(ip - 1) == FASTLZ_READU16(ip + 1)) {
+			distance = 1;
+			ip += 3;
+			ref = anchor - 1 + 3;
+			goto match;
+		}
+#endif
+
+		/* find potential match */
+		HASH_FUNCTION(hval, ip);
+		hslot = htab + hval;
+		ref = htab[hval];
+
+		/* calculate distance to the match */
+		distance = anchor - ref;
+
+		/* update hash table */
+		*hslot = anchor;
+
+		if (!ref)
+			goto literal;
+		/* is this a match? check the first 3 bytes */
+		if (distance == 0 ||
+#if FASTLZ_LEVEL == 1
+				(distance >= MAX_DISTANCE) ||
+#else
+				(distance >= MAX_FARDISTANCE) ||
+#endif
+				*ref++ != *ip++ || *ref++ != *ip++ ||
+				*ref++ != *ip++)
+			goto literal;
+
+#if FASTLZ_LEVEL == 2
+		/* far, needs at least 5-byte match */
+		if (distance >= MAX_DISTANCE) {
+			if (*ip++ != *ref++ || *ip++ != *ref++)
+				goto literal;
+			len += 2;
+		}
+
+match:
+#endif
+
+		/* last matched byte */
+		ip = anchor + len;
+
+		/* distance is biased */
+		distance--;
+
+		if (!distance) {
+			/* zero distance means a run */
+			unsigned char x = ip[-1];
+			while (ip < ip_bound)
+				if (*ref++ != x)
+					break;
+				else
+					ip++;
+		} else
+			for (;;) {
+				/* safe because the outer check
+				 * against ip limit */
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				if (*ref++ != *ip++)
+					break;
+				while (ip < ip_bound)
+					if (*ref++ != *ip++)
+						break;
+				break;
+			}
+
+		/* if we have copied something, adjust the copy count */
+		if (copy)
+			/* copy is biased, '0' means 1 byte copy */
+			*(op - copy - 1) = copy - 1;
+		else
+			/* back, to overwrite the copy count */
+			op--;
+
+		/* reset literal counter */
+		copy = 0;
+
+		/* length is biased, '1' means a match of 3 bytes */
+		ip -= 3;
+		len = ip - anchor;
+
+		/* encode the match */
+#if FASTLZ_LEVEL == 2
+		if (distance < MAX_DISTANCE) {
+			if (len < 7) {
+				*op++ = (len << 5) + (distance >> 8);
+				*op++ = (distance & 255);
+			} else {
+				*op++ = (7 << 5) + (distance >> 8);
+				for (len -= 7; len >= 255; len -= 255)
+					*op++ = 255;
+				*op++ = len;
+				*op++ = (distance & 255);
+			}
+		} else {
+			/* far away, but not yet in the another galaxy... */
+			if (len < 7) {
+				distance -= MAX_DISTANCE;
+				*op++ = (len << 5) + 31;
+				*op++ = 255;
+				*op++ = distance >> 8;
+				*op++ = distance & 255;
+			} else {
+				distance -= MAX_DISTANCE;
+				*op++ = (7 << 5) + 31;
+				for (len -= 7; len >= 255; len -= 255)
+					*op++ = 255;
+				*op++ = len;
+				*op++ = 255;
+				*op++ = distance >> 8;
+				*op++ = distance & 255;
+			}
+		}
+#else
+
+		if (FASTLZ_UNEXPECT_CONDITIONAL(len > MAX_LEN - 2))
+			while (len > MAX_LEN - 2) {
+				*op++ = (7 << 5) + (distance >> 8);
+				*op++ = MAX_LEN - 2 - 7 - 2;
+				*op++ = (distance & 255);
+				len -= MAX_LEN - 2;
+			}
+
+		if (len < 7) {
+			*op++ = (len << 5) + (distance >> 8);
+			*op++ = (distance & 255);
+		} else {
+			*op++ = (7 << 5) + (distance >> 8);
+			*op++ = len - 7;
+			*op++ = (distance & 255);
+		}
+#endif
+
+		/* update the hash at match boundary */
+		HASH_FUNCTION(hval, ip);
+		htab[hval] = ip++;
+		HASH_FUNCTION(hval, ip);
+		htab[hval] = ip++;
+
+		/* assuming literal copy */
+		*op++ = MAX_COPY - 1;
+
+		continue;
+
+literal:
+		*op++ = *anchor++;
+		ip = anchor;
+		copy++;
+		if (FASTLZ_UNEXPECT_CONDITIONAL(copy == MAX_COPY)) {
+			copy = 0;
+			*op++ = MAX_COPY - 1;
+		}
+	}
+
+	/* left-over as literal copy */
+	ip_bound++;
+	while (ip <= ip_bound) {
+		*op++ = *ip++;
+		copy++;
+		if (copy == MAX_COPY) {
+			copy = 0;
+			*op++ = MAX_COPY - 1;
+		}
+	}
+
+	/* if we have copied something, adjust the copy length */
+	if (copy)
+		*(op - copy - 1) = copy - 1;
+	else
+		op--;
+
+#if FASTLZ_LEVEL == 2
+	/* marker for fastlz2 */
+	*(unsigned char *)output |= (1 << 5);
+#endif
+
+	return op - (unsigned char *)output;
+}
+#endif /* !defined(FASTLZ_COMPRESSOR) */
diff --git a/drivers/net/ethernet/chelsio/cxgb4/fastlz.h b/drivers/net/ethernet/chelsio/cxgb4/fastlz.h
new file mode 100644
index 00000000..ef8761fc
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/fastlz.h
@@ -0,0 +1,78 @@
+#include <linux/types.h>
+#include "cudbg_if.h"
+#include "cudbg_lib_common.h"
+#ifndef FASTLZ_H
+#define FASTLZ_H
+
+#define FASTLZ_VERSION 0x000100
+
+#define FASTLZ_VERSION_MAJOR	 0
+#define FASTLZ_VERSION_MINOR	 0
+#define FASTLZ_VERSION_REVISION  0
+
+#define FASTLZ_VERSION_STRING "0.1.0"
+
+extern struct cudbg_private g_context;
+
+#if defined __cplusplus
+extern "C" {
+#endif
+
+	/**
+	  Compress a block of data in the input buffer and returns the size of
+	  compressed block. The size of input buffer is specified by length. The
+	  minimum input buffer size is 16.
+
+	  The output buffer must be at least 5% larger than the input buffer
+	  and can not be smaller than 66 bytes.
+
+	  If the input is not compressible, the return value might be larger
+	  than length (input buffer size).
+
+	  The input buffer and the output buffer can not overlap.
+	  */
+
+	int fastlz_compress(unsigned char *hash_table, const void *input,
+			    int length, void *output);
+	/**
+	  Compress a block of data in the input buffer and returns the size of
+	  compressed block. The size of input buffer is specified by length. The
+	  minimum input buffer size is 16.
+
+	  The output buffer must be at least 5% larger than the input buffer
+	  and can not be smaller than 66 bytes.
+
+	  If the input is not compressible, the return value might be larger
+	  than length (input buffer size).
+
+	  The input buffer and the output buffer can not overlap.
+
+	  Compression level can be specified in parameter level. At the moment,
+	  only level 1 and level 2 are supported.
+	  Level 1 is the fastest compression and generally useful for short
+	  data.
+	  Level 2 is slightly slower but it gives better compression ratio.
+
+	  Note that the compressed data, regardless of the level, can always be
+	  decompressed using the function fastlz_decompress above.
+	  */
+
+	int fastlz_compress_level(unsigned char *hash_table, int level,
+				  const void *input, int length,
+				  void *output);
+#if defined __cplusplus
+}
+#endif
+
+#endif /* FASTLZ_H */
+
+int fastLz_compress(unsigned char *hash_table, const void *input,
+		    int length, void *output);
+int fastlz_compress_level(unsigned char *hash_table, int level,
+			  const void *input, int length,
+			  void *output);
+/* prototypes */
+int write_magic(struct cudbg_buffer *);
+int write_to_buf(void *, u32, u32 *, void *, u32);
+int write_chunk_header(struct cudbg_buffer *, int, int, unsigned long,
+		       unsigned long, unsigned long);
diff --git a/drivers/net/ethernet/chelsio/cxgb4/fastlz_api.c b/drivers/net/ethernet/chelsio/cxgb4/fastlz_api.c
new file mode 100644
index 00000000..3c11f4e1
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/fastlz_api.c
@@ -0,0 +1,204 @@
+#include "fastlz.h"
+#include "fastlz_common.h"
+#include "cxgb4.h"
+
+unsigned char sixpack_magic[8] = {137, '6', 'P', 'K', 13, 10, 26, 10};
+
+int write_magic(struct cudbg_buffer *_out_buff)
+{
+	int rc;
+
+	rc = write_to_buf(_out_buff->data, _out_buff->size, &_out_buff->offset,
+			  sixpack_magic, 8);
+
+	return rc;
+}
+
+int write_to_buf(void *out_buf, u32 out_buf_size, u32 *offset, void *in_buf,
+		 u32 in_buf_size)
+{
+	int rc = 0;
+
+	if (*offset >= out_buf_size) {
+		rc = CUDBG_STATUS_OUTBUFF_OVERFLOW;
+		goto err;
+	}
+
+	memcpy((char *)out_buf + *offset, in_buf, in_buf_size);
+	*offset = *offset + in_buf_size;
+
+err:
+	return rc;
+}
+
+int write_chunk_header(struct cudbg_buffer *_outbuf, int id, int options,
+		       unsigned long size, unsigned long checksum,
+		       unsigned long extra)
+{
+	unsigned char buffer[CUDBG_CHUNK_BUF_LEN];
+	int rc = 0;
+
+	buffer[0] = id & 255;
+	buffer[1] = (unsigned char)(id >> 8);
+	buffer[2] = options & 255;
+	buffer[3] = (unsigned char)(options >> 8);
+	buffer[4] = size & 255;
+	buffer[5] = (size >> 8) & 255;
+	buffer[6] = (size >> 16) & 255;
+	buffer[7] = (size >> 24) & 255;
+	buffer[8] = checksum & 255;
+	buffer[9] = (checksum >> 8) & 255;
+	buffer[10] = (checksum >> 16) & 255;
+	buffer[11] = (checksum >> 24) & 255;
+	buffer[12] = extra & 255;
+	buffer[13] = (extra >> 8) & 255;
+	buffer[14] = (extra >> 16) & 255;
+	buffer[15] = (extra >> 24) & 255;
+
+	rc = write_to_buf(_outbuf->data, _outbuf->size, &_outbuf->offset,
+			  buffer, 16);
+
+	return rc;
+}
+
+int write_compression_hdr(struct cudbg_buffer *pin_buff,
+			  struct cudbg_buffer *pout_buff)
+{
+	struct cudbg_buffer tmp_buffer;
+	unsigned long fsize = pin_buff->size;
+	unsigned char *buffer;
+	unsigned long checksum;
+	int rc;
+	char *shown_name = "abc";
+
+	/* Always release inner scratch buffer, before releasing outer. */
+	rc = get_scratch_buff(pout_buff, 10, &tmp_buffer);
+
+	if (rc)
+		goto err;
+
+	buffer = (unsigned char *)tmp_buffer.data;
+
+	rc = write_magic(pout_buff);
+
+	if (rc)
+		goto err1;
+
+	/* chunk for File Entry */
+	buffer[0] = fsize & 255;
+	buffer[1] = (fsize >> 8) & 255;
+	buffer[2] = (fsize >> 16) & 255;
+	buffer[3] = (fsize >> 24) & 255;
+	buffer[4] = 0;
+	buffer[5] = 0;
+	buffer[6] = 0;
+	buffer[7] = 0;
+	buffer[8] = (strlen(shown_name)+1) & 255;
+	buffer[9] = (unsigned char)((strlen(shown_name)+1) >> 8);
+	checksum = 1L;
+	checksum = update_adler32(checksum, buffer, 10);
+	checksum = update_adler32(checksum, shown_name,
+				  (int)strlen(shown_name)+1);
+
+	rc = write_chunk_header(pout_buff, 1, 0,
+				10+(unsigned long)strlen(shown_name)+1,
+				checksum, 0);
+
+	if (rc)
+		goto err1;
+
+	rc = write_to_buf(pout_buff->data, pout_buff->size,
+			  &(pout_buff->offset), buffer, 10);
+
+	if (rc)
+		goto err1;
+
+	rc = write_to_buf(pout_buff->data, pout_buff->size,
+			   &(pout_buff->offset), shown_name,
+			   (u32)strlen(shown_name)+1);
+
+	if (rc)
+		goto err1;
+
+err1:
+	release_scratch_buff(&tmp_buffer, pout_buff);
+err:
+	return rc;
+}
+
+int compress_buff(unsigned char *hash_table, struct cudbg_buffer *pin_buff,
+		  struct cudbg_buffer *pout_buff)
+{
+	struct cudbg_buffer tmp_buffer;
+	struct cudbg_hdr *cudbg_hdr;
+	unsigned long checksum;
+	unsigned char *result;
+	unsigned int bytes_read;
+	int chunk_size, level = 2, rc = 0;
+	int compress_method = 1;
+
+	bytes_read = pin_buff->size;
+	rc = get_scratch_buff(pout_buff, CUDBG_BLOCK_SIZE, &tmp_buffer);
+
+	if (rc)
+		goto err;
+
+	result = (unsigned char *)tmp_buffer.data;
+
+	if (bytes_read < 32)
+		compress_method = 0;
+
+	cudbg_hdr = (struct cudbg_hdr *)  pout_buff->data;
+
+	switch (compress_method) {
+	case 1:
+		chunk_size = fastlz_compress_level(hash_table, level,
+						   pin_buff->data,
+						   bytes_read, result);
+
+		checksum = update_adler32(1L, result, chunk_size);
+
+		/* This check is for debugging Bug #28806 */
+		if ((chunk_size > 62000) && (cudbg_hdr->reserved[7] < (u32)
+		    chunk_size))   /* 64512 */
+			cudbg_hdr->reserved[7] = (u32) chunk_size;
+
+		rc = write_chunk_header(pout_buff, 17, 1, chunk_size, checksum,
+					bytes_read);
+
+		if (rc)
+			goto err_put_buff;
+
+		rc = write_to_buf(pout_buff->data, pout_buff->size,
+				  &pout_buff->offset, result, chunk_size);
+
+		if (rc)
+			goto err_put_buff;
+
+		break;
+
+		/* uncompressed, also fallback method */
+	case 0:
+	default:
+		checksum = update_adler32(1L, pin_buff->data, bytes_read);
+
+		rc = write_chunk_header(pout_buff, 17, 0, bytes_read, checksum,
+					bytes_read);
+
+		if (rc)
+			goto err_put_buff;
+
+		rc = write_to_buf(pout_buff->data, pout_buff->size,
+				  &pout_buff->offset, pin_buff->data,
+				  bytes_read);
+		if (rc)
+			goto err_put_buff;
+
+		break;
+	}
+
+err_put_buff:
+	release_scratch_buff(&tmp_buffer, pout_buff);
+err:
+	return rc;
+}
diff --git a/drivers/net/ethernet/chelsio/cxgb4/fastlz_common.h b/drivers/net/ethernet/chelsio/cxgb4/fastlz_common.h
new file mode 100644
index 00000000..53ae1bac
--- /dev/null
+++ b/drivers/net/ethernet/chelsio/cxgb4/fastlz_common.h
@@ -0,0 +1,107 @@
+#ifndef __FASTLZ_COMMON_H__
+#define __FASTLZ_COMMON_H__
+
+#define FASTLZ_HASH_LOG  13
+#define FASTLZ_HASH_SIZE (1 << FASTLZ_HASH_LOG)
+#define FASTLZ_HASH_MASK  (FASTLZ_HASH_SIZE - 1)
+
+/*
+ * Always check for bound when decompressing.
+ * Generally it is best to leave it defined.
+ */
+#define FASTLZ_SAFE
+
+/*
+ * Give hints to the compiler for branch prediction optimization.
+ */
+#define FASTLZ_EXPECT_CONDITIONAL(c)	(__builtin_expect((c), 1))
+#define FASTLZ_UNEXPECT_CONDITIONAL(c)	(__builtin_expect((c), 0))
+
+/*
+ * Use inlined functions for supported systems.
+ */
+#define FASTLZ_INLINE inline
+
+/*
+ * Prevent accessing more than 8-bit at once, except on x86 architectures.
+ */
+#if !defined(FASTLZ_STRICT_ALIGN)
+#define FASTLZ_STRICT_ALIGN
+#if defined(__i386__) || defined(__386)  /* GNU C, Sun Studio */
+#undef FASTLZ_STRICT_ALIGN
+#elif defined(__i486__) || defined(__i586__) || defined(__i686__) /* GNU C */
+#undef FASTLZ_STRICT_ALIGN
+#elif defined(_M_IX86) /* Intel, MSVC */
+#undef FASTLZ_STRICT_ALIGN
+#elif defined(__386)
+#undef FASTLZ_STRICT_ALIGN
+#elif defined(_X86_) /* MinGW */
+#undef FASTLZ_STRICT_ALIGN
+#elif defined(__I86__) /* Digital Mars */
+#undef FASTLZ_STRICT_ALIGN
+#endif
+#endif
+
+/*
+ * FIXME: use preprocessor magic to set this on different platforms!
+ */
+
+#define MAX_COPY       32
+#define MAX_LEN       264  /* 256 + 8 */
+#define MAX_DISTANCE 8192
+
+#if !defined(FASTLZ_STRICT_ALIGN)
+#define FASTLZ_READU16(p) (*((const unsigned short *)(p)))
+#else
+#define FASTLZ_READU16(p) ((p)[0] | (p)[1]<<8)
+#endif
+
+#define HASH_FUNCTION(v, p) {\
+				v = FASTLZ_READU16(p);\
+				v ^= FASTLZ_READU16(p + 1)^\
+				     (v>>(16 - FASTLZ_HASH_LOG));\
+				v &= FASTLZ_HASH_MASK;\
+			    }
+
+extern unsigned char sixpack_magic[8];
+
+#define CUDBG_BLOCK_SIZE      (63*1024)
+#define CUDBG_CHUNK_BUF_LEN   16
+#define CUDBG_MIN_COMPR_LEN   32	/*min data length for applying compression*/
+
+/* for Adler-32 checksum algorithm, see RFC 1950 Section 8.2 */
+
+#define ADLER32_BASE 65521
+
+static inline unsigned long update_adler32(unsigned long checksum,
+					   const void *buf, int len)
+{
+	const unsigned char *ptr = (const unsigned char *)buf;
+	unsigned long s1 = checksum & 0xffff;
+	unsigned long s2 = (checksum >> 16) & 0xffff;
+
+	while (len > 0) {
+		unsigned k = len < 5552 ? len : 5552;
+		len -= k;
+
+		while (k >= 8) {
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			s1 += *ptr++; s2 += s1;
+			k -= 8;
+		}
+
+		while (k-- > 0) {
+			s1 += *ptr++; s2 += s1;
+		}
+		s1 = s1 % ADLER32_BASE;
+		s2 = s2 % ADLER32_BASE;
+	}
+	return (s2 << 16) + s1;
+}
+#endif /* __FASTLZ_COMMON_H__ */
-- 
2.14.1

